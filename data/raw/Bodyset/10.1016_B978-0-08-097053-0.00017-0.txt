In this chapter, following on from Chapter 3, some more of the important tools used by the mineral process engineer are briefly described. Under modeling is included computer simulation for circuit design, computational fluid dynamics (CFD) and discrete element method (DEM) for equipment design, and design of experiments (DOE) for empirical model building. Under characterization, geometallurgy and applied mineralogy are the topics; another characterization tool, surface analysis, is covered in Chapter 12. Computer simulation has become an important tool in the design and optimization of mineral processing plants. The capital and operating costs of mineral processing circuits are high and in order to reduce them consistent with desired metallurgical performance, the design engineer must be able to predict the metallurgical performance, relate performance to costs, and select the circuit for detailed design based on these data. Simulation techniques are suitable for this purpose, provided that the unit models are valid, and considerable progress continues to be made in this area. Computer simulation is intimately associated with mathematical modeling and realistic simulation relies heavily on the availability of accurate and physically meaningful models. Several commercial simulators for mineral processing are available, including: JKSimMet/Float, USimPac (  Compared to laborious and expensive plant trials, computer simulation offers clear advantages in assessing alternative circuits, optimizing design, and estimating flow rates of process streams, which can be used to size material handling equipment (conveyors, pumps, and pipelines). However, the dangers of computer simulation also come from its computational power and relative ease of use, which encourage searching the “what-if” space. It is always necessary to respect the operating range over which the models are valid, as well as the realistic limits which must be placed on equipment operation, such as pumping capacity. In addition, it is worth remembering that good simulation models combined with poor data or poor model parameter estimates can produce highly plausible looking nonsense. Simulation studies are a powerful and useful tool, complementary to sound metallurgical judgment and familiarity with the circuit being simulated and its metallurgical objectives. To improve particle size reduction and particle separation systems and thus increase productivity of mineral Still today much testing is empirical, which can mean years to reach a conclusion straining budgets and financial backers’ patience. Employing mathematical modeling tools can significantly reduce the time and cost involved. Advances in computational power enable multiple simulated iterations of a device’s operation. Computational fluid dynamics (CFD) is the application of algorithm and numerical techniques to solve fluid flow problems ( CFD is now applied to complex fluid flow problems with a high degree of confidence in the retrieved solution, even in the case of a mixture of two fluids like air and water. A sound knowledge of fluid dynamics, however, remains central to comprehending the simulations. An important point to remember when applying CFD techniques to mineral processing problems is that slurries do not behave like water. This is due to the influence of variable viscosity and density depending on the local solids concentration and particle size. The model needs to take account of those slurry properties. Adequately incorporating slurry properties is a major focus of particle technology research. The discrete element method (DEM) is a numerical technique to simulate the behavior of a population of independent particles ( An example of this process is shown in Combinations of CFD and DEM have been used to describe the behavior of particles moving and colliding inside a flowing fluid. In mineral processing where the ore is mostly processed wet, these CFD-DEM coupled approaches are of interest as they promise to optimize equipment design. However, it should be noted that these coupled simulations require large computational power and advanced technical knowledge. A model has no value without validation to assess the fidelity of the simulation to the real process. This requires a certain level of physical testing, which can be performed on simple set-ups representing specific parts of the process, or on lab-scale equipment representing the whole process. In validating a mineral processing operation, one of the difficulties is the observation of the slurry flow and how particles interact in this flow. Few techniques are able to quantitatively track and display particle or fluid behavior. Some of the techniques are: particle image velocimetry (PIV), laser Doppler anemometry (LDA), dye injection, high-speed imaging, and acoustic monitoring. While widely used, each has limitations, mostly the opacity and the lack of precision with the high solids content slurries common to mineral systems. The technique of Positron Emission Particle Tracking (PEPT) is attractive as it enables particles to be tracked in opaque and high particle concentration systems. PEPT is based on tracking a radioactive tracer particle that is mixed with the feed to a unit (  Beginning with its earliest incarnation called factorial design, the experimental approach now known as Experimental design is a structured process for investigating the relationship between input variables and output effects, Modern experimental design has evolved from the full factorial designs of Fisher, which had the serious limitation of being constrained by the number of factors that could be reasonably investigated. The following relationship illustrates the point. If At 3 levels (low, high, mid-point), a 3 factor design would require 27 tests, a 4 factor design, 81 tests, and for 5 factors, 243 tests. If only high and low levels are tested, the test numbers become a more reasonable 8, 16, and 32, respectively. Full factorial designs are therefore typically conducted at low and high levels only and are suitable for up to, at most, 5 (=32 runs) or 6 (=64 runs) factors. Modern DOE has divided the overall design process into two stages. In the first stage many factors are tested to determine those that are critical to the process, and is called the A favored experimental design for optimization is the Central Composite Design (CCD), shown in The rigor of the experimental design method makes it well suited for laboratory and pilot testing where the effect of uncontrolled variables can be minimized ( The traditional approach to plant design involved the extensive testing of a single large composite sample or a small number of composite samples that are reputed to represent the ore body. It is accepted that laboratory tests can accurately measure factors such as the grindability, floatability, or other process parameter of the sample by a technique representing that to be used in the plant. The size of equipment required to achieve a specified throughput and product quality is then calculated from one of a variety of models that have been developed over the years, with some examples being given in previous chapters. Since these tests and models are tried and tested, they are accepted as reasonably precise. During operation of the resulting plant, the design is sometimes found to be inadequate. It is then suspected that the flaw in the design process lies in the samples not being sufficiently representative of the ore body, since using only a single or small number of composite samples does not recognize the variability of the ore, nor does it allow for the lack of precision in the value of the metallurgical parameter used in the design. A geometallurgical approach uses a design procedure suited to an ore body that is described by a geostatistical analysis of a reasonably large number of small samples of drill core. The analysis requires the identification of the location of the sample points and a geological plan of the ore body, together with a mine plan of the blocks to be mined during the proposed life of the mine. The plant design can then be made using the estimated metallurgical parameter of each individual block. A statistical error can be assigned to the estimated parameter for each block and for each production period such that the final design Consequently, we can define “geometallurgy” as: The geologically informed selection of a number of samples for the determination of metallurgical parameters The distribution of these parameters across the blocks of the ore body by some accepted geostatistical technique, where the distribution is usually influenced by the geology because lithology/alteration/texture has an effect on the parameters The subsequent use of the distributed data in metallurgical process models to generate economic parameters such as throughput, grind size, grade and recovery for each mine block for plant design and production forecasting that can be used in mine planning. This “geometallurgical approach” is needed because: Ore bodies are variable in both grade and metallurgical response The variability is a source of uncertainty that affects plant design, results (both metallurgical and financial) and capital investment decisions Deposits are becoming lower grade and more complex Throughputs are necessarily increasing and profit margins reduced; the financial risks are escalating Mining industry risk must be more carefully managed for projects to attract the necessary finance. A “geometallurgical project” takes a step-by-step approach: Geologically informed selection of a number of variability samples (i.e., samples selected to reveal variability) Determination of relevant metallurgical parameters for those samples Populating a spatial model of the ore body by geostatistical distribution of those parameters Use of the distributed dataset of parameters in metallurgical models for design and forecasting Estimation of the uncertainties in the knowledge of the ore body to calculate lack of precision in the results Managing risk by adding safety factors to designs and calculating error bars for the forecasts. It is important to realize the interrelated technologies involved in a mining operation and to include all departments in the design and production forecasting project: Metallurgy determines expected results for throughput and recovery of the plant by: The use of tests on drill core samples to generate metallurgical parameters for the ore: for example, grindability, flotation kinetics The use of these parameters in process models. Mining determines temporal variability in the production sequence; consequently, the metallurgical parameters vary from time to time in the plant Geology affects geographic variability in the mineral assemblage of the ore body; consequently, the metallurgical parameters vary from place to place Geostatistics use the geological information to make an estimate of the metallurgical parameters for each block and get an idea of the estimation errors (i.e., the uncertainty). It is self-evident that the more samples that are tested and the better they are selected, the greater the certainty in the data used for forecasting and the lower the project risk. Effective sampling of an ore body is both difficult and expensive, but important to reduce the greater cost from the financial risk of failure to meet the expected results in production ( The stage of the project: that is, preliminary, pre-feasibility, or full feasibility The size of the ore body, and complexity of the geology, and resulting metallurgy. The number of samples depends on the project stage, plant throughput, and ore variability. Some guidelines for sample selection of drill core are: Always consult with the Geology and Mine Planning departments Try to include the variability of the ore types, that is, lithology, alteration, mineral occurrence of both values and gangue minerals; use geochemical and structural information Choose a representative number of each ore type; validate the sample set against the resource population. Outliers or unusual ore types should not be over- or under-represented Fresh core is better. Old badly stored core can supply erroneous data. But near-surface weathered ore must be included for testing, as it will be included in the mine plan Full or half-core is better than “assay rejects” Space the samples to allow uncertainty to be calculated; some close together, but most to cover the complete area of interest between drill holes and down-hole. A random distribution is acceptable for preliminary stages, but use of the drilling grid is better for feasibility design and forecasts Select samples to match the mining method while still showing variability; for example, composite by length Choose a relevant mining time period from the mine plan as a source of most of the samples; for example, most of them from the first 5 years of production for new plant design. Add more samples for testing each year for on-going production forecasting Identify the number of samples needed for the current stage of the project: Preliminary stage of a large project may need 35 samples to demonstrate the variability of the ore body and allow estimate of equipment size (albeit imprecise) Rule-of-thumb for pre-feasibility life-of-mine sampling is one sample per million tons of ore under evaluation, or 1 sample per 400,000 A full feasibility study will require more samples for a large ore body, but only the first 10 years of the mine plan are of immediate interest. The number of samples should be based on a statistical analysis to ensure that it meets the required level of confidence Collect a sample set that is representative of the variability of the section of the deposit that is of most interest when considering the financial risk in the project—not a “representative sample” (since there is no such thing). Variability samples must be tested for the relevant metallurgical parameters. Ball mill design requires a Bond work index, BWi, for ball mills at the correct passing size; SAG mill design requires an appropriate SAG test, for example, SPI ( Understanding and using the measured metallurgical parameters of the whole ore body requires that the test data are distributed across all the blocks in the mine plan. This exercise involves the consideration of much more information about the mine than would traditionally be used in plant design, which includes: Location of each sample within the ore body in terms of co-ordinates and section of core used Geological description of the sample, for example, lithology, alteration, rock type, and perhaps metal grade Mine block plan with similar geological information Planned mining schedule for the mine blocks, for example, by year. The objective of the analysis is the distribution of the metallurgical test data across the blocks in the mine plan, assigning each block (and each mining period) an estimated metallurgical parameter value (e.g., BWi), and a precision of each estimate. A suitable method involves: Basic statistical study to identify Use of a geostatistical technique, such as Kriging, or regression analysis, for distribution of metallurgical parameters within each geometallurgical domain. Independent variables such as BWi that can be used in statistical analysis can be distributed directly to mine blocks by the distance weighting method known as Kriging. Variables that are dependent on other parameters, such as maximum attainable recovery that is dependent on head grade, may be distributed to blocks by regression analysis using the estimated block value for head grade, and the estimate of maximum recovery can be improved by Kriging of “residuals.” Parameters that are not amenable to statistical analysis, such as flotation rate constant, must be recognized and handled by transposing to some variable that is suitable for Kriging or some other geostatistical technique Acceptance of the uncertainty in the estimated value of each block. Distribution of metallurgical parameters is best done using a method that involves a measure of the statistical error. The error can be included in simulations of plant performance to show the uncertainty in the forecast results. A short description of Kriging is a pre-requisite to understanding the geometallurgical approach ( In practice, the estimate for a block is made from samples within an ellipsoid with dimensions selected after consideration of the geological structure, using the weighted average of a minimum of 3 samples that are not all in the same drill hole. If there are insufficient samples within those dimensions, a further ellipsoid is chosen with larger dimensions, and so on until there are sufficient samples. Enlarging the dimensions to use samples that are further away produces block estimates with larger standard errors. The use of data averaged from at least 3 samples results in a smoothing of variability in the block estimates. It is always instructive to compare the frequency distribution of parameters measured from the samples with that attributed to the blocks; excessive smoothing or shift in the mean indicates too few samples were tested. Since blocks are normally identified by year on the mining plan, the geostatistical analysis also allows the determination of annual average parameter values and their statistical errors by the same Kriging technique. So, it is possible to design the plant to deliver a specified throughput and with optimum grade and recovery in each production year. It is also possible to extend the analysis to calculate how many more samples need to be tested from within the range of the blocks mined in a production year in order to improve the precision to any desired level. Metallurgical parameters (such as hardness work indices or mineral flotation kinetics) are estimated for each block in the mine model and all the blocks are used as an input dataset for process models. These process models are used for simulations to determine throughput, grade and recovery per block for new plant designs or for forecasting the results from existing operations ( It is important that the blocks are populated with the estimated metallurgical parameters. The results of the simulations in terms of throughput or recovery can then be assigned to each block. Remember that these results are specific to the plant design used in the model, and that changing the plant design will produce a new set of results for allocation to each block. Never try to populate the blocks with plant results that are determined by simulation from individual samples. Using a statistical approach to distribution of metallurgical parameters ensures that each block value is accompanied by a standard error, for example, Kriging or regression errors. This allows process models to be run as Monte Carlo simulations using the estimated value as the mean of a distribution of possible values that has a standard deviation based on the standard error. Hence the standard error of the resultant plant forecast can be determined, as a measure of uncertainty. Since the geostatistics also allows the estimation of annual average parameter values and their statistical errors, it is possible to calculate the uncertainty in the forecast for each production year. Error bars can be fitted to the production forecasts for blocks and for average annual production. The size of the bar is determined from standard error and the confidence that is to be applied. The more confidence we wish to place in the forecast, the larger the error bar to encompass Adding a safety factor to equipment size may not reduce the risk in failing to achieve specified flotation recovery to an acceptable level, since maximum attainable recovery is a fixed limit. However, this approach quantifies the remaining risk, which can be invaluable to the financial viability of the project. In a study for the design of a SAG-Ball mill grinding plant ( The samples were tested for SPI value for SAG design and BWi value for ball mill design. The variability in each case was slightly less than typical; standard deviation is normally about 20% of the mean value in terms of specific energy, kWh t The sample data were distributed across 10,903 mine blocks, representing approximately 130 million tons of ore to produce a dataset for the ore body on which to base the plant design. Statistical analysis indicated significant grindability differences between lithologies but, with only 100 samples in total, there was insufficient data to do separate geostatistical analysis of each type. This difference indicated that lithology can be used as a guide to the hardness of any block, but the wide range of values within each domain suggested that there is some other factor (such as degree of alteration) that has a regional basis. The samples and mine blocks were grouped into three different ore types and there were sufficient samples in two of these to conduct a separate geostatistical analysis for each, allowing block values to be estimated from samples of the same ore type treated as domains. Blocks from the third ore type were estimated from a combination of all samples. Subsequently, the block values were estimated using Kriging within each domain using only the neighboring samples within that domain. Spacing of samples was adequate since, for each ore type, at least 75% of the mine blocks had a sample of the same type within 100 The power required to achieve the average specified throughput and target grind size from the 10,903 blocks mined over an 18-year-life was determined using simulations in the CEET process model ( Examination of datasets of blocks grouped into annual production periods indicated years when throughput was below target due to limitations from either the SAG or the ball mill circuits. Model simulations indicated that this could be avoided to some extent by planning changes to the SAG discharge screen aperture. However, it was necessary to increase the SAG mill power requirement by 5% to ensure that target throughput was met on average during the year with hardest ore. The statistical errors in average annual estimates of grindability were used in Monte Carlo simulations within the CEET simulator to determine uncertainty in the forecast throughput. Error bars were put on annual forecasts at 90% confidence limits, indicating that a safety factor of approximately 14% must be added to the power requirements for both mills to ensure that the specified throughput was achieved in every year of the mine life. This still left the 5% statistical chance that specified throughput will not be achieved on average in every year. Quantitative automated mineralogy (QEMSCAN: Quantitative Evaluation of Minerals by Scanning Electron Microscopy; MLA: Mineral Liberation Analyser; and recently TIMA–Tescan Integrated Mineral Analyser) is increasingly being applied to the study of ore deposits and the evaluation of mineral processing operations. The three instruments identified are all based on electron microscopes, although they may be operationally different, and form the main basis of this section. They provide information on polished sections, that is, 2D data, and have now been joined by X-ray microtomography, which Samples can be analyzed in many forms either as intact drill core, coarse reject material, and composite plant samples when evaluating mineral processing operations. Intact drill core samples are analyzed to determine rock-types and provide textural characterization. Intact core can be mounted on a polished section (or polished thin section) where chemical spectra are collected at a set interval within the field of view. Each field of view is then processed offline and a pseudo color image of the sample is produced, from which the modal mineralogy and texture of the sample can be extracted. (“Modal” refers to mineral proportion or weight% of mineral in a sample calculated taking into consideration the mineral specific gravity.) Data are acquired over the polished sections at a varied pixel size (e.g., 5 or 25 The information is presented according to the questions being asked, as illustrated below. The emphasis is on quantitative data presented in a way to aid the mineral processor reach an informed decision. Blended and homogenized coarse reject samples from, for example, 2-3 meter drill core intervals used for geochemical analyses, can be analyzed to determine mineral variability which might have an impact on the metallurgical response. These coarse reject samples are further ground and homogenized and a pre-defined number of particles are mapped at a selected resolution. Such studies are critical in defining the distribution of minerals in a deposit. As an illustration, Grain size distribution of the individual minerals can be extracted from an automated mineralogical analysis. This information illustrates the relationship of the grain size of the various phases within a sample. An example is shown in Particles are classified into groups based on mineral-of-interest area percent: for example, free (≥95% of the total particle area), liberated (≥80%), and non-liberated (<80%). The non-liberated grains can be further classified according to association characteristics into binary and complex groups. Valuable metals can occur in different minerals and in trace quantities in gangue minerals. Instrumentation used to quantify elements include electron probe micro analysis, Laser Ablation ICP-MS, dynamic Secondary Ion Mass Spectrometry, and micro-pixie. (Which instrument to employ is mainly dependent on the ore type and the mineral assemblage.) Coupling with automated mineralogical analyses, the distribution of metals among the minerals can be quantified. As an example, consider a Cu-Ni deposit. Commonly, Ni is carried by the sulfides pentlandite, millerite, and violarite, but can also occur in small amounts (a few ppm to ~1 Introduced in An example is given is Based on electron beam instruments, the mineralogical data is 2D. At one time the apparent need to correct to 3D was a major research activity ( Cone beam X-ray microcomputed tomography (micro CT) systems were introduced commercially a decade ago.