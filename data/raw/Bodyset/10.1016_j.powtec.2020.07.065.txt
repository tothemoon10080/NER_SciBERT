Classification of the flotation method is an important stage in the design of the flotation process. This study faces the problems of small samples and category imbalance through the following steps: (1) The XGBoost was chosen as the multiple classifier, and the geometric mean of the recall rates was used as the evaluation metric. (2) The proposed evaluation set validation greatly reduced the standard deviation (Std) of the evaluation metrics compared with cross-validation. (3) A training set of minority categories oversampled by the synthetic minority oversampling technique (SMOTE) improved the of classification effect of minority categories. (4) The Tree Parzen Estimator (TPE) was used as a hyper-parameter optimization method and realized better performance of the model. The results show that the mean value and Std of GM were 0.867 and 0.014, respectively, and the recall rates of preferential flotation, partial flotation and mixed flotation were 0.849, 0.831 and 0.922, respectively.The traditional process of the flotation flowchart design includes process mineralogy research, systematic beneficiation test research (including small-scale tests, expanded continuous tests, and semi-industrial or industrial tests), recommended process schemes, design of the beneficiation plant according to the recommended process and commissioning. There are many disadvantages to this model, such as a long development cycle, high cost, low efficiency, and waste caused by repeated experiments. Thus, Sun Chuan-Yao [ The construction of a decision tree is suitable for exploratory knowledge mining and can handle high-dimensional data. Deconinck et al. [ XGBoost was chosen because it performs better on this problem than other decision tree ensemble models, but there are several challenges. First, the small size of samples leads to the instability of the evaluation index obtained by cross-validation. The evaluation set validation method proposed in this study greatly reduced the standard deviation (Std) of the evaluation metrics. Second, category imbalance results in poor classification of the categories with fewer samples. In this study, the training set of minority categories was oversampled using the SMOTE algorithm, such that minority categories achieved a higher recall rate, and the evaluation metric GM of the proposed model was improved correspondingly. Third, XGBoost has too many hyper-parameters to optimize. The values of the hyper-parameters affect the prediction effect of XGBoost (this has been ignored by many studies). In this study, the Tree Parzen Estimator was used to optimize hyper-parameters and improve the performance of the model. Bagging is an ensemble learning method based on a bootstrap aggregating algorithm that has random sampling with put-back [ Boosting is divided into AdaBoost and gradient boosting [ The steps are as follows: Use the training set to train the first base learner to obtain the residual of each training sample. Use the residual as the fitting target on the next base learner until the new residual is not smaller than the previous one. Add the output of each base learner to obtain the predicted value. Decision tree learning includes CT and regression tree structures. When used alone, CT can solve only classification problems, and a regression tree can solve only regression problems. However, a regression tree can solve classification problems through integration learning. A regression tree uses mean square error (MSE) as the basis of node splitting, as shown in Eq. If node D (sample number A regression tree traverses all values of all features in each split, looking for the splitting feature and splitting value that maximizes After the training set completes the training of the regression tree, the splitting feature and splitting values of each node in the regression tree are determined. When making predictions on a sample x A Gradient Boosting Decision Tree (GBDT) is an ensemble model acquired by gradient boosting based on a regression tree. GBDT does not use CT as base learners, even in a multiple classification problem. This is because GBDT must fit the gradient value in every iteration, and the gradient value is a continuous value. Thus, only a regression tree can be used. The basic idea behind GBDT is to learn incrementally and become increasingly closer to the final prediction. A decision tree is taught with an initial value, and then the predicted value in the leaf node and the residual after the prediction is obtained. The subsequent decision tree will learn based on the residual of the previous decision tree until the residual of the predicted value and the real value is zero or cannot be reduced. Eventually, the predicted value of the test sample is the sum of the predicted values of all decision trees. For the multiple classification problem, GBDT uses a one-to-many strategy. In other words, if M regression trees are trained for each category, and there are K categories, then there are M × K trees after training. In each category, the second tree can be fitted only after the first tree has been fitted. The M trees of a particular category cannot be taught before they learn another category. If the number of categories is K, and each category uses M regression trees, the generation steps of GBDT are as follows (the annotation of symbols is presented in Step 1: Assume Step 2: Assume Step 3: Using Eq. Using the second-order Taylor expansion formula of Step 4: Calculate the prediction function Step 5: Generate the m Step 6: After the model training is completed, Extreme Gradient Boosting Machine (XGBoost) is a special case of GBDT and has the following characteristics [ XGBoost adds a regularization term to the objective function to control the complexity of the model in the process of generating the m In Eq. XGBoost performs a second-order Taylor expansion of the objective function, using both first and second derivatives. The simplified objective function is shown in Eq. XGBoost supports column sampling, which reduces overfitting and is borrowed from RF [ When seeking the optimal splitting point, a greedy method considering all possible splitting points for each feature is inefficient. XGBoost implements an approximate algorithm, the idea behind which is to list several candidates that are likely to be splitting points using the percentile method and then calculate the optimal splitting points from the candidates using the formula for splitting points. One hundred forty-seven samples of copper ore flotation were provided by a research institute, among which 57 were  In this study, the recall rate is used to evaluate the classification performance of the model for each category.  The recall rate is calculated using Eq. Traditional evaluation indicators, such as accuracy, do not apply to category imbalance as a main metric. If we aim to improve the accuracy in the process of feature screening, model selection, and optimization, the classifier will pay more attention to the majority category and ignore the classification effect of the minority class [ For the dichotomous problem, Kubat and Matwin proposed evaluating the performance of the model with the geometric mean of the recall rates for positive samples and negative samples. In this study, GM is defined as the geometric mean of all categories, and the recall rate is calculated as follows. The recall rate of Cross-validation is a common method in machine learning. However, it does not apply to this paper owing to the small sample size and category imbalance in this study. If the number of training set samples is insufficient, the sample distribution characteristics cannot be fully learned by classifiers. If the number of training set samples is satisfied, the sample size of the test set will be too small to guarantee the reliability of the evaluation metric on the test set because of the large standard deviation. To solve this contradiction, this study proposes evaluation set verification for small sample machine learning. The steps of evaluation set verification are as follows: Step 1: Set a large training set proportion. In this study, the training set proportion is set to 95%, and the test set is set to 5%. The training set samples are used to generate a classifier, the categories of the test set samples are predicted by the classifier, and the real and predicted categories of the test set samples are recorded. Step 2: Sets the multiple of the evaluation set relative to the test set, which is set as 50 in this study. Repeat step 1 50 times, and obtain 50 test sets to form an evaluation set. The evaluation metrices of the evaluation set are calculated according to the real and predicted categories of the evaluation set samples. Step 3: Repeat step 2 ten times to obtain ten evaluation sets. Repeat step 2 ten times to obtain the evaluation metrices of ten evaluation sets, whose average value represents the evaluation metric value of the classifier, and the standard deviation represents the credibility of the evaluation metric. The smaller the standard deviation, the higher the credibility. For the imbalance of categories, training the model with the raw training set will result in a low recall rate of minority classes. In this study, SMOTE is applied. SMOTE is a heuristic oversampling technique designed to solve the category imbalance and significantly improves the overfitting caused by non-heuristic random oversampling methods. The core idea is to insert randomly generated new samples between a sample of minority categories and their neighboring classes, which can increase the number of minority categories and improve the unbalanced category distribution of the dataset. The samples of a minority category in the training set are set as S A sample x A sample x The first two steps are repeated until the number of samples for the category in the training set is equal to the number of samples for the category with the largest number of samples. The decision tree and decision tree ensemble model can provide variable importance measures (VIM) after training the model [ To evaluate the performance of XGBoost on the flotation methods recommended, it is compared with other classification models. Logistic regression (LR), support vector machines (SVM), and the CT are commonly used classifiers. CT can realize multiple classifications directly, but LR and SVM are binary classifiers. For binary classifiers, multi-classification is realized via the one-to-many method. Each classifier distinguishes only the current category from other categories, so K binary classifiers are required to realize K classification. As a classification algorithm rather than a regression algorithm, LR [ This approach was first proposed by Vapnik as a solution to the dichotomy problem and is currently a mainstream supervised machine learning method [ Gini is used as the basis for node splitting in CT. The Gini value reflects the probability that two samples are randomly selected from a node (dataset D), and their category labels are inconsistent. As shown in Eq. As shown in Eq. RF is an ensemble model of the CT [ Select n samples from the training set by sampling with replacement. Randomly select Repeat In the prediction of test set samples, each tree produces a result and adopts a majority voting mechanism to output. Like XGBoost, Light GBM is a special case of GBDT and has the following characteristics [ Gradient-based one-side sampling (GOSS) distinguishes samples of different gradients and then retains large gradient samples and randomly samples from small gradient samples to improve efficiency. Exclusive Feature Bundling (EFB) refers to the method of reducing feature dimensions through feature bundling to improve computing efficiency. Light GBM chooses the decision tree algorithm based on a histogram. Compared with a presorting algorithm, the histogram has several advantages in terms of memory consumption and calculation cost; e.g., it packs the eigenvalues first and forms bins one by one. For continuous features, packing is a discretization method in feature engineering. Hyper-parameters are parameters that are set before the learning process and not the parameter data obtained through training. Generally, in the process of machine learning, it is necessary to optimize the over-parameters and select a set of optimal hyper-parameters for the learner to improve the learning performance and effect. The main hyper-parameters in XGBoost and their meanings are as follows: N_estimators: number of decision trees. Max_depth: maximum depth of the decision tree. Min_samples_leaf: minimum capacity of the leaf node. If the sample capacity of children nodes after partitioning is less than this value, no partitioning will be carried out so that the node becomes a leaf node. Min_samples_split: minimum capacity of the split node. If the sample capacity of a node is less than this value, it will not be divided, making the node become a leaf node. Learning_rate: learning rate, also known as step size. The smaller the learning rate, the more decision trees must be iterated. Subsample: subsample proportion. This parameter controls the proportion of random samples for each tree. The value algorithm that reduces this parameter will be more conservative and avoid overfitting. However, if this value is set too small, it may result in underfitting. The following four hyper-parameter optimization methods are applied and compared: Grid search. Grid search is the most widely used hyper-parameter optimization method [ Random search. Compared to grid search, random search does not try all hyper-parameter values but samples a fixed number of hyper-parameter values from a specified distribution. By randomly sampling the search range, random search is generally faster than grid search. Bergstra and Bengio [ Genetic algorithm optimization. The initial population was constructed, and then cross-validation and mutation in the genetic algorithm were used to generate a model with a good hyper-parameter combination. Bayesian optimization. Bayesian optimization [ TPE. TPE [ In this study, sequential forward search (SFS) is applied to select features. SFS generates a series of feature subsets by placing the most important feature into the subset and iteratively adding the remaining feature with the highest VIM to the subset. Finally, the subset of the feature that maximizes the model performance is selected. Selected features of XGBoost and their VIM are shown in  As shown in  As shown in To sum up, evaluation set validation, on the one hand, makes the classifiers learn more training set samples to achieve a better classification effect, and on the other hand ensures the stability of the evaluation metric, which is suitable for machine learning with small samples. Before using LR and SVM, the features are normalized and screened by a chi-square test. A decision tree and its integrated model do not need to normalize the features in advance, because they focus on the distribution of variables and the conditional probability between variables.  As shown in  As shown in  By comparing  As shown by comparing To sum up, SMOTE-XGBoost is the ideal classifier for copper flotation method recommendation. The effects of various optimizations are shown in As shown in When TPE-SMOTE-XGBoost is used as the classifier, the mean value and Std of GM are 0.867 and 0.014, respectively. The recall rates of The purpose of this study is to realize copper flotation method classification. The first problem to be solved is the unstable evaluation metric caused by the insufficient sample size, and the evaluation set validation proposed in this study performed better than cross-validation with lower Std. The second problem to be solved is the poor classification effect for minority categories. The proposed model oversampled the training set samples before learning and significantly improved the recall rate of minority categories. Through the classifier comparison experiment, the performance of CT was better than LR and SVM, and the ensemble models of the decision tree were better than CT. XGBoost had the best performance of the ensemble models. To further improve the performance of the classifier, TPE was used to optimize the hyper-parameters, and it was proved that its effect was better than that of other hyper-parameter optimization methods. TPE-SMOTE-XGBoost was obtained by combing XGBoost with SMOTE and TPE. The results show that the mean value and the Std were 0.867 and 0.014, respectively, and the recall rates of None. This work was supported by the