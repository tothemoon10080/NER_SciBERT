This chapter deals with the collection, analysis, and use of process data. Collection of reliable data is the science of sampling. The collected samples are analyzed for some quality, metal content (assay), particle size, etc. and the data are used for process control and metallurgical accounting.Determine an appropriate sample size for mill feed (minus 12 From the simulation, sample increment sizes ranging from 10 to 10,000 The sampling of materials having low valuable mineral content (e.g., parts per million) also increases the risk of large errors if sampling increments are too small. This becomes even more pronounced if there are large density differences between valuable mineral and gangue. Consider a 500 Note that, on average, the 15 Consider a lead ore, assaying ~5% Pb, which must be routinely sampled on crusher product for assay to a 95% confidence level of ±0.1% Pb. Assume the top size of the ore is 2.5 σ: The required precision, σ, in relative terms, is calculated from:   m: Assuming a galena s.g. of 7.6 and gangue s.g. of 2.65, and that galena is stoichiometrically PbS (86.6% Pb), then the ore is composed of 5.8% PbS giving a=0.058, r=7.6, and t=2.65, resulting in a mineralogical composition factor m=117.8  The overall constant Thus the required sample mass becomes In practice, therefore, about 350 A slurry stream containing quartz is diverted into a 1-liter density can. The time taken to fill the can is measured as 7 The density of quartz is 2650 The volumetric flowrate: Therefore, mass flowrate: A pump is fed by two slurry streams. Stream 1 has a flowrate of 5.0 Slurry stream 1 has a flowrate of 5.0 Slurry stream 2 has a flowrate of 3.4 A flotation plant treats 500 The volumetric flowrate of The mass flowrate of water in the slurry stream Therefore, the volumetric flowrate of water is 750 The volumetric flowrate of slurry=750+185.2 Therefore, for a nominal retention time of 5 Calculate the % solids content of the slurry pumped from the sump in The mass flowrate of solids in slurry stream 1 is: 2.73 The slurry contains 40% solids, hence the mass flowrate of water: Similarly, the mass flowrate of water in slurry stream 2: Total slurry weight pumped: Therefore, % solids by weight: The feed to a Zn flotation plant assays 3.93% Zn. The concentrate produced assays 52.07% Zn and the tailings 0.49% Zn. Calculate the solid split, ratio of concentration, enrichment ratio, and Zn recovery. The solid split ( The ratio of concentration is the inverse of the solid split: The enrichment ratio ( The Zn recovery ( The streams in the Zn flotation plant ( Calculate the recoveries of zinc and copper and provide the expression of their variances as functions of the variances of Zn and Cu assays on all 3 streams. The recoveries of Zn and Cu are calculated using For Zn: For Cu: For Zn: For Cu: For the example data set in In this example there is one process unit and three metals assayed; hence, there are four mass conservation equations and hence four imbalances. The criterion The derivatives of Now, the solid split value can be obtained from the above 2 equations: Using the data set of The results of For the example data set of In the present case, The solution of which is: Using the data set of The results of In the present case, Solving the system of equations leads to:  For the example data set of In the present case, the Lagrangian is: There is no measured mass flowrate, there are 3 stream samples with 3 metal assays each and there are 3 mass conservation equations: The Lagrangian derivatives are: Therefore, it is only required to find the best value of Assuming an initial value for Calculating the Calculating the criterion Iterating on the value of The solution is given in  A survey campaign is being designed around the grinding circuit in With the slurry feed flowrate to the grinding circuit being measured, water addition stream flowrates being measured and % solids in slurry being measured, the slurry and water mass balances can be calculated and associated measured values adjusted. The mass conservation equations that apply obey to the Slurry and Water Mass Conservation Networks are given in The mass conservation equations for the dry solid obey the same network except water addition streams are removed. Mass size fractions are not conserved through grinding devices (SAG mill and ball mills). Therefore there is no mass conservation equation for size fractions around the grinding devices and consequently there is no node for grinding devices in mass size fraction networks. The same reasoning applies to Cu-by-size fractions. The (overall) Cu on stream samples is conserved through grinding. Since Cu has been assayed only on the SAG mill feed and the cyclone overflows feeding the flotation circuit, the mass conservation network for Cu on stream samples consists of only one node. Assuming eleven sieves have been used for measuring the size distributions, then the total system of mass conservation equations consist in: Seven mass conservation equations for slurry flowrate variables Seven mass conservation equations for % solids variables Seven mass conservation equations for water flowrate variables Seven mass conservation equations for solid flowrate variables Forty eight (4 nodes×12 mass fractions) mass conservation equations for size fractions Forty eight mass conservation equations for Cu-by-size fractions One mass conservation equation for Cu In the following mass balance problem ( One metal has been analyzed on each sample; Two metals have been analyzed on each sample. When one metal is analyzed on each of the 6 streams: There are 4 equations (2 for the mass flowrates and 2 for the metal assay) and 5 unknowns. The system is globally underdetermined since there are not enough equations for the number of unknowns. Around node A, the 2-product formula can be applied and therefore the mass flowrates of unit A products can be estimated. However, around node B, a 3-product formula cannot be applied since only one metal assay has been performed. Unit B product stream flowrates are not estimable. Metal assays are not redundant and cannot be adjusted by statistical data reconciliation. When two metals are analyzed on each of the 6 streams: There are a total of 6 equations and 5 unknowns. The system is therefore globally redundant and estimable The streams around the hydrocyclone in the circuit (  Solids: Particle size: While this may seem that we could simply apply the two-product formula, it is always advisable to set up the solution starting from the basic balances. Attempting to remember formulae is an invitation to error. The estimates of By re-arranging we can write: This has the form of a linear equation passing through the origin (y=mx) where the slope is solids split The plot agrees with expectation and the solids split is therefore:  We could approach the problem in the same way as we did to solve for Thus the circulating load is 1.60, often quoted as a percentage, 160%. We will see that this is a typical value for a closed ball mill-cyclone circuit ( At the same time as the streams around the cyclone were sampled, the streams around the sump were also sampled, and sized. The measurement results for two size classes, +592 Determine the circulating load. The mass balance equations for the sump are: Solids: Particle size: and Combining equations to eliminate The estimates of Clearly now the data do agree more closely with the result in Feed to a thickener is 30% solids by weight and the underflow is 65% by weight. Calculate the recovery of water to the overflow We can approach the problem in two ways: 1, starting with a balance on the slurry, and 2, using dilution ratios. Using  Slurry: Solids: Water: Since  Feed solids: Feed water: From the ratio: Analogous expressions for water flowrate in the other streams can be written. It will be noticed that the expression for After substituting by the dilution ratios and re-arranging we obtain: Thus the two approaches give the same result, as must be the case. Solving we obtain: Referring to The problem could be set up starting with the slurry balance, but it lends itself to the dilution ratio approach. Using the same symbolism as in  Solids: Water:  giving: Solids split: By combining equations to eliminate This compares well with the estimate in  The water split to underflow, also referred to as water recovery to underflow, is an important parameter in modeling the performance of a cyclone ( A concentrator treats a feed of 2.0% metal. Compare the 95% confidence interval on the recovery for the following two conditions: Producing concentrate grading 40% metal and a tailings of 0.3% metal. Producing concentrate grading 2.2% metal and a tailings of 1.3% metal. Take that the relative standard deviation on the metal assays is 5% in both cases.  From  Solving for To insert the values of σ σ and σ Substituting gives (recalling that   That is: σ The standard deviation on the recovery is ±1.05% or an approximate 95% confidence interval of 2×1.05% or 2.1%. The recovery is best reported, therefore, as:  From  Solving for To insert the values of σ σ Substituting gives (recalling that That is: And the 95% confidence interval is: The much higher uncertainty in the  Computer control of mineral processing plants requires continuous measurement of such parameters. The development of real-time on-line sensors, such as flowmeters, density gauges, and chemical and particle size analyzers, has made important contributions to the rapid developments in this field since the early 1970s, as has the increasing availability and reliability of cheap microprocessors. Metallurgical accounting is an essential requirement for all mineral processing operations. It is used to determine the distribution and grade of the values in the various products of a concentrator in order to assess the economic efficiency of the plant ( Sampling the process is often not given the consideration or level of effort that it deserves: in fact, the experience is that plants are often designed and built with inadequate planning for subsequent sampling, this in spite of the importance of the resulting data on metallurgical accounting, process control or plant testing and trials ( In order to understand the sources of error that may occur when sampling the process, reference will be made to probability theory and classical statistics—the Some appropriate terminology aids in the discussion. The One of the pioneers of sampling theory was It is evident from Also known as composition heterogeneity, it is dependent on factors that relate to the individual particles collected in an increment, such as particle top size, size distribution, shape, density and proportion of mineral of interest, degree of liberation, and sample size, and is largely the basis for Pierre Gy’s well-known fundamental equations and  This is an important factor that needs to be understood about the system that is being sampled. The micrographs in Selecting the appropriate sample size is also key to minimizing composition variance as noted in A program of sampling plus mineralogical and liberation data are important components of establishing the appropriate sampling protocol in order to minimize composition variance. We will now examine the second main source of overall sampling error, the distribution variance. Also known as distribution heterogeneity, or All Sample increment collected is proportional to the mass throughput, that is, weighted average Randomized sequence of sampling to avoid any natural frequency within the sampling unit All particles have an equal probability of being sampled The ideal sampling model, sometimes referred to as the It is evident that, in order to establish a proper sampling protocol that minimizes composition and distribution error, information is needed about the total size of, and segregation present in, the sampling unit, the size distribution and mineralogy of the particles, approximate concentrations of the elements/minerals of interest, and the level of precision (i.e., acceptable error) required from the sampling.  Estimates of In his pioneering work in the 1950s and 60s Pierre         In the limit where Note that Use can be made of As noted, total error is comprised of additional components (to fundamental sampling error) so a rule of thumb is to at least double the sample mass indicated by the Gy relationship. The literature continues to debate the applicability of Gy’s formula ( If instead of the crushing product as in These are often conducted within the plant or process to assess metallurgical performance for benchmarking, comparison, or other process improvement-related purposes. The data collected in such sampling campaigns will, typically, be subjected to consistent (reproducible) coherent (in=out) unbiased with redundancy (extra data for the data reconciliation step) Extra process measurement and sampling locations (i.e., more than the minimum) and multiple element, mineral or size assays will provide data redundancy and improve the quality of the reconciled data. How many “cuts” to include in the sample or sampling increment, and for how long to conduct the sampling survey, are important considerations. Use is again made of the CLT, which states that the variance of the mean of  Sampling surveys need to be conducted for a sufficient length of time to allow all elements within the process volume (i.e., sampling unit) an equal chance of being sampled (the equi-probable sampling model). Defining the mean retention time of the process volume Once a sampling survey has been completed, a check of the process Sampling surveys are not all successful as there are many uncontrolled variables impacting a process, and as many as 30% to 50% may need to be rejected and rerun. It is far better to do so before time and resources have been committed to sample preparation and data analysis on a poor test run. Several guidelines have evolved to aid in sampling process streams ( The sample size collected should be at least 1000 times the mass of the largest individual particle; this may encounter a practical limit for material with a very large top size. The slot on the sampler should be at right angles to the material flow and at least 3x the width of the largest particle (minimum width 10 The sampler cross-cutting speed should not exceed 0.6 Sampler cutting edges need to be knife edge in design so that impacting particles are not biased toward either side. Installed sampling systems such as those required for metallurgical accounting, on-stream analysis or bulk materials handling need to be properly designed in terms of sample size and sampling frequency. The concepts of Proper sampling protocols go beyond simply good practice. There are international standards in place for the sampling of bulk materials such as coal, iron ore, precious metals and metalliferous concentrates, established by the International Standards Organization (ISO, website Recalling the ideal sampling model previously introduced, the concept that every particle or fluid element should  Linear samplers are the most common and preferred devices for intermittent sampling of solids and pulp streams at discharge points such as conveyor head pulleys and ends of pipe. Examples are shown in Non-probabilistic designs have found use in less critical applications where some amount of measurement bias is deemed acceptable. This includes, as examples, on-stream analyzer (OSA) and particle size analyzer (PSA) installations and belt sampling of solids for moisture content. They violate the ideal sampling model in that they do not cut the entire stream, so not every particle or fluid element has an equal chance of being sampled. Strong proponents of proper sampling practice such as An example of a gravity sampler for slurry flow in horizontal lines, often used for concentrates feeding OSA systems, is shown in Poppet samplers (not shown) use a pneumatically inserted sampling pipe to extract the sample from the center of the flow. This is a sampler design that deviates significantly from probabilistic sampling and should therefore be used for liquids and not slurry, if at all. A cross-belt sampler ( Sampling systems for pulp and solids will typically require primary, secondary and possibly tertiary sampling stages. Manual or automatic (e.g., using small pumps) in-pulp sampling is a non-probabilistic process and should not be used for metallurgical accounting purposes, but is sometimes necessary for plant surveys and pulp density sampling. An example of a manual in-pulp device is shown in Traditional methods for manually extracting a sample increment, such as grab sampling, use of a riffle box, and coning, are inherently non-probabilistic and should be avoided. The caveat being that they may be acceptable if the method is used to treat the entire lot of material, On-line analysis enables a change of quality to be detected and corrected rapidly and continuously, obviating the delays involved in off-line laboratory testing. This method also frees skilled staff for more productive work than testing of routine samples. The field has been comprehensively reviewed elsewhere ( Basically, on-line element analysis consists of a source of radiation that is absorbed by the sample and causes it to give off fluorescent response radiation characteristic of each element. This enters a detector that generates a quantitative output signal as a result of measuring the characteristic radiation of one element from the sample. Through calibration, the detector output signal is used to obtain an assay value that can be used for process control ( The benefits of continuous analysis of process streams in mineral processing plants led to the development in the early 1960s of devices for X-ray fluorescence (XRF) analysis of flowing slurry streams. The technique exploits the emission of characteristic “secondary” (or fluorescent) X-rays generated when an element is excited by high energy X-rays or gamma rays. From a calibration the intensity of the emission gives element content. Two methods of on-line X-ray fluorescence analysis are centralized X-ray (on-stream) and in-stream probe systems. Centralized on-stream analysis employs a single high-energy excitation source for analysis of several slurry samples delivered to a central location where the equipment is installed. In-stream analysis employs sensors installed in, or near, the slurry stream, and sample excitation is carried out with convenient low-energy sources, usually radioactive isotopes ( One of the major problems in on-stream X-ray analysis is ensuring that the samples presented to the radiation are representative of the bulk, and that the response radiation is obtained from a representative fraction of this sample. The exciting radiation interacts with the slurry by first passing through a thin plastic film window and then penetrating the sample which is in contact with this window. Response radiation takes the opposite path back to the detector. Most of the radiation is absorbed in a few millimeters depth of the sample, so that the layer of slurry in immediate contact with the window has the greatest influence on the assays produced. Accuracy and reliability depend on this very thin layer being representative of the bulk material. Segregation in the slurry can take place at the window due to flow patterns resulting from the presence of the window surface. This can be eliminated by the use of high turbulence flow cells in the centralized X-ray analyzer. Centralized analysis is usually installed in large plants, requiring continuous monitoring of many different pulp streams, whereas smaller plants with fewer streams may incorporate probes on the basis of lower capital investment. Perhaps the most widely known centralized analyzer is the one developed by Outotec, the Courier 300 system ( The measurement sequence is fully programmable. Critical streams can be measured more frequently and more measurement time can be used for the low grade tailings streams. The switching time between samples is used for internal reference measurements, which are used for monitoring and automatic drift compensation. The probe in-stream system uses an isotope as excitation source. These probes can be single element or multi-element (up to 8, plus % solids). Accuracy is improved by using a well-stirred tank of slurry (analysis zone). Combined with solid state cryogenic (liquid N X-ray fluorescence analysis is not effective on light elements. The Outotec Courier 8 SL uses laser-induced breakdown spectroscopy (LIBS) to measure both light and heavy elements. Since light elements are often associated with gangue this analyzer permits impurity content to be tracked. Using a technique known as prompt gamma neutron activation analysis (PGNAA), the GEOSCAN-M (M for minerals applications) is an on-belt, non-contact elemental analysis system for monitoring bulk materials ( Laser induced breakdown spectroscopy (LIBS) is also used for on-belt analysis, supplied by Laser Distance Spectrometry (LDS, Rather than elemental analysis, there is a case to measure actual mineral content as it is the minerals that are being separated. The analysis of phase can be broadly divided into two types: 1. elemental analysis coupled with mineralogical knowledge and mass balancing; and 2. direct determination using diffraction or spectroscopy. The first is essentially adapting the techniques described in Of the second category X-ray diffraction (XRD) is the common approach and on-line applications are being explored ( The various technologies being developed to identify phases in feeds to ore sorters are another source of on-line measurements. For example, The operating principle is based on the concept that when a material is subjected to irradiation by X-rays, a portion A number of analyzers have been designed, and a typical one is shown in A key sensor required for the development of an effective method of controlling coal flotation is one which can measure the ash content of coal slurries. Production units have been manufactured and installed in coal preparation plants ( Measuring the size of coarse ore particles on conveyor belts or fine particles in slurries can now be done on-line with appropriate instrumentation. This is covered in Many schemes are used for determination of the tonnage of ore delivered to or passing through different sections of a mill. The general trend is toward weighing materials on the move. The predominant advantage of continuous weighing over batch weighing is its ability to handle large tonnages without interrupting the material flow. The accuracy and reliability of continuous-weighing equipment have improved greatly over the years. However, static weighing equipment is still used for many applications because of its greater accuracy. Belt scales, or It is the dry solids flowrate that is required. Moisture can be measured automatically (see The drying temperature should not be so high that breakdown of the minerals, either physically or chemically, occurs. Sulfide minerals are particularly prone to Periodic testing of the weightometer can be made either by passing known weights over it or by causing a length of heavy roller chain to trail from an anchorage over the suspended section while the empty belt is running. Many concentrators use one master weight only, and in the case of a weightometer this will probably be located at some convenient point between the crushing and the grinding sections. The conveyor feeding the fine-ore bins is often selected, as this normally contains the total ore feed of the plant. Weighing the concentrates is usually carried out after dewatering, before the material leaves the plant. Weighbridges can be used for material in wagons, trucks, or ore cars. They may require the services of an operator, who balances the load on the scale beam and notes the weight on a suitable form. After tipping the load, the tare (empty) weight of the truck must be determined. This method gives results within 0.5% error, assuming that the operator has balanced the load carefully and noted the result accurately. With recording scales, the operator merely balances the load, then turns a screw which automatically records the weight. Modern scales weigh a train of ore automatically as it passes over the platform, which removes the chance of human error except for occasional standardization. Sampling must be carried out at the same time for moisture determination. Assay samples should be taken, whenever possible, from the moving stream of material, as described earlier, before loading the material into the truck. Tailings weights are rarely, if ever, measured. They are calculated from the difference in feed and concentrate weights. Accurate sampling of tailings is essential. Mass balance techniques are now routinely used to estimate flows (see By combining slurry flowrate and % solids ( The operating principle of the magnetic flowmeter ( Two types of ultrasonic flowmeters are in common use. The first relies on reflection of an ultrasonic signal by discontinuities (particles or bubbles) into a transmitter/receiver ultrasonic transducer. The reflected signal exhibits a change in frequency due to the Doppler Effect that is proportional to the flow velocity; these instruments are commonly called “Doppler flow meters”. As the transducer can be attached to the outside of a suitable pipe section, these meters can be portable. The second type of meter uses timed pulses across a diagonal path. These meters depend only on geometry and timing accuracy. Hence they can offer high precision with minimal calibration. These operate by using an array of sensors and passive sonar processing algorithms to detect, track, and measure the mean velocities of coherent disturbances traveling in the axial direction of a pipe ( The velocities are determined as follows: as each group of disturbances pass under a sensor in the array, a signal unique to that group of disturbances is detected by the sensor. In turbulent flow, this signal is generated by the turbulent eddies which exert a miniscule stress on the interior of the pipe wall as they pass under the sensor location. The stress strains the pipe wall and thus the sensor element which is tightly coupled to the pipe wall. Each sensor element converts this strain into a unique electrical signal that is characteristic of the size and energy of the coherent disturbance. By tracking the unique signal of each group of coherent disturbances through the array of sensors, the time for their passage through the array can be determined. The array length is fixed, therefore the passage time is inversely proportional to the velocity. Taking this velocity and the known inner diameter of the pipe, the flow meter calculates and outputs the volumetric flowrate. This technology accurately measures multiphase fluids (any combination of liquid, solids and gas bubbles), on almost any pipe material including multilayer pipes, in the presence of scale buildup, with conductive or non-conductive fluids, and with magnetic or non-magnetic solids. In addition, the velocity of the acoustic waves, which is also measured, can be used to determine the gas void fraction of bubbles in the pipe. The gas void fraction is the total volume occupied by gas bubbles, typically air bubbles, divided by the interior volume of the pipe. When gas bubbles are present, this measurement is necessary to provide a true non-aerated volumetric flowrate and to compensate for the effect of entrained gas bubbles on density measurements from Coriolis meters or nuclear density meters. This principle can be used to determine void fraction, or The density of the slurry is measured automatically and continuously in the nucleonic density gauge ( The mass-flow unit integrates the rate of flow provided by the flowmeter and the pulp density to yield a continuous record of tonnage of dry solids passing through the pipe, given that the specific gravity of the solids comprising the ore stream is known. The method offers a reliable means of weighing the ore stream and removes chance of operator error and errors due to moisture sampling. Another advantage is that accurate sampling points, such as poppet valves ( From the grinding stage onward, most mineral processing operations are carried out on slurry streams, the water and solids mixture being transported through the circuit via pumps and pipelines. As far as the mineral processor is concerned, the water is acting as a transport medium, such that the If the volumetric flowrate is not excessive, it can be measured by diverting the stream of pulp into a suitable container for a measured period of time. The ratio of volume collected to time gives the flowrate of pulp. This method is ideal for most laboratory and pilot scale operations, but is impractical for large-scale operations, where it is usually necessary to measure the flowrate by online instrumentation. Volumetric flowrate is important in calculating retention times in processes. For instance, if 120 Retention time calculations sometimes have to take other factors into account. For example, to calculate the retention time in a flotation cell the volume occupied by the air (the gas holdup) needs to be deducted from the tank volume (see Slurry, or pulp, density is most easily measured in terms of weight of pulp per unit volume. Typical units are kg m Small flowstreams can be diverted into a container of known volume, which is then weighed to give slurry density directly. This is probably the most common method for routine assessment of plant performance, and is facilitated by using a density can of known volume which, when filled, is weighed on a specially graduated balance giving direct reading of pulp density. The composition of a slurry is often quoted as the % solids by weight (100–% moisture), and can be determined by sampling the slurry, weighing, drying and reweighing, and comparing wet and dry weights ( Wash the density bottle with acetone to remove traces of grease. Dry at about 40°C. After cooling, weigh the bottle and stopper on a precision analytical balance, and record the weight, Thoroughly dry the sample to remove all moisture. Add about 5–10 Add double distilled water to the bottle until half-full. If appreciable “slimes” (minus 45 Place the density bottle in a desiccator to remove air entrained within the sample. This stage is essential to prevent a low reading. Evacuate the vessel for at least 2 Remove the density bottle from the desiccator, and top up with double distilled water (do not insert stopper at this stage). When close to the balance, insert the stopper and allow it to fall into the neck of the bottle under its own weight. Check that water has been displaced through the stopper, and wipe off excess water from the bottle. Record the weight, Wash the sample out of the bottle. Refill the bottle with double distilled water, and repeat procedure 9. Record the weight, Record the temperature of the water used, as temperature correction is essential for accurate results. The density of the solids ( Knowing the densities of the pulp and dry solids, the % solids by weight can be calculated. Since pulp density is mass of slurry divided by volume of slurry, then for unit mass of slurry of Assigning water a density of 1000 Having measured the slurry volumetric flowrate ( The computations are illustrated by examples ( In some cases it is necessary to know the % solids by volume, a parameter, for example, sometimes used in mathematical models of unit processes: Also of use in milling calculations is the ratio of the weight of water to the weight of solids in the slurry, or the This is particularly important as the product of dilution ratio and weight of solids in the pulp is equal to the weight of water in the pulp (see also Control engineering in mineral processing continues to grow as a result of more demanding conditions such as low grade ores, economic changes (including reduced tolerance to risk), and ever more stringent environmental regulations, among others. These have motivated technological developments on several fronts such as: Advances in robust sensor technology. On-line sensors such as flowmeters, density gauges, and particle size analyzers have been successfully used in grinding circuit control. Machine vision, a non-invasive Other important sensors are pH meters, level and pressure transducers, all of which provide a signal related to the measurement of the particular process variable. This allows the final control elements, such as servo valves, variable speed motors, and pumps, to manipulate the process variables based on signals from the controllers. These sensors and final control elements are used in many industries besides the minerals industry, and are described elsewhere ( Advances in microprocessor and computer technology. These have led to the development of more powerful Distributed Control Systems (DCS) equipped with user friendly software applications that have facilitated process supervision and implementation of advanced control strategies. In addition, DCS capabilities combined with the development of intelligent sensors have allowed integration of automation tasks such as sensor configuration and fault detection. More thorough knowledge of process behavior. This has led to more reliable mathematical models of various important process units that can be used to evaluate control strategies under different simulated conditions ( Increasing use of large units, notably large grinding mills and flotation cells. This has reduced the amount of instrumentation and the number of control loops to be implemented. At the same time, however, this has increased the demands on process control as poor performance of any of these large pieces of equipment will have a significant detrimental impact on overall process performance. Financial models have been developed for the calculation of costs and benefits of the installation of automatic control systems ( The concepts, terminology and practice of process control in mineral processing have been comprehensively reviewed by The overall objective of a process control system is to maximize economic profit while respecting a set of constraints such as safe operation, environmental regulations, equipment limitations, and product specifications. The complexity of mineral processing plants, which exhibit multiple interacting nonlinear processes affected by unmeasured disturbances, makes the problem of optimizing the whole plant operation cumbersome, if not intractable. In order to tackle this problem a At the lowest level of the hierarchical control system is the i The The most widely used regulatory control algorithm is the Proportional plus Integral plus Derivative (PID) controller. It can be found as a standard function block in DCS and PLC systems and also in its stand-alone single station version. There are several types of algorithms available. The standard (also known as non-interacting) algorithm has the following form: Besides tuning, there are other issues the control engineer must deal with when implementing a PID controller. For example, if for some reason the closed-loop is broken, then the controller will no longer act upon the process and the tracking error can be different from zero. The integral part of the controller, then, will increase without having any effect on the process variable. Consequently, when the closed-loop is restored the control output will be too large and it will take a while for the controller to recover. This issue is known as There are situations when a closed-loop system becomes open-loop: (1) the controller is switched to manual mode, and (2) the final control element, such as a valve, saturates (i.e., becomes fully open or fully closed). A simple model of a valve, for instance, is that it is fully open when control action is larger than 100% (e.g., >20 There are some cases where PID controllers are not sufficient to produce the target performance. In those cases,  A disadvantage of feedback control is that it compensates for disturbances once they have already affected the process variable, that is, once an error has been produced. A typical application of a type of feedforward control is Processes that exhibit large transport delays relative to their dynamics can seriously limit the achievable performance of any feedback control, regardless of the complexity of the controller. In those cases, PID control performance can be improved by augmenting with a predictor, that is, a mathematical model of the process without the transport delay. This strategy is known as “Smith predictor controller” and it has been successfully applied to improve control performance of solid mass flowrate for SAG mills when manipulating the feed to a conveyor belt ( Changes in raw material characteristics and/or changes in the operating conditions mean any mathematical model used for controller synthesis may no longer reflect the actual process dynamic behavior. In those cases a control system that modifies controller parameters based on the mismatch between process model and actual process behavior is known as adaptive control ( The control strategy for the grinding section shown in MPC translates a control problem into a dynamic optimization problem over a finite future horizon. A discrete mathematical model is used to generate predictions of the future evolution of the process variable based on current information, future control actions, and assumptions about the future behavior of disturbances. The solution to this optimization problem is a sequence of control moves. Due to the uncertainty in the process model and the disturbances, only the first calculated optimal control move is implemented and the optimization problem is solved again based on new arriving information. This way, the optimization horizon is receding and thus a feedback mechanism is incorporated. To illustrate MPC, consider a SISO discrete model of the process represented by a difference equation where This model can be used recursively to obtain predictions of the process variable An optimization problem can then be formulated: find the sequence of future control moves Note that the solution to the optimization problem will be a function of the future values of the disturbance, which are not known in advance. There are different strategies to deal with this issue, a simple way being to assume that the disturbance is constant over the time horizon and takes the same value as its current value In cases where the identification and maintenance of good mathematical models for the application of MPC technology become unwieldy, expert systems, which do not require an algorithmic mathematical model, are an alternative ( Fuzzy-logic theory involves the development of an ordered set of fuzzy conditional statements, which provide an approximate description of a control strategy, such that modification and refinement of the controller can be performed without the need for special technical skills. These statements are of the form: if   Control systems based on rules with fuzzy logic support have steadily become more ambitious. The objective of this control layer is to determine the plant operating point that maximizes economic profit while satisfying a set of constraints. This can be formulated as an optimization problem where an objective function, a scalar positive real-valued function, provides a measure of performance. An example of a potential objective function is the Net Smelter Return (NSR) introduced in  To deal with complexity, plant optimization is usually broken down into multiple staged local optimizations coordinated by a supervisory system. For example, optimization of the grinding-flotation section can be decomposed into two optimization problems with specific objective functions and interfacing constraints ( If an accurate model of the process is not available, a model-free optimization strategy based on evolutionary operation EVOP ( There are cases where primary variables related to economic performance are not available or cannot be measured frequently enough for optimizing control purposes. This can arise for multiple reasons: for example, high cost instrumentation, long sampling measurement delays (e.g., on-stream analyzer, lab analysis), or because the primary variable is a calculated variable (e.g., recovery). In those cases the missing variable can be inferred from a mathematical model using secondary (surrogate) variables that are correlated to the primary variable and measured on-line in real time. These models are known as Models relating secondary to primary variables can be classified into three categories, depending whether the model structure and parameters have a physical meaning. Control computers are housed in dedicated rooms. It is recognized that many of the systems used within the control room need to be addressed ergonomically using a human, or a user-centered design approach: Remote monitoring and control are used in several industries, including some examples in mining: remote operation of mining equipment, such as haul trucks, for example, is standard at many sites. Arguably the main driver of remote “command centers” is the retention of highly skilled workforce in an urban setting. Integrated operations (iOps, Mass balancing or accounting for phase masses and component mass fractions by phase is critical to many responsibilities of the metallurgical team of any mineral or metal processing plant: troubleshooting/improving/assessing plant performance, monitoring/controlling plant operation, accounting/reporting metal production, and stock movements. Today, data acquisition and mass balancing are increasingly computerized and automated. Mass balancing techniques have evolved accordingly from the well-known 2-product formula to advanced computerized statistical algorithms that can handle the many situations and issues a metallurgist may have to address when performing mass balance calculations. The evolution is particularly significant in mass balancing for metal and production accounting and reporting needs. With tighter and more stringent accounting regulations, guidelines and good practices in metal accounting have been generated and published ( This section of Chapter 3 is an introduction to mass balancing and metal accounting, summarizing years of evolution from the basic well-known 2-product formula to computerized advanced statistical techniques. While the The node imbalance minimization method. The 2-step least squares minimization method. The generalized least squares minimization method. All three methods apply to mass balancing complex flow diagrams and provide a unique set of estimates in the presence of an excess of data. The For sake of simplicity, let us start with the 2-product formula before introducing the If Assuming Now assuming From the above results, several performance indicators are typically calculated. The The metal The simplicity of the method and resulting equations and the limited amount of data required for providing information on the process performance and efficiency account for the success of the 2-product formula over the years ( Components other than metal assays can be used, including particle size and percent solids data (see By extension of the 2-product formula, the To be solvable the number of required assayed metals depends on While the 2-product formula remains simple, the formula complexity increases very quickly with The 2-product formula calculates the values of Assuming covariance terms can be neglected, the variance of a function is obtained from its first derivatives: Applying In the above equations, the difference Applying This equation clearly shows that the calculated recovery variance is also strongly dependent on the process unit separation efficiency for the metal of interest ( From this numerical example, it is evident that: a) the stream samples that contribute most to the recovery variance are those with the lower assay values (feed and tailings); b) the variance of the least separated metal (Cu) is much higher than the variance of the most separated metal (Zn); and c) when using the 2-product formula, the accuracy of the recovery value comes from the feed and tailings stream assay accuracies rather than from the concentrate. There is a challenge here since these are not necessarily the easiest samples to collect. Worked examples estimating the standard deviation on recovery are given in The  Should the measured assays be free of measurement errors, the solid split values would be identical since, in theory (and reality), there exists only one solid split value. Obviously, the Fe assay is erroneous on at least one of the 3 streams, but it is impossible to identify which one. If each of the Zn and the Cu assays are taken individually, it is not possible to ascertain that the Zn assay or the Cu assay suffers error on one of the 3 samples. It is only because at least 2 metals have been assayed that we can ascertain the presence of measurement errors.  Since an excess of data reveals measurement errors, there is a need for a mass balancing method which can deal with data in excess. Another limitation of the In the process flow diagram of Applying the 2-product formula on process unit A, it is possible to calculate the flowrate of each of the 2 outgoing streams. Considering that the flowrate of the feed stream to process unit B is now known, the 2-product formula provides values of the 2 outgoing stream flowrates of process unit B. As a result of the above calculations, we now have an excess of data for process unit C, since the only unknown flowrate is for the outgoing stream. Consequently the 2-product formula fails to complete the mass balance calculation. It is worth noticing that selecting any other path in the applications of the 2-product formula would have ended up in the same situation: an excess of data preventing the completion of the mass balance calculation. There are various ways to address this kind of issue and to complete the mass balance, but the point is that the The We have seen, however, that the For all the above reasons, the In the Node Imbalance Minimization method, the mass conservation equations are written considering the deviation to rigorous mass conservation due to measurement errors ( In a given mass balance, the total number of mass conservation equations is therefore a function of the number of process units and metals assayed. For the flow diagram of Typically there are more equations than unknowns and, as a consequence, there is not a unique solution that satisfies all equations. The best mass flowrate estimates, In the above equation, The calculation of these best estimates requires the calculation of the derivatives of where The node imbalance minimization method solves two issues with the It handles data in excess It works for more than one process unit at a time However, the method exhibits 2 limitations: It is sensitive to measurement errors as can be seen in It only provides estimates of the non-measured solid flowrates; measured flowrate values and assay values are not adjusted An extension of the method aims to decrease the influence of bad measurements. The criterion node imbalance terms are weighted according to the presence or absence of gross measurement errors. The 2-step least squares minimization method proposes a way to adjust and correct metal assays for the measurement errors that affect them ( In the node imbalance method, the deviations to the mass conservation of solids and of metals are considered all together. The obtained estimated flowrate values minimize the deviations to all the mass conservation equations. In the 2-step least squares method, flowrate values that rigorously verify the mass conservation equations of solids and, at the time, minimize the deviations to the mass conservation equations of metals, are first estimated. In a second step, corrected values of metal assays that rigorously verify the mass conservation equations of metals are estimated. To achieve, taking The general problem can now be stated as: find the best estimate of By comparison with the node imbalance minimization method, the criterion only contains the node imbalances for metals and the search variables are a set of independent relative solid flowrates. Similar to the node imbalance minimization method, the calculation of the best estimates of the independent relative solid flowrates requires the calculation of the derivatives of Having determined In a second step, it is possible to adjust the metal assay measured values ( The problem is now to find the values of The problem is solved by calculating the derivatives of The 2-step least squares minimization method has the advantage over the previous methods of providing adjusted assay values that verify the mass conservation equations. The method remains simple and can easily be programmed (e.g., using Excel Solver) for simple flow diagrams. Although presenting significant improvements over the previous methods, the 2-step method is not mathematically optimal since the mass flowrates are estimated from measured and therefore erroneous metal assay values. Measurement errors are directly propagated to the estimated flowrate values. In other words, the reliability of the estimated flowrate values depends strongly on the reliability of the measured assay values. The equations could, however, be modified to include weighting factors with the objective of decreasing the influence of poor assays. The generalized least squares minimization method proposes a way to estimate flowrate values and adjust metal assay values in a single step, all together at the same time ( The mass conservation equations are written for the theoretical values of the process variables. The process variable measured values carry measurement errors and hence do not verify the mass conservation equations. It is assumed that the measured values are unbiased, uncorrelated to the other measured values and belong to Normal distributions, N(µ,σ). The balances can be expressed as follows: It is evident that the best estimates must obey the mass conservation equations while minimizing the generalized least squares criterion As seen previously, the problem is solved by calculating the derivatives of In While The formulation of the generalized least squares minimization method enables resolving complex mass balance problems. The Lagrange criterion consists of 2 types of terms: the weighted adjustments and the mass conservation constraints. The criterion can easily be extended to include various types of measurements and mass conservation equations. The whole set of mass conservation equations that apply to a given data set and mass balance problem is called the Conservation of slurry mass flowrates Conservation of solid phase mass flowrates Conservation of liquid phase mass flowrates (for a water balance, for instance) Conservation of solid to liquid ratios Conservation of components in the solid phase (e.g., particle size, liberation class) Conservation of components in the liquid phase (in leaching plants, for instance) There are also additional mass conservation constraints such as: The completeness constraint of size or density distributions (the sum of all mass fractions must be equal to 1 for each size distribution) Metal assays by size fraction of size distributions The coherency constraints between the reconstituted metal assays from assay-by-sizes and the sample metal assays All these constraint types can be handled and processed by the Lagrangian of the generalized least squares minimization algorithm. As mentioned, all the necessary mass conservation equations for a given data set constitute the mass balance model. It is convenient to define the mass balance model using the concept of networks. Indeed, the structure and the number of required mass conservation equations depend on the process flow diagram and the measurement types. Therefore, it is convenient to define a network type by process variable type, knowing that the structure of mass conservation equations for solid flowrates ( Not only does a network type mean a mass conservation equation type, but it also expresses the flow of the mass of interest (solids, liquids, metals…). For the 2-product process unit of For the more complex flow diagram of Assuming a fourth metal, Au for example, has been assayed on the main Feed, main Tailings and main Concentrate streams only, then a third network should be developed for Au as shown in Networks are conveniently represented using a matrix ( Each row represents a node; each column stands for a stream. In the generalized least square minimization method, weighting factors prevent large adjustments of trusted measurements and, on the contrary, facilitate large adjustments of poorly measured process variables. Setting the weighting factors is the main challenge of the method ( Assuming a measure obeys to a Normal distribution, the distribution variance is a measure of the confidence we have in the measure itself. Hence, a measure we trust exhibits a small variance, and the variance increases when the confidence decreases. The variance should not be confused with the process variable variation, which also includes the process variation: the variance is the representation of the measurement error only. Measurement errors come from various sources and are of different kinds. Theories and practical guidelines have been developed to understand the phenomenon and hence render possible the minimization of such errors as discussed in Errors fall under two main categories: systematic errors or biases, and random errors. Systematic errors are difficult to detect and identify. First, they can only be suspected over time by nature and definition. Second, a Measurement errors can also be characterized by their amplitude, small and gross errors, and by the frequency distribution of the measure signal over time: low frequency, high frequency or cyclical. Although there is no definition for a gross error, gross errors are statistically barely probable. As such, if a gross error is suspected it should be corrected before the statistical adjustment is performed. Typically, gross errors have a human source or result from a malfunctioning instrument. Measurement errors originate from two main sources: sampling errors, and analysis errors. Sampling errors mainly result from the heterogeneity of the material to sample and the difficulty in collecting a representative sample; analysis errors result from the difficulty to analyze sample compositions. The lack of efficiency of the instruments used for measuring process variables also contributes to measurement errors. Analysis errors can be measured in the laboratory and therefore the variance of the analysis error is estimable. Sampling errors are tricky to estimate and one can only typically classify collected samples from the easiest to the more difficult to collect. Assuming systematic and gross errors have been eliminated, then purely random measurement errors can be modeled using 2 functions: one that represents the sampling error and the other one that represents the analysis error. The easiest model is the so-called multiplicative error model where the analysis error variance The multiplicative error model can serve as a basis to develop more advanced error models. The remaining unknown, once a mass balance is obtained for a given set of measured process variables and measurement errors, is: how trustable are the mass balance results or is it possible to determine a confidence interval for each variable estimated by mass balance? That is the objective of the sensitivity analysis. Assuming systematic and gross errors have been eliminated, and measurement errors are random and obey to Normal distributions, there are two main ways to determine the variance of the estimated variable values (stream flowrates and their composition): Monte Carlo simulation, and error propagation calculation. Each method has its own advantages and disadvantages; each method provides an estimate of the variance of the adjusted or estimated variables. In the Monte Carlo simulation approach ( In the error propagation approach ( From a set of measured process variables and their variances, therefore, it is possible to determine a set of reconciled values and their variances: It can be demonstrated that the variance of the adjusted values is less than the variance of the measured values. While the variance of the measured values contributes the most to the variance of the adjusted values, the reduction in the variance values results from the statistical information provided. The data redundancy and the topology of the mass conservation networks are the main contributors to the statistical content of the information provided for the mass balance calculation. We have seen with the In such a case, there are just enough data to calculate the unknowns: no excess of data, that is, no data It follows from the definition that the measured value is itself an estimate of the variable value. Other estimates can be obtained by calculation using other measured variables and mass conservation equations. With the What are the factors influencing estimability and redundancy? Obviously, from It is worth noting that analyzing additional stream components does not necessarily increase redundancy and enable estimability. A component that is not separated in the process unit has the same or almost same concentration in each stream sample. The associated mass conservation equation is then similar (collinear) to the mass conservation of solids preventing the whole set of equations to be solved. Estimability and redundancy analyses are complex analyses that are better performed with mathematical algorithm. However, the mathematics is too complex to be presented here. It might be tempting to use spreadsheets to compute mass balances. Indeed, spreadsheets offer most of the features required to easily develop and solve a mass balance problem. However, spreadsheets are error prone, expensive to troubleshoot and maintain over time, and become very quickly limited to fulfill the needs and requirements of complex mass balances and their statistics ( There exist several providers of computer programs for mass balance calculations for a variety of prices. While all the features of an advanced solution may not be required for a given type of application (research, process survey, modeling and simulation project, on-line mass balancing for automatic control needs, rigorous production and metallurgical accounting), a good computer program should offer: An easy way to define, list and select mass conservation equations An easy way to define error models Visualization and validation tools to quickly detect gross errors A sensitivity analysis tool An estimability and redundancy analysis tool Statistical tools to validate the error models against the adjustments made Furthermore, depending on the mass balance types and application, the program should provide: Support for complete analyses (e.g., size distributions) Support for assay-by-size fractions Connectivity to external systems (archiving databases, Laboratory Information Management Systems) for automated data acquisition Automated detection and removal of gross errors Automated configuration of mass conservation models A reporting tool A dedicated database to support ancillary requirements Trend analysis Multi-mass balance capabilities Additional information can be found in One principal output of the sampling, assaying, mass balancing/data reconciliation exercise is the The calculations in the Wt (weight) column is from the solid (mass) split using Mass balancing based on metal assays has been described, and used to illustrate data reconciliation. As noted, components other than metal (or mineral) can be used for mass balancing. The use of particle size and per cent solids is illustrated; this is done without the associated data reconciliation, but remember, this is always necessary for accurate work. Many units, such as hydrocyclones and gravity separators, produce a degree of size separation and the particle size data can be used for mass balancing (   In A unit that gives a large difference in %solids between streams is a thickener ( See 