This paper addresses the problem of simulating a Gaussian random vector with zero mean and given variance–covariance matrix, without conditioning constraints. Variants of the Gibbs sampler algorithm are presented, based on the proposal by Galli and Gao, which do not require inverting the variance–covariance matrix and therefore allow considerable time savings. Numerical experiments are performed to check the accuracy of the algorithm and to determine implementation parameters (in particular, the updating and blocking strategies) that increase the rates of convergence and mixing.The simulation of Gaussian random vectors and random fields arises in many disciplines of the geosciences, in particular in the modeling of spatial and/or temporal phenomena. To this end, many algorithms have been developed, based on matrix decompositions, continuous or discrete Fourier transforms, addition of independent basic random fields, or Markov chain approaches ( This paper focuses on a specific algorithm of the latter family known as the Gibbs sampler ( In practice however, the Gibbs sampler suffers from computational limitations when the random vector to simulate has a large number of components (to fix ideas, more than a few thousands), as the determination of the best linear unbiased predictor requires solving large systems of equations. When the components of the random vector are associated with spatial coordinates, a simplification consists of calculating the predictor by using the closest conditioning components (i.e., using kriging in a moving neighborhood) ( In the following, we investigate a variant of the Gibbs sampler to simulate Gaussian random vectors, which does not require a moving neighborhood implementation. After presenting this variant and implementation aspects ( It is of interest to simulate a Gaussian random vector with  Initialize the simulation by an arbitrary vector of size For Select an index Solve the simple kriging system in order to predict Update the simulated random vector, by putting As To avoid the calculation of  Initialize the simulation by an arbitrary vector of size For Select an index Update the simulated random vector, by putting    Calculate As One can generalize this algorithm by updating several components of  Initialize the simulation by an arbitrary vector of size For Select Perform simple kriging in order to predict Update the simulated random vector, by putting     Calculate The sequence of random vectors { The transition kernel of the Markov chain is irreducible and aperiodic. The target distribution is invariant under the transition kernel. The irreducibility holds because any vector in  The expectation of The variance–covariance matrix of The kriging weights and variance–covariance matrix of kriging errors needed at Step (2b) can be obtained by considering the inverse of the variance–covariance matrix of For Step (2c) of the proposed algorithm, there exist several alternatives to define a The blocking strategy refers to the selection of random drawing without replacement ( selection of when the random vector The blocks associated with successive updates ( random sweep (RS): the index of the first block component is chosen uniformly in {1,…, deterministic updating (DU): the index of the first block component is increased by one unit (modulo reverse updating (RU): the index of the first block component is increased by one unit (first iteration), then decreased by one unit (next iteration), and so on; random permutation (RP): the index of the first block component follows a random permutation of {1,…, For the last three updating strategies (DU, RU, RP), each component of the random vector Many theoretical results on the rate of convergence of Markov chains are available in the literature ( Starting with the variance–covariance matrix Following Another question of interest is to determine, when approximate convergence is reached (say, for For Knowing the variance–covariance matrix The discrepancy between Whether the Gibbs sampler is well or poorly mixing can be determined by analyzing the rate of convergence of In the following, numerical experiments are undertaken in order to assess the convergence and mixing of the proposed Gibbs sampler variant (algorithm 3) and the sensitivity to the implementation parameters, for simulating a stationary standard Gaussian random field on a regular two-dimensional grid with 25×25 nodes. The grid size has been chosen in order to easily calculate the required covariance matrices In this subsection, the three blocking strategies shown in The experiments consider the following implementation settings: The covariance function is either an isotropic spherical model with range 25, or an isotropic Gaussian model with practical range 25 and 1% relative nugget effect. The algorithm uses a random permutation (RP) updating strategy. Therefore, each iteration consists of 625 block updates and each grid node is updated  The updating strategies mentioned in The fastest convergence of the standardized Frobenius norms In this subsection, a square blocking and a random permutation strategy are used, together with several block sizes: The following experiments have been realized by choosing a random permutation strategy and a square blocking with In all the presented cases, the standardized Frobenius norms Other updating and blocking strategies could be tested, such as considering blocks equal to one grid column or one grid row (in the present case, these would be blocks of Finally, since convergence and mixing occur after very few iterations, two approaches are equally valid to obtain independent realizations of the random vector The algorithm described in Simulation is restricted to regular grids in the three-dimensional space and subspaces. The spatial correlation of the target Gaussian random field is represented by a nested covariance model. The list of basic nested structures is available in subroutine COVA, although any new structure can be added in this subroutine. Each structure is coded as a covariance type ( The blocking strategy considers a rectangular parallelepiped block with Multiple independent realizations are obtained by running the Gibbs sampler several times, starting from a vector of zeros and using the same blocks at each update for all the realizations. Apart from memory and disk limitations, there is no restriction on the number of grid nodes, number of realizations to generate, number of iterations for the Gibbs sampler, type and number of nested structures in the covariance model. The program can be run by using an external parameter file (by default, GIBBS.PAR) ( Due to the regular grid structure, the covariance matrix As an illustration of the capabilities of program GIBBS, The convergence is corroborated by calculating the sample variograms along one grid axis of 50 realizations drawn independently: these variograms turn out to fluctuate around the theoretical model (a spherical variogram with range 25) with no significant bias, insofar as the average of the sample variograms almost perfectly matches the theoretical model ( This paper presented a variant of the Gibbs sampler algorithm to efficiently simulate a Gaussian random vector with zero mean and given variance–covariance matrix, avoiding the moving neighborhood problems that arise with the classical implementation. The numerical experiments that have been undertaken indicate that the rates of convergence and mixing of the proposed algorithm increase by considering a random permutation updating strategy and a blocking strategy (with rectangular parallelepiped blocks of adjacent components in the case of grid simulation) in order to update several vector components at each step, and that very few iterations are enough to get an accurate simulation. Also, convergence and mixing turn out to be faster with less structured covariance models (with higher nugget effect or less regular behavior near the origin). From a computational viewpoint, the proposed algorithm is still limited, since the entire simulated vectors must be kept in memory before writing out the results. In this sense, other algorithms may be preferable for simulating very large random vectors or random fields, such as the spectral and turning bands methods ( This research was partially funded by Let Accounting for the definition of Furthermore, if one assumes that   The algorithm described in Accounting for Eqs. This provides the following algorithm for directly simulating  Initialize the simulation by an arbitrary vector, e.g. a vector of zeros: For (a) Select (b) Calculate the inverse of (c) Update For Using the same notations as in Supplementary data associated with this article can be found in the online version at 