Geometallurgy as a multi-disciplinary field has been applied at various levels in different operations. By linking the ore performance in mineral beneficiation processes to the ore block model, it supports estimating the value of a block before it is mined. Efforts in the classification of the ore into geometallurgical classes have led to a better understanding of the entire value chain. While classification provides a convenient tool for forecasting and visualization purposes, it simplifies the actual complexity of an ore body. In mining and process planning, sequential decisions are made to maximize an objective function or equivalently minimize a regret function. Using available information from geology or metallurgical test work, an optimal strategy can be found using tools from the machine learning community.
                  In this study, a framework based on machine learning to maximize the use of such classifications for sequential decision-making is proposed. The concepts of reinforcement learning and bandit algorithms, offer powerful tools to explore and exploit different optimization strategies. In certain cases, theoretical guarantees about the performance of given methods can be obtained by regret bounds.
                  Based on existing models of a porphyry copper deposit and an iron ore deposit, this study presents a methodology and different available algorithms to maximize an objective function that depends on a high number of variables and in the presence of noise or uncertainty in the models. Different numerical experiments provide a basis for discussion and comparison to human decisions. The hypotheses relative to each algorithm are discussed in relation to the mineral processing models.Geometallurgical models are often defined as a set of tools to improve knowledge of an ore body by measuring relevant information for the metal extraction process ( The first objective of this paper is to study the dimensions and regularity of the available data, a second objective is to identify and present strategies as algorithms that offer a good performance in terms of recommendations for mineral processing. Finally, the implications of the investigated methods as well as their limitations for geometallurgy are presented. In a situation where several ore bodies are mined, and several processing options exist, as shown on The task to be solved is then to maximize an economic objective function through assigning an ore block at a time (sequentially) to the processing option that has the highest expected payoff while not knowing the performance of the other processes. At each discrete time step, the virtual plant operator (or software agent) can choose one of the processes in which the ore block will be processed. Once the block has been processed, a performance indicator relative to the chosen process (e.g. grade and recovery of the concentrate, presence or absence of a deleterious element) is observed by the plant manager (also called agent since it can act on the system). With time, the information about each process increases. The model describing this procedure is commonly called “multi-armed bandits” (MAB) setting, by analogy to a player in a casino who can choose between several slot machines equipped with levers called arms. If a bandit model is chosen, a natural aim is to develop a strategy (or policy) to maximize the overall performance in the long run. An alternative aim would be to identify the best process with high confidence. In both cases, the concepts of reinforcement learning are applied, i.e. the acquisition of new knowledge from the process (so-called exploration) and the subsequent improvement of decisions based on the existing knowledge (so-called exploitation). At the beginning, decisions must be taken without knowledge about any of the processes. The software agent must explore the different processes and, once the number of observations increases, exploit the correct process. The objectives used within process optimization and decision-making can be either of binary or continuous type, thus requiring different mathematical formulations for the description of the observations, see To describe the problem, the usual terms and notation from the bandits’ literature are used: At each time step t = 1, …, n the agent (plant manager) choses an arm (sends a mass of ore to one process or another) A The agent observes a reward X The policy ∏ of the agent is the decision rule that selects the choice A The best arm is noted a* and its mean reward is µ The overall performance of a policy can be measured by the cumulative regret R The challenge is to build “good” policies for a specific class of rewards. This has been approached by One of the early examples of designing an optimal strategy (or policy) in a sequential decision process can be found in the works of When different processing paths exist, two extreme strategies can be adopted: either randomly explore all the possible choices (pure exploration) or choose one process and always choose it (pure exploitation). In the first case, one can see that the optimal process will be chosen on average as much as any sub-optimal process. In the second case, either the best process is selected, and, in that case, it is a successful strategy, or another process is chosen, and the reward will always be suboptimal. Additionally, the probability to choose the optimal process from the beginning decreases with the number of available choices. This is referred to as the exploration-exploitation tradeoff. One can see that a reasonable strategy should combine exploration and exploitation. A naïve yet effective approach to the exploration-exploitation tradeoff is the epsilon-greedy strategy in which the arm with the highest mean is selected with probability (1-ε)/K and the others with a probability ε/K where K is the number of arms. Epsilon is often chosen around 0.1 which means that the probability of choosing the arm with the highest mean reward is 0.9 and 0.1 for the other arms. Epsilon-greedy strategies solve the tradeoff by assigning a probability for exploration and exploitation. Another family of policies are based on the construction of intervals of confidence on estimates of the mean of each arm. As an example, the algorithm UCB chooses the arm that has the highest upper confidence bound (hence UCB). The construction of the interval itself is detailed in Applications of the algorithms explained above to mineral processing and geometallurgy are possible in several cases presented in As presented in A stationary process has the same statistical distribution with time. In mineral processing plants, this represents stable operating conditions, i.e. a plant that does not display drastically different average behavior in time. An example of a stationary signal is a constant measurement affected by statistical noise. An example of a non-stationary process is a plant affected by seasonal factors like temperature ( Several of these cases can be solved using existing numerical tools, namely mineral processes simulators. Current software allows the simulation of stationary or non-stationary (also called dynamic) processes. Additionally, different scenarios can simulate changes in the feed composition and process parameters ( The cases where multiple processes exist are more complex (cases 1 to 4 in The data sets are describing two different industrial case studies: Iron oxide ore: an iron ore deposit for which the modal mineralogy and chemistry of the feed is available and where the ore has been classified into 8 geometallurgical classes or ore types as presented in The process consists of wet low intensity magnetic separation with grinding stages, see Copper sulphide ore: a porphyry copper for which the mineralogy and chemistry of the feed is available and where the ore has been classified into 15 geometallurgical classes or ore types as presented on The process consists of froth flotation (rougher, scavenger and cleaner) with regrinding and classification stages, as shown on MATLAB ( This case is an application of bandits to the iron ore example. The simplest situation is when the properties of the feed ore are stationary; in practice, this is achieved by using the mine scheduling software to stabilize the feed grade for example according to the iron and vanadium grade ( R R R R Since these distributions are unknown to the agent, different policies can be employed: A random selection of the process An epsilon-greedy strategy KL-UCB policy The random selection represents an exploration-only strategy, obviously this should not be the optimal method. The epsilon-greedy strategy is equivalent to a reasonable human strategy where the agent empirically estimates which process to exploit but is still exploring every now and then. UCB is a strategy based on the construction of intervals of confidence and selects the option that has the higher upper confidence bounds ( When processes are not stationary, classical UCB policies are not appropriate anymore since the rewards change with time ( R R R R The adapted policies in this case are A random selection of the process A sliding-window epsilon-greedy strategy (SW-Epsilon-greedy) A sliding-window UCB policy (SW-UCB) The SW-Epsilon-greedy strategy stands for the case of a periodical estimate of the best process on average. In this case, different ore types have been identified that behave differently during processing. Each ore type has a direct impact on the process performance and therefore the available information should be used by the policies. Bandit methods where side information is used are referred to as contextual bandits and an extensive literature is available on the subject ( Contextual data in geometallurgy can be a normalized vector that contains mineral grades, grindability indicators, textural properties or a scaled version of a combination of all these properties. Geometallurgical ore types are usually a classification of high-dimensional data. One method to reduce dimension for classification purposes is k-nearest neighbors. As an example, The kNN-UCB proceeds in several steps: For each arm (process), the number of neighbors k is chosen to minimize a measure of uncertainty Given a context x at time t, it selects the k neighbors around x A local upper confidence bound on the reward functions is computed The arm with the highest bound is selected Several hypotheses are needed to establish optimality bounds, i.e. hypotheses that suggest smooth and bounded reward functions. For the problem stated here, a simplified set of assumptions involves: Dimension assumption: the context data is supported by a d-dimensional manifold where d is smaller than the dimension of the context. This assumption means that the vectors that contain the mineralogy and grinding indicators can be projected on a surface of lower dimension than the vectors themselves as displayed on Margin assumption: the arms have rewards which are different enough Lipschitz assumption: for similar context vectors the rewards are similar. Another way to interpret the assumption is that the reward functions should be varying smoothly (in a continuous way). Rewards are bounded in [0,1] The application to the porphyry ore deposit is straightforward: a context is built by normalizing a vector that contains the pyrite grade p (wt%), the chalcopyrite grade c (wt%), the sulfur grade s (wt.%) and a grindability index g (kWh/t), giving the entire context vector denoted by x The agent can choose between the following rewards P(b, R R R R P is the recovery performance Q a measure for grindability performance The reward is then given by The selected policies in this case are: A random selection of the process A contextual epsilon-greedy strategy (C-Epsilon-greedy) A kNN-UCB strategy (kNN-UCB) This case is treated as a contextual variant of case 2, by adapting algorithms from the previous subsection and using a sliding time window. This represents seasonal variations in the performance in flotation. The models are modified accordingly R R R R The selected policies in this case are A random selection of the process A sliding-window contextual epsilon-greedy strategy (SW-C-Epsilon-greedy) A sliding-window KNN-UCB strategy (SW-KNN-UCB) For each case, numerical experiments have been conducted to compare the different strategies or policies. The cumulative regret is used as the main metric since it is representative of the optimality of a given policy. The process models were created in HSC Chemistry and calibrated with plant surveys. The policies and plots were implemented in MATLAB 2018b. Each experiment consisted of 50,000 blocks and was run 150 times due to the stochastic nature of the policies and intervals of confidence were calculated from the repeats. These are not to be confused with upper confidence bounds that are calculated inside the UCB algorithm during each round. The cumulative regret is defined as Where These algorithms can run on a recent laptop. As an example, on a laptop, 50 In this case, the benefits of having a policy are clear. Random choices lead to the highest regret, followed by the epsilon-greedy strategy. Policies based on confidence bounds perform best. In this case, each process presents temporal variations while the feed varies around a controlled average. When the performances of a process are not known in advance, especially when the variation cannot be predicted, computing the average will fail since the period of the variations is not known. While the period can be estimated, the presence of noise can make the estimation difficult if the amplitude is small. Moreover, any other change to the plant (maintenance or automatic control) could influence the estimates. Using sliding windows reduces the difficulty but still requires the window’s size to be calibrated.  Once contextual data is introduced, the optimal decision depends on the block. As seen before, a purely random strategy will fail here as well, but the difference between epsilon-greedy strategies and UCB is much larger ( To have a better view at the difference in behavior rather than the global one, it is useful to switch to a logarithmic y axis to see the order of magnitude of the improvement by using UCB, see This case is the most realistic one in which both the ore type and the process change in time. The proposed solution is to combine kNN-UCB with a sliding time window. Additionally, a variant of kNN-UCB that uses the Kullback-Leibler (KL) divergence is used ( The application of bandit algorithms seems to provide a solution for metallurgical process design and control using geometallurgical data. These methods do not only offer guarantees in terms of regret, but they also provide suggestions for action. Regret is useful to quantify optimality but for each case, at each step a decision with the probable highest reward is given. One can visualize the decisions in two different ways, either as a table that gives the best decision at each time step as displayed on A major advantage of UCB methods is that they provide confidence bounds that are adjusted at every time step. With time, the bounds become narrower which results in higher certainty about the choices proposed by the method. On Despite an overall good performance, the hypotheses on the models are strong. In the non-contextual case, theoretical results show that UCB strategies offer an optimal behavior but only in the case of exponential family distributions. As such, other parametric families or even non-parametric models offer no strong theoretical guarantees in terms of cumulative regret. For relatively simple cases, a good model of the process can be obtained but the general case is missing. The reduction from a full flowsheet model to a Bernoulli distribution is made simple by the existence of scenarios in most simulation software. The contextual case is delicate due to the hypotheses, especially the Lipschitz continuity assumption. This means that similar geometallurgical ore types will behave similarly in the process. This is justified if these ore types produce the same particle size distribution, the same liberation spectrum and the same particle’s geometry. If a sufficient number of tests have been done and if the mineralogy is well known, it can be assumed but it remains a hypothesis. A weaker hypothesis could be to use Hölder continuity instead which allows not so smooth function for rewards, but no guarantee has been presented in literature in that case. This would allow slightly different populations of particles, some noise in the context data or irregular process performance. Empirical results suggest that it could be enough or at least that the performance is like the Lipschitz case. In practice, a tool to characterize the regularity of a signal (for example the recovery or throughput of a concentrate in the plant) is based on wavelet leaders ( The existence of a lower dimensional manifold that supports the contextual data is another challenge. In general, the problem of finding the right representation for high-dimensional data is hard but in the case of ore physical properties, if the assumption of some form of geological continuity is accepted, then the existence of such a manifold is implied. In any case, one can plot the representation and check whether the assumption holds (see for instance Bandit problems formulation for decision support in geometallurgy is a promising approach due to several advantages. The main one is that a perfect knowledge of the process is not needed since in the stochastic setting, bandit algorithms perform well in face of uncertainty. Therefore, it can be used either on calibrated models with uncertainties, or directly on the process if a reward function can be obtain solely based on the measured performances. An additional advantage for contextual methods is the fact that each decision can be explained and motivated based on observations and available context. Experts could then determine whether their experience gives similar policies. The reward function also offers flexibility since one can construct a weighted sum of several parameters including environmental scores and energy consumption for example (this was already present in case 3 and 4 in which a grindability index was included in the calculation of reward functions). If several ore types are available and the problem is to blend the ores to obtain the best performances, one can use bandit algorithms to optimally choose between blends. In that case the different processes (or arms) will be the same flowsheet with the same operating parameters but different performances as a function of the ore type. This case can be further adapted to time-varying blends that depend on mine production. From an environmental point of view, if a life-cycle assessment score is calculated (for example the CO In practice, be it for full scale tests or simulated tests, the selection of the method is the same and is illustrated in The integration of such methods on a mining or processing site is straightforward if element grades and throughput are measured in the plant since it can be packed as an independent software package. As for now, these methods are not included in mineral circuit simulation software to the best knowledge of the authors but again in the case of HSC Chemistry, the data structure is appropriate and could be implemented. One limitation lies in the connection with the mining scheduling software since it is not always possible to have a choice in terms of blending or which material to feed at a certain point but even in the case of a single process it is a valuable tool to evaluate decisions. Limitations on theoretical guarantees on optimality can be an obstacle to the adoption of bandit algorithms for geometallurgy. However, the results presented in this study suggest that it always provides a better policy than pure random decisions (guided by other factors) or epsilon-greedy strategies that are comparable to human-based decisions (estimates of the mean for each arm and a low probability of exploration). One solution could be to use these policies and compare them against current policies over a limited time horizon. Cumulative regret offers a quantitative assessment of policies. As a conclusion, the proposed numerical methods provide good strategies for sequential resource allocation in geometallurgy and tools to evaluate these policies over time.  The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper. The authors wish to thank LKAB, Boliden Mineral AB and Outotec (Sweden) AB for their support in this study, Erdogan Kol for the interesting discussions about process simulation and flexibility. This work is part of PREP project funded by