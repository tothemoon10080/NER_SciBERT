Chemometric data analysis and modeling necessitate appropriate quality (accuracy and precision) with respect to what data represent, irrespective of whether they have been acquired in the field, in the laboratory, or in an industrial (process) setting. The following trinity must be recognized and respected in order for chemometric data modeling to succeed:
                     
                        1.
                        Data must be acquired by representative sampling, ensuring accuracy with respect to the lot to be represented by the samples and precision with respect to all sampling process errors.
                     
                     
                        2.
                        Data must have an appropriate analytical quality (usually the least of the problem as most analytical laboratories strive hard to be certified or accredited).
                     
                     
                        3.
                        Chemometric data models must respect a demand for reliable performance validation, for example, regarding prediction, classification, time forecasting.
                     
                  
               
               Without representative sampling, the entire chain of evidence versus the lot characteristics is flawed because of the ever-present heterogeneity of the material (at all scales); the same holds if the precision of analysis is inferior. It is not possible to correct for any of these deficiencies in the subsequent data analytical modeling. As analytical errors are typically one or two orders of magnitude smaller than the combined sampling errors, every aspect of ‘data quality’ in practice hinges overwhelmingly on the sampling, which is either fully representative (acceptable) or nonrepresentative (worthless). A brief discussion of the criteria to be met in order to satisfy these demands is presented from the realm of the theory of sampling (TOS). A working minimum of TOS is delineated for both zero-dimensional (0-D) and 1-D sampling (process sampling), including heterogeneity characterization, comprehension of eight sampling errors, and mastering of seven practical sampling unit operations (SUOs) with which to combat heterogeneity. The effect of nonrepresentative sampling shows up as an inherent sampling bias, which cannot be estimated and therefore never be corrected for, as well as an inflated sampling reproducibility (imprecision), which, if unnoticed, leads to large, tedious, but futile, chemometric data modeling efforts. The only scientific basis for reliable performance validation of chemometric data models rests with the concept of test set validation. Popular schemes for cross-validation all fail to achieve a reliable estimate of the prediction error (classification rate, forecasting reliability) and reproducibility in more than one singular data set, the training set, which holds no information from the future model use. However, in nearly all situations, sampling errors can be significantly reduced, indeed sampling biases can be eliminated, simply by respecting an authoritative set of rules and guidelines provided by TOS (but often there is great reluctance to accept this). A systematic approach for practical sampling is described, which is based on in-depth knowledge about the specific variability of any 0-D or 1-D lot. Selected practical examples illustrate the use of 0-D TOS and 1-D variography to design optimal sampling protocols for a variety of typical situations.Data must be acquired by representative sampling, ensuring accuracy with respect to the lot to be represented by the samples and precision with respect to all sampling process errors. Data must have an appropriate analytical quality (usually the least of the problem as most analytical laboratories strive hard to be certified or accredited). Chemometric data models must respect a demand for reliable performance validation, for example, regarding prediction, classification, time forecasting. general symbol for ‘analyte’ (component of interest) concentration of fragment/increment (grade) average lot concentration (grade) mass of unit grade of sampling volume sampling constant (material dependent) top particle diameter, also known as group with index average group mass individual heterogeneity contributions heterogeneity of unit heterogeneity of group lag parameter mean squared mass of fragment average fragment mass lot mass average unit mass number of fragments in lot number of group in lot number of units representativeness variance variogram function distance between unit pairs minimum distance between unit pairs  All naturally occurring materials are heterogeneous at all operative scales related to sampling, Any sampling process (following the tenets of TOS, or not) interacts with the heterogeneous material making up the lot. Because of this, sampling is far from trivial as all sampling procedures unavoidably will be affected by the heterogeneity of the lot material at all scales larger than the operative sampling unit volume/mass. In addition, the sampling process creates sampling errors of its own nature, due to noncompliance with the practical, mechanical, maintenance, and operative procedural tenets of TOS. There are always two main contributors to the total sampling error (TSE): the heterogeneity of the lot material and the sampling process itself. For stationary lots, this generates five principal types of sampling errors, to which must be added two errors specific for the dynamic case, that is, process sampling. Caveat: There exist two classes of materials that are exempt from the stringent principles and rules regarding naturally occurring materials delineated in this chapter, namely ‘fine’ or ‘pure’ chemicals, reagents, powders, and liquid solutions. Fine powders can be of uniform composition and sampling variability less than any predetermined threshold, say, 1% (rel.) or less, either because they have been manufactured for this specific purpose (production of reagent grade fine/pure chemicals inevitably includes the most thorough mixing conceivable) or because of exceedingly thorough comminution and mixing. Liquid solutions are of course very much easier to mix as they practically only show compositional heterogeneity at the molecular level. Thus, analytical chemistry often differentiates sampling of ‘homogeneous’ liquid solutions prepared by mixing chemicals in the laboratory. Although both of the above classes of materials may give rise to some comfort within the analytical laboratory, they are rare exceptions to the complete suite of material types in need of analytical characterization (qualitative, quantitative) where a substantial amount of sample preparation (especially representative mass reduction) is needed – so the analytical chemist is not exempt from the imperative of mastering the principles of TOS. Uniform materials are the only case in which the devastating sampling bias problems dealt with here are practically absent. Sampling of such materials does not give rise to specific problems and can be accomplished on a basis of conventional statistics, in which the precision follows an inverse square root of the number of samples, etc. There exist scores of statistical textbooks and numerous textbooks on analytical chemistry, which deal comprehensively with this ideal situation; references can be found in Chapters Heterogeneity can be divided into two fundamental parts: constitutional heterogeneity (CH) and distributional heterogeneity (DH). CH describes the heterogeneity dependent on the physical or chemical differences between the individual basic lot units, with TOS termed ‘fragments’ (‘grains’ would be a useful imaginary metaphor for ‘fragments’ in the following: mineral grains and seeds). Any given system (lot, geometry, material, state, grain-size distribution) will exhibit a CH, which can only be reduced by altering the physical state of the material, specifically only by comminution. CH plays its role at the scale of the individual fragments, grains, kernels, seeds, etc. The DH complements this issue by describing all aspects of heterogeneity dependent upon the spatial distribution in the lot as gauged by operative sampling units corresponding to the sampling volume/mass used (in a full analysis this is also ultimately related to CH Dependent on the purpose and scale of sampling (‘scoop size’), CH may be close to negligible, but it is never nil. Homogeneity is defined as the (theoretical) limiting case of zero heterogeneity. Indeed if such a thing as a homogeneous material did exist, sampling would not be needed – as all sampling errors would be zero, all ‘samples’ would be identical. Tche heterogeneity between individual fragments is of interest. TOS’s most fundamental theoretical achievement comes in the form of the heterogeneity contribution to the total heterogeneity of the lot, defined by focusing on an individual fragment: TOS characterizes all fragments according to the component of interest (the analyte, A), described by the proportion (or grade), This definition of heterogeneity contribution is dimensionless and hence can be used for any intensive unit used in characterizing the material, for example, concentration, %, size. Delineating the compositional deviations of each fragment, it also compensates for variation in the fragment masses; larger fragments result in a larger influence on the total heterogeneity than smaller ones. This viewpoint constitutes a major distinction from ‘classical statistics’ where all units contribute equally (with equal statistical mass). The total CH of the lot, CH For the above derivations as well as the one given below, whenever increment/sample masses are not identical, estimation of the total lot mass, Ascending one hierarchical scale level – from the scale of fragments, grains, etc. to the operative level of one sampling unit (sampling scoop) – one moves into the realm of DH of the lot, DH The DH for the entire lot can again be calculated as the variance of all group heterogeneity contributions: By observing that the sum of all (virtual) groups actually constitutes the physical lot in its entirety, it can be appreciated that DH DH Unlike CH TOS has much to say (all negative) regarding the universal futility of grab samples, which are never representative in practice against all realistic heterogeneous lots and materials. Grab samples are thus never reliable and accordingly must never be used. Full details can be found in any TOS literature. Full analysis of the phenomenon of heterogeneity CH Grouping (depends on the size (volume/mass) of the extracted increments) Segregation (depends on the spatial distribution of fragments in the lot). Both segregation and grouping can be quantified, and methods and equations are described in detail in the pertinent main literature, In order to extract samples from heterogeneous materials with sufficiently low variation, it is necessary to minimize DH Decreasing the size of the extracted increments, thereby increasing the number of increments (or increasing the sampling frequency) combined to form a given sample mass, Mixing/‘homogenizing’ the lot (reduces macroscale lot segregation). If these measures are insufficient for a given sampling process and total error specification, it is necessary to reduce the CH itself, which necessitates physical reduction of the fragment sizes, comminution (grinding or crushing), and/or increasing the total sample mass, It is important to note that the quantitative sampling effect of DH All analytical results are associated with some nonzero uncertainty, often taken to be the analytical uncertainty Often TAE is under strict control in the laboratory, and is usually of no significant concern in comparison to sampling, as TAE is always significantly smaller than the sum of errors stemming from sampling (TSE). In fact, TSE is very often 50–100 times larger than TAE; TSE has many sources. Indeed the primary objective of representative sampling is to identify, eliminate, or reduce all contributing sampling errors. The compound sampling variance, as delineated above, is specifically influenced by the heterogeneity of the material as well as the sampling procedure. Although much of this effort is to some extent under the control of the sampler, the part from CH is dependent on the material properties only. This error is termed the fundamental sampling error (FSE), as it cannot be altered for any given system (lot, geometry, material, state, size distribution); altering FSE necessitates physical intervention, crushing, comminution. On the contrary, the contribution from the spatial distribution of the material is not fixed and can be altered. This is dependent not only on the material characteristic itself but also on the sampling procedure and whether counteraction measures are invoked (e.g., mixing). The variation stemming from distribution heterogeneity is collected in the grouping and segregation error (GSE). The single rule to respect in regard to selecting smaller increments is that all possible extractions from the lot (all possible virtual increments) must have the same probability of being selected. This is called the fundamental sampling principle (FSP) – FSP must never be compromised, otherwise all possibilities of documenting accuracy of the sampling process are abandoned. FSP implies physical access to all geometrical units of the lot. TOS contains many practical guidelines of how to achieve compliance with FSP. TOS employs a strict terminology, in which all aspects of noncompliant sampling can be specifically named. Thus, TOS specifies as ‘correct’ only those features that will contribute toward the ultimate goal of being able to demonstrate representativeness of the particular sampling process employed, The sum of FSE and GSE is termed the ‘correct sampling errors’ (CSEs), as they are not due to erroneous sampling or wrong procedures; in fact, CSEs occur even when the sampling procedure is ‘correct’ (meaning representative), hence perhaps at first sight somewhat peculiar naming. Errors that are connected to erroneous sampling procedures are contrarily summed in the ISEs. ISEs comprise three parts: one stemming from not delineating correct increments from the lot, the second from not extracting exactly what was intended, and the third form of error is induced after the extraction of the increment (or sample). The increment delineation error (IDE) can be avoided by always selecting (delineating) an increment that completely covers the relevant dimensions of the lot, for instance a complete cross-sectional slice if the lot is a (very) long pile of material, or a ‘drill core’ to the very bottom of the layer(s) of interest if the lot is a 3-dimensional (3-D) volume or of a similar shape. The increment extraction error (IEE) arises when particles inside the delineated increment do not end up in the sample, for instance by bouncing or dusting, or if particles outside the delineated increment find their way into the sampling tool, contamination. It is normally stated that (only) particles or fragments with their center of gravity inside the delineated increment should become part of it. The last incorrect error arises when the sample is altered after extraction, for instance by absorbing moisture, by spillage, cross-contamination, or some similar phenomenon. Sample tampering and downright fraud is also a type of ‘error’, which is likewise collected under the term incorrect preparation error (IPE). All these errors can be minimized, and some can indeed be completely eliminated, by full understanding of the guidelines in TOS for ‘correct’, that is, representative, sampling: There are many practical and mechanical aspects of this issue, all relatively simple and almost trivial to implement, but only if they are properly recognized. The TOS literature deals with all these issues at great length, with a focus on explaining the connection between individual errors and how they can be minimized completely. The selected literature list below is sufficiently comprehensive for this introduction, but not complete; The above analysis of heterogeneity, types of sampling error, and criteria for practical representative sampling holds for all lots/types of materials that are stationary, that is, nonmoving. Although the physical and geometrical configuration of lots may vary, 3-D, 2-D, and 1-D, sampling from all nonmoving lots is governed by the above features. TOS recognizes this invariant issue by defining a generic, nonmoving type of lot, termed a 0-D lot ( As mentioned earlier, two additional errors are found when dealing with the dynamic case, that is, in process sampling. Very often the focus is on selecting the final sample mass way too early in the sampling process. Contrary to this erroneous objective, it is necessary, and far more beneficial, to focus only on how to make the primary sample be representative – even if this means procuring an ‘oversized’ sample, in order to overcome problems with heterogeneity that cannot be dealt with satisfactorily Any and all mass reduction steps (whether singular or in series) will generate sampling variation, no matter how well it is performed. Hence, it is only a matter of minimizing this inevitable contribution to the variation of the final result. Any mass reduction method must be in appliance with FSP, that is, all parts of the lot (or subsamples) must have an equal probability of ending up in the final sample (subsample). This means that a correct device or method must ensure complete randomness at the lowest possible scale, ideally at the fragment level, which is however very rarely obtained in practice (only with very small lots comprising relatively large fragments). An extensive benchmark study has been carried out in order to identify the optimal mass reduction principle(s). The mass reduction techniques and methods investigated cover Grab sampling (a single increment used as the sample). Fractional and alternate shoveling (dividing sequentially into two (alternate) or more piles, scoop by scoop). Riffle splitters, with varying number of chutes, chute width, feeding mechanism, dust minimizing systems. A widely used circular riffle splitter for seeds and grains (Boerner divider). Rotational dividers in which samples are distributed over radial chutes by a rotating feeding nozzle. An elaborate manual technique recommended in international seed and grain communities (the ‘spoon method’), which involves a rudimentary form of bed blending, followed by selecting a few spoonfuls in a composite sampling context. In Petersen The conclusions and recommendations regarding mass reduction are clear: It is evident that any method involving manual shoveling, grabbing, or similar simplistic selection of some material is by far the worse: grab sampling, fractional shoveling, alternate shoveling, or the ‘spoon method’. The simplest riffle splitting method forms a transition from these totally unacceptable methods to a suite of 12 acceptable methods. Optimal under all conditions are only riffle splitters or methods based on the same underlying principle (Boerner and rotating devices), namely that all material is distributed uniformly over an equal number of equally sized chutes facilitating a representative split into well-specified portions of the original material (lot). Standard riffle splitters end up with two identical 50% portions, necessitating a stepwise repeated procedure to achieve larger reduction ratios. This is easily evaded using the slightly more technical rotating ‘Vario’ dividers, where the total sample mass can be reduced to a lower proportion sample and a larger proportion (‘waste’ or bypass), as all chutes are used many times, yielding a large total number of effective chutes. The Vario device principle is comparable to using a static, but significantly cheaper, riffle splitter in sequence, but many more times over. Furthermore, it is crucial that the equipment is operated correctly. There are a number of practical rules to be respected when using any of the acceptable devices – neglecting one or more of these can end up in the entire mass reduction being biased, or result in unnecessary large variation. The complete benchmark report A 1-D object is a lot in which two dimensions are negligible in size, compared to the third, and where a distinct spatial or temporal correlation exists along the singular elongated dimension. A 1-D lot for example can appear as an ordered series of units from a production line (time or space) or as a flowing stream of material. The operative implication of this is that all increments must be in the form of complete, planar-parallel cross-stream cuts In order to sample a 1-D lot correctly, one of only three possible sampling schemes (systematic, stratified random, or random) must be selected, and further a set of rules regarding the design of equipment must be respected in order to have an unbiased sample with minimum sample variation. A variogram is used to characterize autocorrelation (1-D heterogeneity) of 1-D lots as a function of the distance between extracted units (increments). A variogram is superior in identifying trends (increasing/decreasing) and periodic cycles (often hidden to the naked eye) in process data. One-dimensional sampling requires some understanding of the nonrandom heterogeneity fluctuations along the extended dimension of the lot, that is, the hereogeneity between the extracted increments. Correct increments must be a discrete, complete planar-parallel cut across the entire stream of material, or similar extractions of material, chronologically ordered ( The heterogeneity contribution, The heterogeneity contribution, A short-range fluctuation component, describing the heterogeneity within that particular increment (actually all the 0-D sampling errors A long-range fluctuation contribution that describes the longer term trends in the process. A cyclic term that describe any periodic variation of the lot. To characterize the heterogeneity of a 1-D lot, the chronological order of the units must be included in the analysis; a variogram is the perfect tool. A variographic analysis requires a complete set of analytical results based on representative increments, all extracted equidistantly over a sufficiently long interval so as to cover the expected process variation well. Clearly, some first-hand knowledge of the process to be characterized/analyzed/sampled is advantageous, but Normally, 50–60 increments are considered minimum for a thorough variographic analysis. A dimensionless, relative lag parameter, If a number of increments, The variogram function, The practical interpretation of the resulting variograms is the most important step in a variograpghic analysis. The variogram level and form provide extensive information on the process variation captured. Normally, only three primary types of variograms are encountered: An increasing variogram (normal variogram shape). A flat variogram (no autocorrelation along the defining dimension). A periodic variogram. The above variograms are outlined in When the variogram type has been identified, information on optimized 1-D sampling can be derived. The increasing variogram ( All variograms are not defined for lag MPE includes several error components, FSE, GSE, TAE, and all ISEs, and hence is an appropriate measure of the absolute minimum error that can be expected in practice using the existing or the contemplated sampling scheme at hand. When the increasing variogram becomes more or less flat, the ‘sill’ of the variogram has been reached. The sill provides information on the expected maximum sampling variation if the existing autocorrelation is not taken into account. The ‘range’ of the variogram is found as the lag beyond which there is no autocorrelation. These primary characterizing variogram parameters are illustrated in If a significant periodicity is observed, the sampling frequency must never be similar! Also the specific sampling mode (random sampling (ra), systematic sampling (sy), and stratified random sampling (st)) becomes critically important; full details can be found in Gy, From the variographic experiment, a sampling plan can be derived (see Gy, Avoid extracting increments with a frequency coinciding with a period. Doing so will significantly underestimate the variation of the process. In general, the stratified random sampling mode (st) will always lead to optimal results (absolute lowest TSE), but systematic sampling (sy) will in many cases be marginally just as effective, and very often much easier to implement. Random sampling (ra) is never used in the process realm, Sampling with a frequency below the range will ensure that the process autocorrelation in effect reduces the sampling variation. The variogram is an effective tool to simulate alternative sampling strategies without making real-world experiments. The Use The data in this example are simulated, but patterned from many real-world examples and therefore realistic. A factory produces a powder mixture with three components in a continuous process. The normal sampling scheme implies extracting one increment of 1  The corresponding heterogeneity contributions of the individual increments can easily be calculated, as displayed in From the variogram in From the variogram, a minimum at This example only illustrates how to identify process behavior (trends/periods) from the variogram. A full analysis on how to identify optimal number of increments to form a composite sample, how to zoom in on an optimal sampling interval, and sampling strategy can be found in Petersen and Esbensen Variographic analysis, in the form of kriging and related spatial interpolation methods, is also used extensively in the science of Geostatistics as well as in geographical information systems (GIS). These aspects are essential also when considering sampling and prediction in mining, mineral processing, geochemical mapping, and environmental monitoring programs for example. The pertinent literature is vast; suffice here to direct the attention to Pitard, It is now possible to summarize all errors defined by TOS, governing both the 0-D and 1-D sampling scenarios. The concept of 0-D errors identifies and summarizes every sampling variation source attributed to 0-D sampling: FSE: FSE is related to CH, which is a reflection of the material state of the lot. GSE: This error arises because of meso- and macroscale groupings and segregation and grouping in the lot material. IDE: When the geometrically outlined increment cannot be made to comply completely with the ideal increment requirements, this error crops out. It can be completely eliminated. IEE: When the material extracted is not 100% coinciding with what occupies the delineated increment, this error results. It can be completely eliminated. IPE: The error that sums up all sources of nonstochastic variation after extraction of the material, for example, evaporation, moisture uptake. This error should always be nil. IWE: Where appropriate, Minkkinen For 1-D sampling scenarios, a three-part error due to process variation is added: The so-called continuous selection error (CE) is made up of three parts (only two are new in addition to the 0-D case): CE1: Random fluctuation (identical to the sum of all 0-D errors). CE2: Time fluctuation error (TFE). Error contributions due to trends in the process, which are not captured by the increment. CE3: Cyclic fluctuation error (CFE). Error contributions from periodic variation of the process, which are not captured by the increment. All the above errors form the TSE: A basic set of sampling unit operations (SUOs) has been formulated, which constitute a complete set of procedures and general principles regarding practical sampling. Three general principles – normally utilized only once in planning or optimization of sampling procedures: Transformation of lot dimensionality (transforming ‘difficult to sample’ 2-D and 3-D lots to ‘easy to sample’ 1-D lots). It is always possible to acquire some form of specimen from 3-D and 2-D lots, but whether this is based on probabilistic, correct, unbiased methods is a much more difficult issue – estimates of the primary sampling errors are difficult, sometimes impossible to come by, as are useful estimates of lot heterogeneity and composition Characterization of 0-D sampling variation by a replication experiment. Characterization of 1-D (process) variation by variography. Four practical procedures – often used several times during practical sampling: Lot or sample homogenization by mixing or blending. Composite sampling, using the smallest possible increments. Particle size reduction (comminution). Representative mass reduction. The theory pertaining to the individual SUOs is explained in various sections of the present chapter and in full detail in Petersen Data quality is a very broad, often only loosely defined, term; a suite of problem- and discipline-related definitions can be found in the literature. Here we shall not try to define data quality in any comprehensive far less complete sense, but shall be content to specify that any definition that does not include the specific aspect of sample representativity is suboptimal and very likely to be inferior for its use in any broader sense. The term ‘data’ is often equaled with ‘information’; however, it should be obvious that this can only be in a hidden, latent, potential form. Only data analysis Such issues have not been problematic in chemometrics, where, in general, issues pertaining to the prehistory of a data table (‘data’) usually receive but very little attention. One relevant major exception is the work of Martens and Martens, Inasmuch as the present TOS exposé has specified very clearly that ‘reliable analytical information’ derives from ‘reliable analytical results (data)’ only, meaning analytical results exclusively stem from representative samples and adequate sampling plans only, it stands to reason that any definition of data quality per force must include reflections/reference to representative sampling. Thus, it is crucial to contemplate the specific origin(s) of any data set. From the present perspective, dealing with ‘modeling of data’ without any considerations as to their representativity (strictly speaking, the representativity of the samples analyzed) cannot be considered safe and thus not reliable. Without following the basic rules delineated in TOS, focus on ‘data’ alone, including derived issues, such as data quality, constitutes a sin by omission. In the context of process analytical technologies (PATs) for example, all aspects related to process analytical sampling (0-D and 1-D TOS) are nonexisting: TOS constitutes a veritable missing link in PAT. The type of errors known as ‘measurement errors’ in chemometrics typically only relate to the X-data (‘instrumental signal errors’), and the magnitude of this type of error is inherently only of the same order of magnitude as the analytical errors – making this type of error almost as irrelevant as TAE compared to the physical sampling errors (TSE and its breakdowns) treated here. Section In statistics, data analysis, and chemometrics, a much favored method of validation is cross-validation, in which a subset of the training set apparently performs as an ‘independent test set’ in a sequential manner. Depending on the fraction of training set samples (totaling Against this background, Esbensen The main lesson from TOS’ more than 50 years of practical experience is that there is no such thing as a constant sampling bias – sampling bias changes with every new sampling from a heterogeneous material. From this it follows that there can never be any guarantee that the specific training set realization will also be representative of all future similar data sets, each of which also repeats Therefore, it is always necessary to test all implicit assumptions regarding sampling bias constancy, in practice every time, say, when a prediction model is to be used for its designated purpose: when predicting this is also based on a bona fide new data set, that is, a new where RMSEP is the root mean square error of prediction. From this discussion, one can conclude with complete generality that all types of variants or schematics of the cross-validation ilk by necessity must be inferior, indeed unscientific, It is interesting to note that this highly compressed argumentation is made infinitely easier to express – and comprehend – by invoking the heterogeneity characteristics embedded in TOS. Caveat: Within process sampling, there are many opinions on the market as to the optimal definition of the training set versus the test set. In a 5-year data series for example, would the first 3 years constitute a viable/optimal training set, with the last 2 years serving as an appropriate test set? Or, would pruning over the entire 5-year set, for example taking out every fifth sample to make up the test set (of size 20% relative to the 80% training set), be a better solution? Specifically regarding process sampling, there are always many options: Local, problem-specific schemes are legion, but only seldom comprehensive and authoritative enough to hold up to systematic scrutiny. The specific process sampling scenarios in TOS are in fact (much) more complicated than what simple carrying over of 0-D notions allow for. Suffice here to mention only seasonality, trends, or even more complex issues (process upsets a.o.). Indeed TOS has made specific allowance for the first two general factors in the form of TFE and CFE, respectively. Current experience would indicate that these features have stood process sampling in very good stead so far…