The International Measurement Evaluation Program (IMEP) organised the IMEP-24 interlaboratory comparison after reports in the media about high levels of lead in toys. The aim of this comparison was to verify the laboratories’ capacity to evaluate trace-element levels in a possible toy-like material according to the European Standard EN 71-3:1994. As test material, it used a former certified reference material containing levels of antimony, arsenic, barium, cadmium, chromium, mercury, lead and selenium around the limits set in the standard.
                  Four expert laboratories confirmed the reference values (Xref) for all elements but Hg, and established a reference value for Hg. The scatter of the results reported by the participants was large, as expected, but showed a close to normal distribution around the reference values for five of the eight trace elements. The spread of results was mainly attributed to sampling and sample preparation.
                  One major issue observed in this exercise was the lack of legislative rules about how to report the result, or, more specifically, the use of the analytical correction, which was introduced in EN 71-3:1994 to achieve consistent interpretation of results and which is to be applied when values are equal to or above the maximum limits set in the standard. Its application by the participants was very inconsistent and led to problems in their evaluation. There is clearly a need for clarification and for more formal regulations with regard to result reporting in order to minimise the risk of confusion.
                  Participants were also asked to give their opinion with regard to the acceptability of the test material for the market. The majority correctly considered the material as non-compliant. However, almost one-third incorrectly assessed the material as compliant.It is the policy of the European Union (EU) to give high priority to the protection of the health of children and ensuring the safety of toys is an important way to achieve this. The European Commission (EC) proposes to fulfill this policy by setting standards for organic and inorganic substances in toys. The Toy Safety Directive The International Measurement Evaluation Program (IMEP), a registered trademark of the Joint Research Centre (JRC) of the EC, covers a wide range of measurement problems from food safety to environmental pollution. IMEP enables laboratories to assess their measurement performance and demonstrate their competence at a high-quality level to accreditation, authorisation, and inspection bodies, and their customers In support of the Toy Safety Directive, IMEP organised an interlaboratory comparison (ILC), IMEP-24, with the aim of testing laboratories’ performance in the analysis of trace elements in toys according to EN The measurands were the migrated amounts of antimony (Sb), arsenic (As), barium (Ba), cadmium (Cd), chromium (Cr), lead (Pb), mercury (Hg), and selenium (Se), according to EN 71-3:1994. The test material used for this ILC was the former certified reference material (CRM) BCR The principle of the procedure described in the standard involves the extraction of soluble elements from toy material under the conditions simulating the material remaining in contact with stomach acid for a period of time after swallowing. Coatings have to be removed mechanically from their support in order to have at least 100 The migration of the eight trace elements from toys should comply with the limits ( This AC was introduced after former ILCs revealed scattered results A number of laboratories were nominated by their national accreditation bodies. Furthermore, laboratories specialised in toy-safety-related analyses were invited to register and 14 participated. Finally, the exercise was also made public on the IRMM website, so that any laboratory interested to participate could register. Some 40 participants from 18 countries registered for the exercise, of which 39 submitted results ( As this exercise was run to verify the performance of the laboratories when applying the EN 71-3:1994, they were recommended to apply the corresponding procedure. Laboratories had 4–5 weeks to do the measurements. The test material – BCR The paint was produced using dark grey “base” paint and adding a series of “tinters”, each containing one of the eight toxic elements at concentrations sufficient to yield migrated element concentrations at or around the maximum permissible levels. It was sprayed on mild-steel plates of the size of 150 The original certification measurements were carried out by 10 expert laboratories following the sample-preparation procedure given in EN 71-3:1994 and using different techniques to analyse the sample extracts. The mean of the laboratory means was adopted as the certified value for each element in BCR 620, except for Hg (due to a high dispersion of its results In view of the doubts about the stability of BCR Since the material had been withdrawn from the market, it was decided to carry out homogeneity and short-term stability studies before starting the ILC exercise. Ten samples stored for the past seven years at 18°C were tested in two replicates each for homogeneity, and three samples stored at 4°C were tested in two replicates each for stability. Results of the homogeneity study were evaluated according to ISO The results of the homogeneity test can be found in The results of the homogeneity study were compared with the results from the stability study. Changes should happen more slowly at lower temperatures and thus comparison of samples stored at 18°C with those stored at 4°C gives a good estimate of degradation while being independent of laboratory bias. Both groups of results were also compared to the certification measurements from 10 years earlier. The various results agreed within their uncertainties, despite being analysed after such long time interval in some cases ( Four laboratories that had been involved in the original certification measurements were selected to perform accurate analyses, and their values were used to confirm the reference values from the certificate or to establish new reference values, if needed. Thus, a reference value was determined for Hg, for which no certified value was available. The four laboratories reported results that were used to calculate the different mass fractions as the mean of the four independently reported values, X For Sb, As, Cd, Hg and Se, results do not fully agree within their respective uncertainties and the application of Equation The E No significant difference could be observed (E The standard deviation for proficiency testing, All results are summarised in A general outcome of this interlaboratory comparison was that the standard EN 71-3:1994 appears to be unclear when it comes to the question of how to deal with the interpretation of the analytical results and how to submit them to end users. The standard states that the AC “shall be applied”, but it does not clarify by whom (e.g., control authorities or customers) or when (e.g., reporting of results or interpretation of results). This lack of clarity is clearly reflected in the spread of results reported by the participants. For this reason, observed problems in result reporting led to separate evaluations of (i) analytical performance and (ii) reporting and interpretation of results, which were a rather new aspect in IMEP-ILC evaluation but it appeared to be almost more important in this exercise than the usual assessment of the analytical performance. It was observed that the AC was not applied by all participants, or if applied, not always consistently. Thus, in order to get comparable results, all participants were contacted and asked whether they had applied the AC, and if so, to submit their raw “non-corrected” values to the ILC provider. These values were then used for the evaluation. For each element, the results were graphically represented, as shown in Both scores can be interpreted as: satisfactory result for |score| questionable result for 2 unsatisfactory result for |score| The z-score indicates whether a laboratory is able to perform the measurement in accordance with what is considered good practice by the organizers of the ILC or the legislator. The ζ-score states if the laboratory result agrees with the assigned value within the respective uncertainties. It is the most relevant evaluation parameter, as it includes all parts of a measurement result, namely the expected value and its uncertainty, as well as those of the assigned value Although the results are generally normally distributed around the assigned value, the Kernel plots displayed some extreme results. Some of these deviations may be due to the application of the AC; five laboratories did not answer whether or not they applied the AC, which was taken as non-application of the AC (for those who answered, results were adjusted to non-corrected or raw results, if necessary). For Hg, Se and As, there was a tendency to underestimate the mass fraction. Medians Some influence of the instrumental detection on the results was observed. About 85% of the laboratories used inductively coupled plasma-mass spectrometry (ICP-MS) or ICP-optical emission spectrometry (ICP-OES) and the remaining 15% atomic absorption spectroscopy (AAS), except in the case of Hg, for which about one-third of the participants applied AAS. For example, considering Hg, results obtained with AAS appeared to be lower than those obtained with ICP techniques. This observation is confirmed by a two-sided t test: the difference between the two groups was indeed significant at the 95% level of confidence with p An influence on the results similar to that for Hg could be observed for Se, but not as pronounced. No instrumental influence was observed for As. Considering that low results were observed with all three techniques, the main contributor to error seems to be sample preparation, as already observed by Darrall et al.  The situation was different for the More than half of the participants stated that they do not usually report the uncertainty to their customers. This may explain the lack of experience in uncertainty estimation. However, when plotting the scores as a function of the reporting or non-reporting to customers, there was no tendency showing that the laboratories reporting uncertainties to customers obtained better scores. According to Annex D.4 of EN sample preparation (i.e. scrapping technique and particle sieving); and, analytical conditions (i.e. solution to be kept at 37°C before leaching; shaking/stirring and measurement of freshly prepared sample solutions). Both sources of information were accounted for in a questionnaire that the participants had to complete when submitting their results. In addition, the questionnaire also included information about the uncertainty estimate, experience and quality control. The answers were used in a multivariate analysis to identify reasons for scattering results. Statistical data treatment was performed using The Unscrambler 9.8 Of the 16 predictors initially selected, two groups of predictors – identified as major factors influencing the most the quality of the analytical results – are related to: laboratory expertise in the field (number of samples analysed per year); and, the sample treatment (deviation from the EN 71-3 protocol; analysis performed on the day of sample preparation; acid adjustment for storage; type of shaker; type of filter; and the analysis of the base material). As expected, most of the experienced laboratories performing many toy tests provided satisfactory results. As for the sample-preparation-related variables, generally most of Darrall et al.’s recommendations According to Annex D.4 of EN 71-3, a defined scraping technique resulted in a better statistical agreement between laboratories, but not when a 300–500μm test portion was used. Furthermore, improvements were not considered significant enough to justify changes in the standard The Annex also mentions that the use of different instrumental techniques contributes to the statistical uncertainty of the procedure, but this was not observed in IMEP-24, and the spread of results could not be attributed to the techniques used. The participants did not receive any instructions concerning the application of the AC. Reporting is part of the proficiency test, so it should be taken into account in the evaluation. Thus, the participants were asked to submit the results as they would do to their customers. Differences in reporting were clearly an issue in this exercise. Four different behaviours were observed, as follows. Application of the AC “by default” (to all elements) (13 laboratories): defendable for practical reasons but not always necessary (e.g., in the case of As, Ba and Se, where most or even all of the laboratories had results below the legislative limit, there was no need to apply it). No application at all (21): justified, if low results (5). Otherwise not (16), since the results for some elements are close or above the limit. Here the question is: “How real cases are dealt with?”. Application only to the means (while reporting the uncorrected replicate measurements): unclear, requires specification (3). Application to a part of the analysed elements: justified, but requires specification (4). Considering the clause about the AC in the standard, one would expect that not only more laboratories would apply the AC, but also they would include information about its application or non-application in reporting their results. However, most laboratories did not specify whether or not the AC was applied, and only eight participants did so. The different ways of reporting led to the question of how participants dealt with the overall interpretation of their results. They were asked whether they would “accept or reject the material on the market”. Their assessment relied on the simple test for each of the eight elements: “Does the mean of my reported values, X Knowing the established reference values, X Most laboratories correctly assessed the material as being non-compliant. However, many of the laboratories under-estimated the Pb content ( Furthermore, nine laboratories were found to give an incorrect overall interpretation of their results: because they wrongly because they “rejected” material while providing measurement values below X Finally, two laboratories provided a correct overall assessment, but on the basis of very unreliable results, reflected by high z-scores and ζ-scores. EN 71-3:1994 is currently being modified. However, it was judged important to see how laboratories currently perform in the analysis of trace elements in toys, in order to highlight potential problems linked to the procedure. As expected, the variation in the results was high, but generally showed close to normal distribution around the reference values. However, laboratories seem to have problems with performing sound uncertainty assessment of their measurement, as reported uncertainties were often too small compared with the known variation in results for this type of analysis. The correct assessment of the uncertainty linked to an analytical result is as important as the result itself, especially in cases of dispute. Further recommendations for reliable measurements can be distilled from this exercise: Great care should be taken with regard to sampling and sample preparation as they have been identified as the critical steps of the analytical protocol described in EN A clear statement whether or not the correction has been applied must be delivered with the results. While the majority considered the material correctly as non-compliant with the EN standard, 28% of the participants wrongly assessed the material. Considering the great attention heavy metal in toys and related legislation received in the media in recent years, it is also worth mentioning that a high number of wrong assessments of the presence of Pb in the test material were observed, presenting a potential health risk because of undetected high levels of Pb. Philip Taylor from the Institute for Reference Materials and Measurements is acknowledged for having made the project possible. The authors also kindly thank all the laboratories for their participation in this exercise (