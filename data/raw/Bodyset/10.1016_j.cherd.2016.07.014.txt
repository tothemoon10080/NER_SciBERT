In this study, process monitoring based on signal decomposition by use of singular spectrum analysis (SSA) is considered. SSA makes use of adaptive basis functions to decompose a time series into multiple components that may be periodic, aperiodic or random. Two variants of SSA are considered in this investigation. In the first, the conventional approach is used based on latent variables extracted from the covariances of the lagged trajectory matrix of the process variables. The second approach is identical to the first approach, except that the covariances of the lagged trajectory matrices are replaced by Euclidean distance dissimilarities to decompose the variables into additive components. These components are subsequently monitored and the merits of the two approaches are considered on the basis of two case studies using simulated nonlinear data and data from the benchmark Tennessee Eastman process.Several key drivers in modern chemical and metallurgical industries have led to increased interest and adoption of data driven innovations in process control and monitoring technology. These include enhanced process safety, process efficiency, and improved performance in achieving product quality specifications through the management of process variation. As a result, large volumes of data are routinely logged and stored in data warehouses, and can be used in fault detection and diagnostic tasks in process operations. Owing to the highly correlated nature of the observed process variables, the challenge associated with these process monitoring schemes is the development of efficient modeling techniques that can account for the relationships between process variables in order to detect faults in the process operations ( The most common multivariate statistical process monitoring (MSPM) methods are principal component analysis (PCA) and partial least squares (PLS) ( As a result, a plethora of nonlinear extensions to PCA have been proposed to handle nonlinear processes, such as principal curves ( In some instances, a more effective approach would take advantage of multiple representations of measurements, that is, the representation of features that occur with different localization in time, space and frequency ( In other approaches, decomposition of the variables is aimed at reconstruction of a common subspace, a specific subspace and a residual subspace. Monitoring is consequently performed in every subspace ( To better summarize multivariate data in a lower dimension than the conventional approach inspired the development of MSPM frameworks using multidimensional scaling ( The same approach is followed in this paper via the use of squared dissimilarity matrices in lieu of correlation matrices as used conventionally in PCA. This approach can also handle nonlinear correlations in the data like other nonlinear methods in process monitoring ( In the area of chemical process monitoring, singular spectrum analysis (SSA) has become a promising tool to prefilter process data before the application of multivariate statistical process control (MSPC) tools for fault detection in the process ( In this paper, these studies are extended by considering two variants of SSA as a basis for process monitoring. The first is based on the use of classical SSA (cSSA) ( The application of DSSA is demonstrated using simulated data and the Tennessee Eastman Challenge process and the paper is organized as follows. Section In this section the basic steps involved in SSA is briefly explained with its applications in time series analysis. Singular spectral time series analysis prefilters the original time series into a sum of series that contains components such as a trend, periodic or quasi-periodic components, or noise. This is done by the singular value decomposition (SVD) of a trajectory or lagged covariance matrix obtained from the original time series, followed by reconstruction of the series using subsets of eigenfunctions and corresponding principal components. Standard PCA is performed on the trajectory matrix of the time series and hence the mathematical and statistical properties of PCA extend to SSA. The time series is first embedded into an An outline of the basic SSA methodology ( Given a time series These embedded vectors The trajectory matrix By using singular value decomposition (SVD), the trajectory matrix can be represented as the sum of biorthogonal elementary matrices of unity rank. More specifically, denote During grouping, the elementary matrices in Eq. Each matrix In this way, the original time series  The coarse scale signals are reconstructed by using those components with the largest singular values and the remaining components are reconstructed by using those components with singular values decreasing in ascending order. Application of SSA results in the decomposition of a signal into component signals that tend to capture distinct features in the original signal, if present, e.g. the deterministic mean and random variations ( In SSA, a compact linear approximation of the embedded data set is sought that accounts for the maximum amount of variance in the data. With SSA, this is accomplished by reconstructing the data based on the leading eigentriples, while all minor eigentriples are discarded in the reconstruction process. This is done regardless of whether they could carry important information or not ( To address this problem, the analysis of the lagged copies of the time series by use of multidimensional scaling, instead of SVD, is proposed. CMDS uses interobject distances as the basic measure for projecting multivariate scores to handle nonlinear correlation in the data. The basic methodology outlined for univariate time series is also directly applicable to multivariate time series. More formally, consider a set of In this case, the trajectory matrix of the system is a stacked Hankel matrix of the form As with the one-dimensional time series, SSA decomposes each of the time series in the multivariate system into Finally, it is also worth noting that if the matrix That is, Multidimensional scaling (MDS) is a popular multivariate statistical method used for dimensionality reduction and data visualization. With MDS data points are placed in a For the development of a multivariate statistical process monitoring (MSPM) framework, In the second phase, through a multidimensional scaling process, the dissimilarity matrix is converted into a manageable, lower dimensional configuration of points specified by coordinates, which is easier to interpret and portray ( The eigenvectors and eigenvalues of the minor product moment are obtained from Eq. In this study, the interdistance dissimilarity measures are developed based on the relationship between the scalar product, The projection of multivariate scores is subsequently rearranged in the form of Cartesian coordinates. The computation of Application of multidimensional scaling in multivariate statistical process control based on reproducing scores either in the sample or variable configuration ( The goal of dissimilarity scale based SSA is to decompose the data into multiple modes using dissimilarity scale measures instead of correlation matrix based eigenvectors to project the scores to capture nonlinear information in the data. The trajectory matrix Then the double centering is implemented on The application of singular value decomposition on The dimension of the score vectors is selected based on the ratio of the retained and total eigenvalues ( The multimodal approximations of the reconstructed signals can be achieved through diagonal averaging as outlined in step 4 of the SSA decomposition. The ability of SSA to decompose the signal into multiple components and the applications of MSPC methods for process monitoring and fault detection have generated an interest for a new process monitoring method in petrochemical processes ( In cSSA, analysis of process variables at different resolution levels can be accomplished by applying SSA to each variable separately using a common window of size In this study, the window length More formally, given Scaled versions of the original data matrix Hence, at the coarsest level (scale The Note that these loadings and corresponding scores In a similar manner as with conventional PCA, the appropriate number of principal components that must be retained ( The squared residuals or If, at a specific scale, The use of multiple tests increases false positives errors, i.e. the likelihood of incorrectly assigning an event as abnormal. For a set of independent tests, the significance level of each test must be adjusted so that the overall significance for all tests taken together equals the nominal value. Bonferroni's method can be used to adjust the significance values ( The DSSA is the same as cSSA, except that reconstructed signals are derived from the multiscale decomposition of the original signals using dissimilarity scale based SSA (DSSA). The DSSA methodology consists of decomposing each variable by DSSA into multiple scales, after which the PCA model is developed using the approximated variable at each scale. Control limits for scores and residuals at different scales are computed as in classical multivariate statistical process monitoring method ( The first step in DSSA is the decomposition the data matrix The original data matrix is then reconstructed by This is repeated for all The cSSA and DSSA monitoring schemes are illustrated diagrammatically in After singular value decomposition of each of the Finally, for each of these M versions of the original data matrix, a PCA model is built and the When used online for process monitoring, new data samples are collected, until a multivariate time series segment equal to the length of the window ( The DSSA process monitoring scheme is similar to the cSSA scheme, except that the trajectory matrix of the time series is based on a In this section cSSA and DSSA approaches together with conventional PCA are applied to data representing a simple example (  The noise vectors were of the form The first 100 samples obtained by Eqs. These data for normal (Case 0) and fault conditions (Case 1) are shown in For both the SSA based approaches, an embedding window of size The singular spectra of the trajectory matrices of the three variables are shown in All components in the DSSA model could be approximated by multiplying the approximated score vectors with the corresponding eigenvectors and diagonally averaging the matrices The reconstructed components were monitored using PCA as explained in the monitoring step of cSSA in Section The DSSA method yielded results comparable to that obtained with the nonlinear method proposed by Dong and McAvoy, while cSSA appeared to be more reliable than the DSSA or the Dong and McAvoy approach in that 62% of the samples were flagged as faulty, compared to the 31% and 32% of the DSSA and NLPCA methods. These results were obtained with the 2nd component of the DSSA decomposition, as well as the 34th component of the cSSA decomposition. These components are shown in The Tennessee Eastman Process (TEP) is a simulation of an actual chemical process and has served as a realistic industrial case study for numerous plant-wide process control problems, including process monitoring and fault diagnosis ( The reactions in the reactor are represented by: The reactions in Eqs. The reactant gases form liquid products, which are catalyzed by a non-volatile catalyst dissolved in the liquid. The heat of reaction is reduced by cooling water that is circulated in the reactor. The gaseous products are separated from the reactor, while the catalyst is retained. The product gas is cooled by using a condenser and then fed to a vapor⿿liquid separator. The recycling of the non-condensed vapor from the separator to the reactor is done through a compressor. A portion of the recycled vapor is purged to keep the inert product and byproduct from accumulating in the process using the vapor⿿liquid separator. The remaining reactants in the condensed stream from the separator are removed in the stripper. From a data perspective, the process has 12 manipulated or control variables and 41 measured or observed variables. Of the latter, 22 are continuous and 19 are composition measurements. Of the 53 process variables, 16 variables as selected by The TEP process contains 21 faults listed in The data representing normal operating conditions and the 21 fault cases contains 500 samples in this study. The proposed DSSA framework is compared against cSSA and conventional PCA. As in the first case study, 95% control limits are set for both Using PCA, 13 PCs explaining 96% of the variation in the data for the normal case were retained in the PCA model. The estimated reliability of For cSSA, an embedding window of size As with cSSA, a window size of The first 145 of 463 components were used to build the PCA model for each fault condition. The reconstructed components were monitored using PCA. The PCs retained in this PCA model captured at least 90% of the variance of the data. As with PCA and cSSA, the reliability of the The control limits for both statistics were set at a 95% confidence level in all cases, hence the approximately 5% detection rates for the methods in Case 0 (the normal data). Unlike in the previous example, based on the data in SSA is a nonparametric spectral technique based on PCA (or SVD) to decompose variables into additive components. For this purpose, it uses data adaptive basis functions, and is therefore in principle more flexible than other methods, such as the use of wavelets, relying on predefined basis functions. As a consequence, SSA can effectively decompose short and noisy signals into deterministic and stochastic components that can then be monitored separately. In this study, fault detection in process systems based on the use of two variants of SSA have been investigated, viz. classical SSA or cSSA and dissimilarity based SSA (DSSA). Whereas cSSA reconstructs representations of a data matrix based on the covariance structure of the data, DSSA does so based on dissimilarities in the data (Euclidean distances) and the concept of multidimensional scaling. The results from the two case studies considered in this paper suggest that cSSA and DSSA both have their comparative merits and choosing one over the other would be dependent on the specific process system to be monitored. Further work is required in this regard, as well as to get a better understanding of the comparative advantages or disadvantages with regard to other state of the art methods. Even so, the preliminary results reported in this paper, seem to suggest that the overall methodology may be a promising approach to process fault detection in complex, nonlinear systems.