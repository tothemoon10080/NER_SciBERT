The use of artificial neural networks (ANNs) for the modelling of water resources variables has increased rapidly in recent years. This paper addresses one of the important issues associated with artificial neural network model development; input variable selection. In this study, the partial mutual information (PMI) input selection algorithm is modified to increase its computational efficiency, while maintaining its accuracy. As part of the modification, use of average shifted histograms (ASHs) is introduced as an alternative to kernel based methods for the estimation of mutual information (MI). Empirical guidelines are developed to estimate the key ASH parameters as a function of sample size. The stopping criterion used with the original PMI algorithm is replaced with a more computationally efficient outlier detection technique based on the Hampel distance. The performance of the proposed PMI algorithm, in terms of computational efficiency and input selection accuracy, is first investigated by using it to identify significant variables for data series where dependencies of attributes are known a priori. The proposed ASH PMI input variable selection algorithm with the Hampel distance stopping criterion consistently selects the correct inputs, while being computationally efficient. The modified PMI algorithm is then applied to identify suitable inputs to forecast salinity in the River Murray at Murray Bridge, South Australia, with a lead time of 14 days using an ANN approach. The ANN models developed with the inputs selected with the modified PMI algorithm perform very well when compared with results obtained using ANN models with different input sets developed in previous studies. Furthermore, the proposed input variable selection algorithm results in more parsimonious ANN models.There has been a rapid increase in the use of artificial neural networks (ANNs) for hydrological modelling ( Due to the negative consequences of over- and under-specification outlined above, there are distinct advantages in using analytical procedures for selecting an optimal model input vector from a set of candidates. When choosing an appropriate input selection algorithm, the following four factors need to be considered. Input selection algorithms need to be able to determine the strength of the relationship between potential model inputs and outputs. The approaches used to achieve this generally fall into two categories: model-based and model-free. Model-based approaches use the performance of calibrated models with different inputs as the basis for choosing the most appropriate input vector. This has the advantage that non-linear relationships in the data can be taken into account. However, as ANNs have to be trained before the strength of the relationship between potential model inputs and output(s) can be determined, near-optimal model structures and values of the connections weights have to be obtained for each of the models developed. If this is not the case, the model inputs obtained based on the strength of the relationships extracted from the trained model are likely to be sub-optimal or even misleading. Model-free approaches overcome this shortcoming by using statistical measures of dependence to determine the strength of the relationship between candidate model inputs and the model output prior to model specification and calibration. However, care needs to be taken that non-linear dependence measures, such as mutual information, are used, rather than linear measures, such as correlation. Input selection algorithms should cater for redundancy in candidate model inputs. Although a candidate model input might have a strong relationship with the model output, this information might be redundant if the same information is already provided by another input. In model-based approaches, redundancy is generally taken into account implicitly by use of a stepwise model-building process (e.g. forward selection or backward elimination). In model-free approaches, stepwise partial model building approaches (e.g. partial correlation, partial mutual information) can be used to eliminate redundant inputs. Both model-based and model-free approaches generally use a stepwise process in the identification of appropriate model inputs. Consequently, there is a need for a stopping criterion that helps determine when to stop adding or removing candidate inputs. In model-based approaches, information criteria, such as Akaike’s and Bayes’ information criterion, provide viable alternatives, as they balance prediction error with model complexity. When using model-free approaches, the use of significance measures is well established for correlation-based methods. However, the same does not apply to non-linear dependence measures, such as mutual information, although The computational efficiency of the input selection approach used is paramount, particularly when dealing with large datasets, as is the case for many hydrological modelling applications. Model-based approaches are generally very computationally inefficient, as they require the development (e.g. determination of optimal model structure, model calibration) of a large number of models. The efficiency of model-free approaches is a function of the dependence measure used. Use of correlation as the dependence measure is computationally efficient, but does not cater for non-linear dependencies, as discussed above. Use of mutual information as the dependence measure is generally computationally inefficient, although this depends on the method used to obtain estimates of mutual information, as well as the stopping criterion used. In a review of approaches used to select the inputs to ANN models, The PMI algorithm’s computational efficiency is a function of the method used to estimate mutual information (MI), as well as the number of times estimates of MI have to be obtained. In relation to the calculation of mutual information, this requires the estimation of marginal and joint probability densities. Commonly used density estimation techniques in the context of calculating MI include histograms and kernel density methods. Kernel based MI estimators are generally considered to be more reliable than histogram based approaches ( In this paper, a more computationally efficient version of the PMI algorithm is developed, tested and applied. As part of the proposed algorithm, computationally efficient estimates of MI are obtained by using average shifted histograms (ASHs) for density estimation. In addition, the number of times MI estimates have to be obtained is reduced significantly by using a stopping criterion that does not rely on the bootstrap method. As the accuracy of ASHs is a function of two user-defined parameters, guidelines for choosing these parameters are developed. In order to test the utility of the proposed algorithm, its performance is compared with that of the traditional PMI algorithm for a number of benchmark test cases in terms of accuracy and computational efficiency. Finally, the proposed algorithm is applied to the case study of forecasting salinity in the River Murray at Murray Bridge, South Australia, 14 days in advance, and the results compared with those obtained in previous studies in terms of the inputs identified as significant and the accuracy of the resulting forecasts. For a set of The mutual information score in MI can be used to identify non-linear dependence between candidate input and output variables. However, as is the case with linear correlation, MI cannot deal with dependencies among input variables directly. For example, consider the case where two potential inputs The discrete version of Computation of PMI is similar to that of MI, where both require estimation of probability densities. In MI, the probability densities are estimated for the original inputs and output, whereas in PMI, the densities are estimated for the residual information in variables As discussed in the Introduction, the computational efficiency of the PMI algorithm depends on the method that is used for the estimation of mutual information, as well as the stopping criterion that is used to indicate whether a selected candidate variable is significant or not, as highlighted in Calculation of mutual information requires the estimation of marginal probability density functions (pdfs) of Kernel density estimates are sensitive to the bandwidth and there are many techniques available to select optimal bandwidth including data driven, automatic bandwidth selection techniques ( Although kernel based approaches give efficient and reliable density estimates for smaller data sets, their computational efficiency decreases dramatically with increasing sample size as shown in The PMI algorithm requires a reliable and efficient criterion to decide when to stop the addition of new inputs to the list of selected inputs. Although The proposed modifications to the PMI algorithm are designed to improve its computationally efficiency. This is achieved by using a more computationally efficient approach to obtaining estimates of MI for large data sets and an alternative stopping criterion, that does not require repeated calculations of MI. In this paper, it is proposed to use average shifted histograms (ASHs), rather than kernel methods, for the estimation of MI. ASHs were introduced as a means of density estimation by The use of histogram methods for obtaining probability density estimates is extremely computationally efficient. However, as mentioned previously, the choice of bin width, Scott’s ASH density estimator for a univariate sample can be defined as follows. Consider a random sample { A general weighting function Then the generalized ASH becomes Kernels in the shifted Beta family can be used as the weighting function ( As the number of shifts The extension of the univariate ASH density estimator to bivariate data is straightforward. For example, consider a bivariate data series ( Instead of using different shifts, ASH probability density estimates are sensitive to two parameters: the bin width,  Furthermore, These rules tend to result in oversmoothed bin widths and are called “oversmoothed rules” ( In order to develop guidelines for the selection of appropriate values of For two Gaussian variables The errors between the exact value of MI and the MI estimates obtained using the ASH and kernel methods are therefore given by These relationships were used to compare the performance of the ASH and kernel MI estimates for a large number of synthetic bivariate data sets with different sample sizes (100–15,000) and correlation coefficients (0.1–0.9). Correlation coefficients were varied, as MI estimators are sensitive to the strength of the dependence between two variables ( Initialise vector sample size, Do while Initialise vector correlation coefficient, Do while Set repeat Do while repeat Generate two random numbers Compute exact MI between Estimate kernel MI between Estimate ASH MIs between  repeat Go to step 6  Go to step 4 Compute average ASH errors for Compute average kernel error for sample size N(i) by considering all errors computed during step 9  Go to step 2 The algorithm only reaches this point after completing all of the simulations The results obtained from the 21,780 simulations were first analysed to assess the reliability of the ASH and kernel MI estimates. Next, empirical guidelines for estimating optimal values of bin width and As, expected, the errors associated with the ASH MI estimates varied with In a similar manner, an empirical relationship between bin width and sample size was obtained by combining the optimal values of In order to increase the computational efficiency of the PMI algorithm, it is suggested to use the stopping criterion introduced by The For a data set { Details of the PMI algorithm using the Hampel outlier detection test as the stopping criterion are given in Data sets with known attributes were generated from three benchmark test cases, including the AR9, TAR1 and ADD15 models. The AR9 model is a ninth order linear autoregressive model, whereas the TAR1 model is a non-linear threshold autoregressive model. The data generated from both of these models have been used in previous studies to test the accuracy of the PMI input selection algorithm in hydrology ( In order to test the performance of the original and modified PMI algorithms, 30 random realisations of each model with sample sizes, In order to assess the performance of the proposed modifications to the PMI algorithm, different versions of the PMI algorithm with various combinations of MI estimation techniques and stopping criteria were applied to the generated test data ( Each of the six trials consisted of 270 simulations (resulting in a total of 1620 simulations), as each version of the PMI algorithm was applied to all 30 realisations of each of the three case studies for each of the three sample sizes. The accuracy of each version of the PMI algorithm was assessed in terms of the percentage of under-, correct- and over-specification of the inputs for the 30 realisations of each of the three synthetic case studies considered. The chi-square test statistic was used to assess whether there was any significant difference in the degree of under-, correct- and over-specification between the different methods. Computational efficiency was assessed in terms of CPU time, as the number of mutual information (or partial mutual information) estimates does not depend on the mutual information estimation technique used. While this measure is dependent on the computer used to implement the algorithm, it provides an indication of the relative computational effort required for each version of the PMI algorithm considered. For ASH MI estimates, bin widths and The CPU time taken to obtain the significant inputs is a function of the time it takes to obtain one MI/PMI estimate and how many times these estimates have to be made. The former is a function of the technique that is used to estimate MI/PMI values (e.g. ASH or kernel methods) and sample size, while the latter is a function of the number of candidate inputs, the number of significant inputs and the stopping criterion used. The CPU run-times for the various trials conducted for the ADD15 case study are given in The merit of using ASHs for MI/PMI estimation when dealing with large datasets is highlighted further by considering a hypothetical hydrological case study with 100 potential inputs, of which 10 are significant. For a sample size of 5000, the computational effort associated with obtaining the required MI/PMI estimates when kernel methods are used is estimated to be 15,811 The results obtained indicated significant CPU time savings when using the Hampel distance outlier detection stopping criterion as opposed to the bootstrap method ( The results obtained indicate that, based on the values of the chi-squared statistic, there was no significant difference between the degrees of under- and over-specification between the six variants of the PMI algorithm used. This is despite the potentially vastly different computational efficiencies of the algorithms, as discussed above. Overall, the degree of correct specification of model inputs for all variants of the PMI algorithm was approximately 85%. Typical results are shown in All variants of the PMI algorithm were generally able to choose inputs in their correct order of significance. For the AR9 data sets, all methods consistently selected input variable The results obtained also indicate that there is no significant difference between the MI estimates obtained using the ASH and kernel based approaches. Both methods consistently obtained MI values that were similar for all 270 data sets tested, as shown in In relation to the different stopping criteria used, the results obtained indicate that they had a statistically insignificant impact on input selection accuracy. This is despite the significant relative differences in their computational efficiency. Consequently, the Hampel distance outlier detection stopping criterion appears to be a better alternative than the bootstrap method. In addition, use of this stopping criterion does not rely on any user-defined parameters, such as the significance level and the number of bootstraps used. In order to further test the utility of the modified PMI algorithm, it was applied to the case study of forecasting salinity in the River Murray at Murray Bridge, South Australia, 14 days in advance. This case study was selected as ANNs and the original PMI algorithm have been applied to this case study previously ( The River Murray is the largest single surface water resource in South Australia. In addition to supplying water for irrigation, it supplies about 40% of Adelaide’s urban water in an average year. In dry years, this can increase to as much as 90%. Water from the River Murray is delivered to Adelaide by pumping water from three locations. By forecasting salinity several weeks in advance, pumping policies can be developed such that more water can be pumped at times of low salinity and less water pumped at times of high salinity.  The data set for the period 01-12-1986 to 30-06-1992 was used to identify the input variables for the ANN model. The ASH PMI algorithm with the Hampel distance outlier detection approach as the stopping criterion was used to identify the significant inputs from among the 1304 candidates. The The three selected input variables are directly related to salinity at Murray Bridge with a 14 days lead time. The travel time from Waikerie to Murray Bridge is approximately 14 days for high flows, whereas the travel time from Mannum to Murray Bridge is in the vicinity of 14 days at flow values above 4000 In the study conducted by Based on the CPU time estimates presented in Testing of modified PMI algorithm, the time taken to obtain the three inputs identified as significant in this study by using the modified PMI algorithm (i.e. use of ASH to obtain MI/PMI estimates and Hampel distance outlier detection stopping criterion) was 86 A multi-layer perceptron (MLP) neural network with a single hidden layer was used to obtain the 14 days forecasts of salinity in the River Murray at Murray Bridge. The three model inputs were identified using the modified PMI algorithm proposed in this paper, as discussed above. The two data division methods employed by The hyperbolic tangent function was used as the activation function for both hidden and output layers. The input variables, as well as the target values, were scaled to lie in the range −0.8 to 0.8. The synaptic weights of the networks were initialised with uniformly distributed random numbers in the range −1 to 1. The model was calibrated with the aid of the backpropagation algorithm with momentum. The combination of a learning rate of 0.01 and a momentum term of 0.5 resulted in a smooth error reduction curve and was therefore used in this study. The order in which the training samples were presented to the network was also randomised from iteration to iteration. The cross validation technique was used as the stopping criterion. i.e. training was stopped when the RMSE of the test data set started to increase. The number of nodes in the hidden layer affects the performance of the trained network and therefore should be optimised. In general, networks with fewer hidden nodes are preferable, as they usually have better generalisation capabilities and fewer over-fitting problems and are also more computationally efficient. However, if the number of nodes is not large enough to capture the underlying behaviour contained in the data, the performance of the network might be impaired. In this study, a trial and error procedure was used to obtain the optimal number of hidden nodes, which involved gradually varying the number of nodes in the hidden layer from 5 to 30. Networks with 20 hidden nodes gave the best results for the testing data for both data sets (i.e. the data sets obtained with conventional data division and GA data division) and were therefore used for forecasting purposes. Time series plots of observed and predicted values of salinity at Murray Bridge with a lead time of 14 days obtained using the ANN with three inputs selected using the ASH PMI algorithm introduced in this paper are shown in The results obtained from this study and previous studies where the available data were divided using the GA data division technique are summarised in Overall, the results indicate that the ANN models developed with the three inputs identified using the modified PMI algorithm introduced in this study perform very well, while also being the most parsimonious. This provides further evidence of the utility of the modified PMI input selection algorithm introduced in this paper, especially when dealing with large data sets. In this study, a modified version of the PMI input selection algorithm for data driven models, such as artificial neural networks, is introduced. The modified algorithm uses average shifted histograms (ASHs), rather than kernel based methods, to obtain mutual information (MI) and partial mutual information (PMI) estimates. In addition, it uses a stopping criterion based on outlier detection, rather than the estimation of critical MI values using bootstrap techniques. In order to ensure application of the algorithm is user-friendly, empirical guidelines were developed for determining the two key ASH parameters as a function of sample size. Tests on three commonly used benchmark problems with known attributes have shown that the modified version of the PMI algorithm is much more computationally efficient, especially for larger sample sizes, without loss of accuracy. Application of the algorithm to the case study of forecasting salinity in the River Murray at Murray Bridge, South Australia, 14 days in advance, has indicated that it provides a robust and computationally efficient means of determining a parsimonious set of significant inputs from a large number of candidates. Use of the ASH as an alternative technique to estimate MI/PMI significantly reduces the computational cost of the original PMI input selection algorithm. The modified algorithm is an attractive tool for identifying input variables for data driven models, especially when dealing with hydrological data sets, where large sample sizes are common, thereby making it inefficient to use kernel methods to estimate MI/PMI. However, for small sample sizes (eg. sample sizes less than 100), the use of kernel methods to estimate MI/PMI is recommended, as kernel methods provide more reliable density estimates for small sample sizes and run times are similar for both kernel and ASH methods when such sample sizes are considered. Funding for this work was provided by an Australian Research Council Discovery Grant (DP0345628).