A different number of citations can be expected for publications appearing in different subject categories and publication years. For this reason, the citation-based normalized indicator Mean Normalized Citation Score (MNCS) is used in bibliometrics. Mendeley is one of the most important sources of altmetrics data. Mendeley reader counts reflect the impact of publications in terms of readership. Since a significant influence of publication year and discipline has also been observed in the case of Mendeley reader counts, reader impact should not be estimated without normalization. In this study, all articles and reviews of the Web of Science core collection with a publication year of 2012 (and a DOI) are used to normalize their Mendeley reader counts. A new indicator that determines the normalized reader impact is obtained –the Mean Normalized Reader Score (MNRS) – and compared with the MNCS. The MNRS enables us to compare the impact a paper has had on Mendeley across subject categories and publication years. Comparisons on the journal and university level show that the MNRS and MNCS correlate larger for 9601 journals than for 76 German universities.Estimating the citation impact of scientists, research groups, and institutes in different disciplines and time periods faces the problem that discipline and time period influence the citation impact of publications independently of the quality of the publications. Normalization for both factors started in the mid-1980s ( In recent years, impact evaluation in scientometric research has been done not only on the basis of citations but also based on alternative metrics (altmetrics) ( Data from Mendeley are among the most important sources for altmetrics: “Mendeley is both a citation management tool and social network for scholars with over two million users” ( Several studies have shown that the Mendeley reader impact – similar to the citation impact, although there are differences between the two – varies across scientific disciplines ( The aim of this study is to apply the most established method of normalization (cited-side) in bibliometrics to the field of altmetrics and propose a normalization scheme for Mendeley reader counts. Independently from and coincidental with our efforts ( It is common practice in scientometrics to evaluate the impact of articles and reviews. Other document types are usually not included in evaluative bibliometrics ( In total, the articles were registered in Mendeley 9352,424 times and the reviews were registered 1335,764 times. For 118,167 articles (10.4%) and 4348 reviews (6.7%), we found the paper at Mendeley but without a reader. Papers without any readers indexed by Mendeley may originate from former readers who removed the paper from their library or closed their Mendeley account. If Mendeley users include too little bibliographic data for a paper in their library, they are not counted as readers either, because there is insufficient information to link them to a Mendeley database entry. Also, Mendeley adds papers to the database without any reader in the first place from publisher feeds. Therefore, papers with zero reader counts should be excluded in this study, or, if they are included, the papers not found at Mendeley should also be counted as papers with zero readers. We tested both approaches and found no significant differences regarding the scope of this study. In the end, we decided to include the papers with zero readers as well as the papers we did not find in the Mendeley API. This is consistent with the way citations are handled in bibliometric databases. The requests to the Mendeley API were made from December 11–23, 2014. All data in this study are based on a partial copy of our in-house database (last updated on November 23, 2014) supplemented with the Mendeley reader counts. Like the citation distribution ( The reader distribution across the categories ranges from 0.22 readers per paper in Poetry (not shown in For reviews, the highest number of readers per paper (85.22) is found in the WoS category Psychology, Experimental (WoS category “vx” in Usually, reviews are cited more often than articles. This seems to hold true for Mendeley reader counts. For normalization purposes in bibliometrics, the citation impact of a focal paper is compared with the expected citation impact. The expected value is the average citation impact in the same discipline, publication year, and document type as the paper in question. Sometimes, publications of different document types are considered together in the calculation of the expected value (as in the Leiden Ranking) or separately (as in InCites from Thomson Reuters, SciVal from Elsevier, and the Institutions Ranking published by the SCImago group). The paper set determining the expected value is referred to as the reference set. The ratio of observed and expected citations is the Normalized Citation Score (NCS). Currently, the NCS is the established standard in bibliometrics to normalize citation impact. A NCS of 1 for a publication indicates an average citation impact. A NCS of 1.5 can be interpreted as a citation impact that is 50% higher than the average ( If a paper has been assigned (e.g., by a database provider such as Thomson Reuters) to more than one subject category, the average value of all NCS values is used (resulting in a mean NCS). The assignment of papers to multiple subjects leads to an average value over all papers of a publication year that differs from 1. To alleviate this disadvantage, one can employ fractional counting ( In the case of aggregations of papers (normalized impact of a researcher or university), the average value of the NCS values of all papers in the set is calculated. This average value is referred to as the Mean Normalized Citation Score (MNCS). The MNCS is used in the SCImago Institutions Ranking ( Following the definition of the MNCS, we propose a Mean Normalized Reader Score (MNRS) using the multiplicative counting method for papers categorized to multiple subjects. Our normalization procedure for Mendeley reader impact starts with the calculation of the average number of Mendeley readers per paper ( The average value over all NRS values equals exactly one (due to the multiplicative counting method). Since we include only papers that were published in 2012, the publication year does not have to be included in the normalization procedure. The overall reader impact of a specific aggregation level (e.g., researcher, institute, or country) can be analyzed on the basis of the mean value over the paper set. This results in the mean NRS (MNRS) for the paper set. As an illustrative example, we show a step by step calculation of the NRS for the article at  Normalized readership values can be calculated for different units: single researchers, research groups, institutions, countries, journals, etc. As examples, we present in the following the MNRS values for journals and several German universities. Papers from 9601 journals out of the 12,334 WoS journals in 2012 are covered in the papers found at Mendeley. The MNRS of all 9601 journals correlates much larger than typical with the MNCS, which is indicated by the Spearman rank coefficient of 0.70 (for the interpretation of correlation coefficients, see  Although the MNRS values of the top 10 German universities indicate that the universities perform up to 46% better than average, their MNRS average values are still quite far away from the top 10% and top 1% thresholds of individual papers: top 10%: 2.43 for articles and 2.39 for reviews; top 1%: 7.11 for articles and 6.72 for reviews (see also A broader comparison of MNCS and MNRS can be performed when all German universities with at least 100 papers in 2012 are included. The MNRS correlates slightly larger than typical with the MNCS in this case, as indicated by the Spearman rank coefficient of 0.41 (for the interpretation of correlation coefficients, see The Directorate for Science, Technology, and Innovation in Brussels stated that the “use of indicators of social media is relatively well established in advertising and marketing. Some organisations are starting to collect altmetrics on a commercial basis in relation to research publications. In this context, little is known about what individual altmetrics mean. This appears to provide a rich set of opportunities for research – though we are probably some time away from being able to think in a precise way about concepts such as a ‘field-normalised tweet’” ( We found a high correlation between the MNCS and MNRS values for the ranking of journals. The MNRS values can be interpreted (analogously to MNCS values) such that a value of 1.5 indicates that a paper has 50% more Mendeley readers than an average paper in the same subject category. Following the rules of thumb by In addition to normalization methods which are based on average reader scores (MNRS), one can also use percentile-based approaches. Percentile-based approaches produce more robust normalized scores because they are not based on average values (of citations or readers) ( What are the limitations of our study? One can see our retrieval strategy on Mendeley reader counts via the API using the DOI as a limitation. However, as we found the vast majority of papers (94.8% of articles and 96.6% of reviews) with this method, we expect no major influence on our results from this retrieval strategy. Another limitation of our study is the exclusion of articles and reviews without a DOI. This reduces the number of publications from 1390,504 to 1198,184. Therefore, 86.2% of the articles and reviews of the WoS core collection of 2012 are included in this study. Under the assumption that publications with and without a DOI are evenly distributed across high and low impact publications, this will not alter our results significantly. This study is not intended to provide reader counts as reference values for later use. The main aim is to explore an established method from bibliometrics in the realm of altmetrics and to propose a method to normalize Mendeley reader counts in order to judge the reader impact of individual publications as well as aggregates of publications. The normalization procedure proposed in this study can in principle also be applied to other altmetrics data, such as tweets and blog posts, as it relies on an external classification system assigned to individual papers (or journals where the paper was published). However, normalization with respect to other altmetrics data requires that a large proportion of the publication set is covered by the specific type of altmetrics source. This is the case for Mendeley reader counts, but this might not be the case for other sources of altmetrics data. The interpretation of Mendeley reader counts seems to be more problematic than the interpretation of citation counts. Many scientists do not read papers in the Mendeley application or on the web interface. Often, scientists add a paper to their Mendeley library when they intend to read it. Although there are reasons to include a paper in one's Mendeley library other than reading it later, it has been proposed to interpret Mendeley reader counts as the number of citers to be (see above). In this study, we have proposed a method for the normalization of Mendeley reader counts that is based on an established method of normalization for citation counts. We have shown that this method is able to normalize Mendeley reader counts. Comparisons on the journal and university level show that the MNRS and MNCS correlate larger for 9601 journals than for 76 German universities. Since the MNRS has been derived from a well-known and accepted variant used in bibliometrics, the conceptual justification of the new indicator seems to be given. However, further large-scale empirical investigations are necessary. We encourage other researchers in scientometrics to calculate MNRS values for different research units (journals, researchers, institutions, etc.) in order to test the reliability and validity of this indicator (e.g., by comparing the MNRS with other performance indicators). The calculation of the MNRS needs further data besides data from Mendeley in order to have a field-classification scheme for normalization. In this study, we used the WoS subject categories as classification scheme. Since Mendeley users classify themselves in scientific disciplines ( The bibliometric data used in this paper are from an in-house database developed and maintained by the Max Planck Digital Library (MPDL, Munich) and derived from the Science Citation Index Expanded (SCI-E), Social Sciences Citation Index (SSCI), and Arts and Humanities Citation Index (AHCI) prepared by Thomson Reuters (Philadelphia, Pennsylvania, USA). The Mendeley reader counts were retried via the Mendeley API. 