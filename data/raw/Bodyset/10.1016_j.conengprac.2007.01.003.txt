On-line optimization provides a means for maintaining a process around its optimum operating range. This optimization heavily relies on process measurements and accurate process models. However, these measurements often contain random and possibly gross errors as a result of miscalibration or failure of the measuring instruments. This paper proposes a data reconciliation strategy that deals with the presence of such gross errors. Instead of constructing the objective function to be minimized on the basis of random errors only, the proposed method takes into account both contributions from random and gross errors using a so-called contaminated Gaussian distribution. It is shown that this approach introduces less bias in the estimation due to its natural property to reject gross errors.The problem of obtaining reliable estimates of the state of a process is a fundamental objective in process control and supervision, these estimates being used to understand the process behavior. For that purpose, a wide variety of techniques has been developed to perform what is currently known as data reconciliation ( Process measurements are inevitably corrupted by errors during the measurement itself but also during its processing and transmission stages. The total error in a measurement, which is the difference between the measured value and the (definitely unknown) value of a variable, can be conveniently represented as the sum of the contributions from two types of errors: random and gross errors. Random errors which are inherent to the measurement process are usually small in magnitude and are most often described by the use of probability distributions. On the other hand, gross errors are caused by nonrandom events such as instrument malfunctioning, miscalibration, wear or corrosion of sensors and so on. The nonrandom nature of these errors implies that at any given time they have a certain magnitude and sign which may be unknown. Thus, if the measurement is repeated with the same instrument under identical conditions, the contribution of a systematic gross error to the measurement value will be the same ( As previously said the measurements collected on the process may be unknowingly corrupted by gross errors. As a result, the data reconciliation procedure can give rise to absurd results and, in particular, the estimated variables are most often corrupted by these biased data. Several schemes have been suggested to cope with the corruption of normal assumption of the errors, for static systems ( The next section is devoted to recall the background of data reconciliation based on the assumption that all the measurements are normally distributed. Robust data reconciliation method is described in The classical general data reconciliation problem ( Clearly, the least-squares objective function in the previous formulation comes from the assumption that all the measurements are normally distributed, without taking into account gross errors that may be present. The influence of these gross errors on the estimates can be minimized by defining robust objective functions. For example, let us consider the following so-called generalized maximum likelihood objective function proposed by Let us define the influence function Using this notation, system Of course, the choice of the objective function introduced in To explain more the role played by this function, let us consider now a family of Cauchy functions parameterized by For The ability to limit the influence of gross errors can also be explained looking at the weight function. Indeed, the more the estimation procedure applies a large adjustment to the measurement of a variable in order to satisfy the process model, the less the weight of this adjustment influences the criterion to be optimized. In fact, this idea is very close to the classical so-called iterative measurement test (IMT) ( Another means to be unaware of the presence of gross errors consists of taking them into account a priori in the error probability distribution. A distribution based on the additive combination of two Gaussian distributions can be used. Indeed, if the measurements contain gross errors, then a single pdf described as in Let us now analyze the behavior of the weight function introduced in We now consider the case of a process characterized by two types of variables: macroscopic variables such as flowrates Using the same resolution scheme as previously, system    For nonlinear systems, the initialization remains a difficult task, convergence of the algorithm being generally sensitive to that choice. In the present situation, measurements are a natural choice for initializing the estimates (step 1 of the algorithm). The solution given by classical least-squares approach can also provide an acceptable initialization although its sensitivity to gross errors may be sometimes important; the reader will be able to verify that this solution may be obtained by redefining the distributions Let us now consider the more realistic situation where only some variables are measured. For that purpose, two selection matrices It is usual to represent the mass conservation laws of a given phase, species or property using an oriented graph. Such a graph schematizes the flowing of that phase, species or property. An arc in the graph corresponds to a stream and a node to a process equipment or to a group of process equipments. Generally, the basic conservation equations are written in a compact form using the graph incidence matrix. The method described in  It is important to note that the presence of the seven biased measurements had only a weak influence on the obtained estimated values. Indeed, In order to analyze the efficiency of the proposed data reconciliation method, corrective terms have been expressed in terms of a percentage of the measurement. Therefore, for each measured variable The analysis of these figures clearly shows the robustness to the presence of gross errors in the data. The contrast between the relative corrections of the biased measurements and the others is considerably enhanced by the proposed method. Only the biased measurements are strongly corrected and the smearing effect classically observed when using least-squares method is considerably minimized. The main goal of the paper is to provide a robust data reconciliation method which results are the more insensitive as possible to the presence of gross errors in the measurement set. However, it is clear that the obtained results, in particular the corrective terms, can be analyzed in order to detect and locate these gross errors. Indeed, the magnitude of the corrective terms, i.e. It is important to note that almost all the existing methods of gross error detection and localization are based on the crossing of a threshold related to the corrective terms (or any standardized corrective terms). When the statistical distribution of these terms is known (and/or calculable), this threshold can be determined on statistical considerations as, for example, the type I error of hypothesis testing. In the other cases, a similar methodology can be applied; however, the threshold must be fixed on an empirical way. For example, one can decide that a measurement contains a gross errors if its corrective term is greater than three or four times its standard deviation. This kind of analysis can also be done, of course, using the relative corrections. Another technique, described by As previously mentioned, the ratio between “normal” and “abnormal” standard deviations of the contaminated distribution do not really constitute a design parameter of the proposed method. Provided the standard deviation ratio If the proposed method functioned perfectly (not any propagation of biases to the healthy measurements), these averages of the relative corrections should be null except for the biased measurements. The results presented in that table are closed to this ideal case and one can notice that the results are not very sensitive to the choice of the contamination ratio. This important result was foreseeable taking into account the shape of the weight functions shown in To deal with the issues of gross errors influence on data estimation, the paper has presented a robust reconciliation approach. For that purpose, a cost function which is less sensitive to the outlying observations than that of least squares is used. The algorithm can handle multiple biases or gross errors at a time. For the given example, seven measurements among 45 have been biased and the proposed method has been able to provide a good estimation of the true values of the different variables of the process. The implementation of the algorithm is easy; moreover many simulations carried out showed a relative low sensitivity of the results to the design parameters, i.e. the standard deviation ratio The results of reconciliation will clearly depend not only on the data, but also on the model of the process itself. As a perspective of development of robust reconciliation strategies, there is a need for taking account of model uncertainties and optimize the balancing parameter