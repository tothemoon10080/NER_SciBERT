 Geostatistics is concerned with constructing high-resolution three-dimensional models of categorical variables such as rock type or facies and continuous variables such as mineral grade, porosity, or contaminant concentration. It is necessary to have At any instance in geologic time, there is a single true distribution of variables over each study area. This true distribution is the result of a complex succession of physical, chemical, and biological processes. Although some of these processes may be understood quite well, we do not completely understand all of the processes and their interactions, and could never have access to the boundary conditions in sufficient detail to provide the unique true distribution of properties. We can only hope to create numerical models that mimic the physically significant features. Uncertainty exists because of our lack of knowledge. Geostatistical techniques allow alternative realizations to be generated. These realizations are often combined in a histogram as a model of uncertainty. Conventional mapping algorithms were devised to create smooth maps to reveal large-scale geologic trends; they are low-pass filters that remove high-frequency property variations. The goal of such conventional mapping algorithms, including splines and inverse distance estimation, is There are often insufficient data to provide reliable statistics. For this reason, data from analogous, more densely sampled study areas are used to help infer spatial statistics that are impossible to calculate from the available data. There are general features of certain geologic settings that can be transported to other study areas of similar geologic setting. Although the use of analogous data is often essential in geostatistics, it should be critically evaluated and adapted to fit any hard data from the study area. A sequential approach is often followed for geostatistical modeling. The overall geometry and major layering or zones are defined first, perhaps deterministically. The rock types are modeled within each major layer or zone. Continuous variables are modeled within homogeneous rock types. Repeating the entire process creates multiple equally probable realizations. The uncertainty about an unsampled value Inference of any statistic requires some repetitive sampling. For example, repetitive sampling of the variable This trade of replication corresponds to the decision of stationarity. Stationarity is a property of the RF model, not of the underlying physical spatial distribution. Thus, it cannot be checked from data. The decision to pool data into statistics across rock types is not refutable Data are rarely collected with the goal of statistical representivity. Wells are often drilled in areas with a greater probability of good reservoir quality. Core measurements are taken preferentially from good-quality reservoir rock. These data-collection practices should not be changed; they lead to the best economics and the greatest number of data in portions of the reservoir that contribute the greatest flow. There is a need, however, to adjust the histograms and summary statistics to be representative of the entire volume of interest. Most contouring or mapping algorithms automatically correct this preferential clustering. Closely spaced data inform fewer grid nodes and, hence, receive lesser weight. Widely spaced data inform more grid nodes and, hence, receive greater weight. Geostatistical mapping algorithms depend on a global distribution that must be equally representative of the entire area being studied.  The covariance, correlation, and variogram are related measures of spatial correlation. The decision of stationarity allows inference of the stationary covariance (also called auto covariance): Spatial continuity depends on direction. Anisotropy in geostatistical calculations is The directions of continuity are often known through geologic understanding. In case of ambiguity, the variogram may be calculated in a number of directions. A The variogram is calculated and displayed in the principal directions. These experimental directional variogram points are not used directly in subsequent geostatistical steps such as kriging and simulation; a parametric variogram model is fitted to the experimental points. There are two reasons why experimental variograms must be modeled: (1) there is a need to interpolate the variogram function for A variogram model can be constructed as a sum of known positive-definite licit variogram functions called nested structures. Each nested structure explains a fraction of the variability. All nested structures together describe the total variability, σ An important application of geostatistics is to calculate estimates at unsampled locations. The basic idea is to propose a liner estimate of the residual from the mean: Least-squares optimization has been used for many years. The idea, proposed by early workers in geostatistics, was to calculate the weights to be optimum in a minimum squared error sense, that is, minimize the squared difference between the true value The geostatistical technique known as simple kriging is a least-squares regression procedure to calculate the weights that minimize the squared error. A set of The basic estimator written in The term Kriging estimates are smooth. The kriging variance is a quantitative measure of the smoothness of the kriging estimates. There is no smoothing when kriging at a data location, σ The idea of simulation is to draw multiple, equally probable realizations from the random function model. These realizatios provide a  Transform the original Go to a location Draw a random residual The simulated value is added to the data set and used in future kriging and simulation to ensure that the variogram between all of the simulated values is correct. A key idea of sequential simulation is to add previously simulated values to the data set. Visit all locations in a random order (return to step 2). There is no theoretical requirement for a random order or path; however, practice has shown that a regular path can induce artifacts. When every grid node has been assigned, the data values and simulated values are back-transformed to real units. Repeating the entire procedure with a different random number seeds creates multiple realizations. The procedure is straightforward; however, there are a number of implementation issues, including (1) a reasonable three-dimensional model for the mean Many algorithms can be devised using the properties of the multi-Gaussian distribution to create stochastic simulations: (1) matrix approaches (LU decomposition), which are not used extensively because of size restrictions (an The aim of the indicator formalism for categorical variables is to simulate the distribution of a categorical variable such as rock type, soil type, or facies. A sequential simulation procedure is followed, but the distribution at each step consists of estimated probabilities for each category: Object-based models are becoming popular for creating facies models in petroleum reservoirs. The three key issues to be addressed in setting up an object-based model are (1) the geologic shapes, (2) an algorithm for object placement, and (3) relevant data to constrain the resulting realizations. There is no inherent limitation to the shapes that can be modeled with object-based techniques. Equations, a raster template, or a combination of the two can specify the shapes. The geologic shapes can be modeled hierarchically—that is, one object shape can be used at large scale and then different shapes can be used for internal small-scale geologic shapes. It should be noted that object-based modeling has nothing to do with object-oriented programming in a computer sense. The typical application of object-based modeling is the placement of abandoned sand-filled fluvial channels within a matrix of floodplain shales and fine-grained sediments. The sinuous channel shapes are modeled by a one-dimensional centerline and a variable cross section along the centerline. Levee and crevasse objects can be attached to the channels. Shale plugs, cemented concretions, shale clasts, and other non-net facies can be positioned within the channels. Clustering of the channels into channel complexes or belts can be handled by large-scale objects or as part of the object-placement algorithm. Object-based facies modeling is applicable to many different depositional settings. The main limitation is coming up with a suitable parameterization for the geologic objects. Deltaic or deep-water lobes are one object that could be defined. Eolean sand dunes, remnant shales, and different carbonate facies could also be used. The indicator approach to categorical variable simulation was mentioned earlier. The idea of indicators has also been applied to continuous variables. The key idea behind the indicator formalism is to code all of the data in a common format, that is, as The aim of the indicator formalism for continuous variables is to estimate directly the distribution of uncertainty All hard data The cumulative distribution function at an unsampled location at threshold The method of simulated annealing is an optimization technique that has attracted significant attention. The task of creating a three-dimensional numerical model that reproduces some data is posed as an optimization problem. An objective function measures the mismatch between the data and the numerical model. An initial random model is successively perturbed until the objective function is lowered to zero. The essential contribution of simulated annealing is a prescription for when to accept or reject a given perturbation. This acceptance probability distribution is taken from an analogy with the physical process of annealing, where a material is heated and then slowly cooled to obtain low energy. Simulated annealing is a powerful optimization algorithm that can be used for numerical modeling; however, it is more difficult to apply than kriging-based methods because of difficulties in setting up the objective function and choosing many interrelated parameters such as the annealing schedule. Therefore, the place of simulated annealing is not for conventional problems where kriging-based simulation is adequate. Simulated annealing is applicable to difficult problems that involve (1) dynamic data, (2) large-scale soft data, (3) multiple-point statistics, (4) object placement, or (5) special continuity of extremes. Reconciling data from different scales is a long-standing problem in geostatistics. Data from different sources, including remotely sensed data, must all be accounted for in the construction of a geostatistical reservoir model. These data are at vastly different scales, and it is wrong to ignore the scale difference when constructing a geostatistical model. Geostatistical scaling laws were devised in the 1960s and 1970s primarily in the mining industry, where the concern was mineral grades of selective mining unit (SMU) blocks of different sizes. These techniques can be extended to address problems in other areas, subject to implicit assumptions of stationarity and linear averaging. The first important notion in volume-variance relations is the spatial or dispersion variance. The dispersion variance   The profile of porosity and permeability from two wells from an offshore petroleum reservoir are shown at the bottom of   Technique to assign relative weights to different data values based on their redundancy with nearby data. Closely spaced data get less weight. After the name of D. G. Krige, this term refers to the procedure of constructing the best linear unbiased estimate of a value at a point or of an average over a volume. Nonunique grid of simulated values. A set of realizations is used as a measure of uncertainty in the variable being studied. Procedure of adding correlated error by Monte Carlo to create a value that reflects the full variability. Basic tool of the theory used to characterize the spatial continuity of the variable.