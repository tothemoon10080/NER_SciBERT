Parameter inference is a key aspect of spatial modeling. A major appeal of variograms is that they allow inferring the spatial structure solely based on conditioning data. This is very convenient when the modeler does not have a ready-made geological interpretation. To date, such an easy and automated interpretation is not available in the context of most multiple-point geostatistics applications. Because training images are generally conceptual models, their preparation is often based on subjective criteria of the modeling expert. As a consequence, selection of an appropriate training image is one of the main issues one must face when using multiple-point simulation. This paper addresses the development of a geostatistical tool that addresses two separate problems. It allows (1) ranking training images according to their relative compatibility to the data, and (2) obtaining an absolute measure quantifying the consistency between training image and data in terms of spatial structure. For both, two alternative implementations are developed. The first one computes the frequency of each pattern in each training image. This method is statistically sound but computationally demanding. The second implementation obtains similar results at a lesser computational cost using a direct sampling approach. The applicability of the methodologies is successfully evaluated in two synthetic 2D examples and one real 3D mining example at the Escondida Norte deposit.Geological models are often built using deterministic techniques, meaning that their construction relies on the knowledge and experience of a specialist that assigns geological attribute values to a given volume. This practice is however not satisfying because it does not allow quantifying geological uncertainty ( Curvilinear or complex structures can be poorly represented by Gaussian simulations and may require higher order statistics ( Variograms do not inform about contacts or contact geometries between different categories (e.g. The reliance of variogram-based Geostatistics on the maximum entropy, multi-Gaussian distribution to model all statistics beyond the two-point statistics results in maximum disconnectivity of extremes and the reproduction of only linear spatial features (e.g. As an alternative, multiple point simulation algorithms, have recently become an important point of focus, with a wealth of different methods developed in the last decade ( Two different types of data can be distinguished to serve as a base for the selection of training images. The first type is indirect state data such as flow and transport, which are typically integrated through inverse methods. The problem consists in evaluating the compatibility of a training image with dynamic outputs (for example time series of contaminant output). Approaches to select training images on this basis have been proposed in the framework of distance-based approaches ( The other type of data, which is the specific focus of this paper, is static data. As for variograms, it consists in quantifying the consistency of a training image based on spatial statistics derived on both training image and data. To date, significant developments are lacking regarding objective criteria for verifying high-order training image consistency with the available scattered data. In this paper we focus on the consistency problem, and we leave aside some related issues as for example training image scaling issues (e.g. One of the first approaches proposed for training image selection based on static data was initially proposed by A different approach is suggested by As an alternative to the methods mentioned above, we note that spatial cumulants are promising because they offer a parametric description of the high-order spatial statistics ( This paper addresses the development of a geostatistical tool that provides two measurable criteria for selecting training images based on their consistency with given data. The first method, applicable in cases where more than one conceptual model is available, allows ranking training images according to their compatibility with a data set in terms of low and high order spatial structures. This represents a The purpose of the proposed algorithm is to generate a ranking of several training images, according to their compatibility with conditioning data. In essence, the algorithm is given a conditioning data set and a series of training images. The method works by defining conditioning data events, which are patterns of spatially distributed data values, and computing their frequency of occurrence in the different training images. The training images that have a higher frequency of data events are deemed more coherent with the data. The result is a ranking of the training images according to their data consistency. The overall algorithm can be divided in a number of steps whose implementation is described below in details. We first present an algorithm that is statistically straightforward but computationally inefficient. In a second step, we present an equivalent alternative that yields similar results with a much lesser computational burden. The first step of the method is to migrate the scattered data to a regular grid. The grid structure permits to accelerate future spatial searches of data events, therefore reducing computation times. To avoid scale issues, the nodes spacing in the user defined migration grid must be the same as in the training images. The algorithm migrates data points to the closest node of the grid. Once migration is completed, conditioning data events ( The data events are then used to compare training images with data. It should be noted that data events with a small number of nodes can be used to reflect lower-order statistics, whereas data events with increasing nodes will represent a higher order spatial distribution. For example, using a set of patterns of 1 node allows representing the marginal distribution, patterns of 2 nodes can represent 2-point dependence (similar to a variogram), etc. The appealing aspect of this approach is that it allows for comparisons using multiple-order criteria. In the following, we will use equivalently the terms of “number of neighbors” and “data event order”. Data events are repeatedly extracted from the conditioning data, and similar patterns in the different training images are inventoried. Event occurrences are searched using a linear search path that runs over all the positions contained in the images. It is considered that an event found in the training image matches the conditioning data event if the positions and facies of all nodes of the conditioning event are matched by those in the training image. Since it can be difficult to find an exact match, a tolerance parameter Considering, Finally, the relative compatibility between the training images and the conditioning data set is calculated by normalizing the sum of frequencies of each conditioning event in a given training image. This relative compatibility is calculated with the following equation: The previously proposed method is statistically sound as it checks the occurrence of conditioning patterns in every position of the ranked training images. However, because of the high amount of positions that need to be analyzed, the CPU cost is important. A faster alternative approach inspired from Direct Sampling ( Scattered conditioning data points migration and conditioning event definition remain unchanged. However the occurrence count of conditioning events in the images and compatibility calculations are achieved differently. In this case, the search for occurrences of a conditioning pattern in the training images is done using a random path that goes through the different training images. Considering the tolerance parameter Compared to the previous method consisting in scanning the entire training image, the user needs to define the maximum fraction The compatibility between the conditioning data set and the training images is finally calculated using the following equation: In several cases it can be desirable to have an absolute measure of the compatibility between training images and data. For example, it may not be relevant to establish that one training image is slightly better than another, if both training images are grossly incompatible anyway. We therefore propose an absolute compatibility measurement, which is represented by the proportion of patterns contained in the data that are matched by the training image. This absolute compatibility is computed by analyzing each training image separately, as this absolute measure depends only of the analyzed training image and is independent from other images. It fluctuates between values of 0 and 1: if each scanned conditioning event is found in the training image, which would represent the best possible case, the absolute compatibility is 1. Conversely, if no data event from the data is found in the training image, the value is 0. To calculate this measure, an indicator ( This section presents three application examples of the proposed method. In all examples, 0 is used for tolerance In this example, 3 different parameter sets were used to construct binary models using the TiGenerator tool implemented in SGeMS ( Four sample sets were obtained, consisting of 1091, 222, 80 and 36 random samples, and then used for determining training image compatibility with all three images corresponding to the first model in each example ( Compatibilities were obtained considering conditioning events of orders: 1, 2, 3, 4, 5, 10, 15, 20, 25 and 30 ( This is explained by a poor and non-representative inference of such high order spatial structures in poorly informed scenarios. This insufficient amount of data for proper inference can be diagnosed by analyzing the change in the amount of patterns of a given order found in the data grid. This value is reported in the output file of line 8. The results of this analysis are shown in This allows deciding whether the inference can be considered representative or if the inference is impossible given the amount of available data. In the 222 and 80 samples examples, it is possible to see an important drop in the amount of patterns found when the order of conditioning events rises, while in the 36 samples example no patterns of order 15th or higher can be found given the search neighborhood used. Furthermore, in the 36 data example, no clear differentiation between training images is observed before the results become erratic. This example shows that the selection tool is not able to differentiate effectively training images in scarcely informed scenarios. The same example is used to test the direct sampling based compatibility measurement algorithm. For each example, the same parameters and inputs considered for the exhaustive scanning method are used. Additionally a training image scan proportion of The results are similar to those obtained with the exhaustive scanning. The main advantage of this method is the speed up of the analysis. A comparison of the computing times is shown in The computing time reduction is substantial when the analyzed data event is of a low order. For high order events, the time reduction is not as important, but still significant, especially for high number of samples. The time reduction depends on the frequencies of data patterns in the analyzed training image set. When large scale event occurrences are rare or even inexistent, the direct sampling method needs to analyze all positions in the images to find a matching occurrence (when the image checking proportion The particular behavior of the computing times in the 80 samples example is due to the drop in the number of conditioning events found (see Absolute compatibility is obtained using the same setting. Results regarding the 1091 samples set are summarized in The results shown above were obtained by considering training images and models for data extraction, both generated through the use of the same method (TiGenerator). In order to further test the training image selection tool, new samples are extracted from a hand-drawn model ( It can be observed that the hand-drawn model has similar structural characteristics to those observed in training image 1. Results are as expected, showing little differences for low orders and an increasing differentiation in favor of image 1 when higher order patterns are analyzed. The behavior of relative compatibility results remains erratic when the available conditioning data are scarce and the complexity of the analyzed patterns is high. The model consists in a realistic alluvial formation obtained with process-based simulation ( Relative compatibilities between training images and 1093 and 222 samples data sets were calculated using the direct sampling approach. Results are summarized in In example 1, similar facies proportions in the training images caused relative compatibilities to become noticeably different only when analyzing conditioning events of higher order. In this example, large differences between images in terms of spatial structures and proportions result in higher variations of relative compatibility, even when low order events are considered and a lower amount of samples is available. Care should be taken when analyzing the results, as these can be driven by the differences in proportions rather than in spatial structure, especially when low order events are considered. Spikes observed in data set #5 can are explained by a low amount of high order conditioning events found. This example shows that the method allows obtaining the most consistent training image of a 3D vertical non-stationary alluvial formation. This can be particularly useful when modeling non-stationary models by assigning a different training image for each zone of the model ( This example is used to address the applicability of the developed tool on a real mine-scale example. In this example, the relative compatibility of different 3-D geological models of the Escondida Norte deposit with the available drill-hole dataset is measured. Minera Escondida is the largest copper mining operation in the world reaching an annual production of 1 Available information corresponds to the current deterministic geological model of the lithologies at Escondida Norte open pit area and to 534 drill-holes. Drill-hole data are composited at 5 [m] length, which means that a total of 26 The spatial location of the available information and the lithologies codification are shown in At the beginning, the available deterministic model is used as training image to build 3 groups of 10 realizations with the Direct Sampling simulation algorithm. Each group is constructed using the different parameter sets shown in Next, the simulations are grouped 3 by 3, each group consisting of one realization obtained with each parameter set. We have in each group one simulation constructed with a low-distance threshold (good reproduction of the training patterns), one realization constructed with a medium and one realization with a high distance threshold (poor reproduction of the training patterns). Realizations with each setting are shown in Interesting features of the grouped realizations are obtained through the use of this methodology. Among these, each realization honors the conditioning data, all have the same lithologies distribution, and variograms computed over the different realizations do not vary significantly (see The parameters used to perform the training image selection are summarized in Relative compatibilities were computed for each of the 10 training image sets. Four different conditioning data sets were used to test the method: the first consisted of all the available drill-holes, a second which considers only a randomly selected 50 [%] of the drill-holes, while the third and fourth conditioning data sets consisted of 15 and 5 randomly selected drill-holes The mean relative compatibilities obtained by image is shown in Results show that the training image selection method is able to select the more compatible training image in terms of high-order spatial structure if 15 or more drill-holes are available. As in example 1, when compatibility in terms of low-order spatial structure is analyzed the method shows no particular preference for a given training image since all the images have the same lithologies proportions (1st order) and very similar 2nd order spatial structure (variograms). However, when the order of the analyzed patterns increases, the preference for the expected training image becomes noticeable. This is not clearly observable in the 5 drill-hole data set due to a lack of high-order patterns. This confirms the limitations of the proposed method when the available conditioning information is scarce. This paper discusses two criteria (relative and absolute compatibility) to compare a data set with candidate training images, and presents a corresponding computer code. One advantage of our comparison approach is that it simultaneously considers different sizes and orders of data events. The relative compatibility measure is especially useful as a training image selection tool in cases when more than a single conceptual model is available as training image. However, it does not allow assessing if all training images are incompatible with the available data. This is palliated by the absolute comparison criterion. For both relative and absolute comparison criteria, two calculation approaches are tested, exhaustive scanning and direct sampling. Both show similar results, but considerably lower computation times are obtained when using the direct sampling approach, especially when lower order events compatibility is assessed. The proposed methods are successfully applied in three examples. The second example shows that the method can also be used as an aid to geological interpretation since it allows identifying the origin of data taken at different depths in a vertically non-stationary 3D volume. The third example demonstrates the applicability of the method to assess training image spatial compatibility with data in real cases. In this sense, a proper Bayesian training image inference method could represent a future research direction. The method would consist in proposing alternate training images with a prior probability based on geological expertise, and then rejecting them based on compatibility measures. In addition to offering the ability to verify training image consistency with data, our method could also be used to assess the validity of spatial parameterization in other contexts, for example in object-based-models, to select sets of rules for the placement of objects. In case of non-stationary MPS simulation, our method is still applicable. When a zonation approach is used with a different training image for each zone, training image consistency should be applied separately for each zone ( Our absolute compatibility metric is presented in a single way (data patterns occurrences are searched in training images), but could also be done looking at the reverse. If every data event is found in the training image, this does not mean that every training image pattern is found in the data. Looking at this reverse problem represents another interesting future research direction, as this measure of compatibility would also consider the completeness of data for the characterization of the training image. If data are scarce and structural patterns complex, this index would be low. The work presented in this paper was financially supported by the Supplementary data associated with this article can be found in the online version at  