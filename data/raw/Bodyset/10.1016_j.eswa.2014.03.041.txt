In this paper, genetic algorithm oriented latent semantic features (GALSF) are proposed to obtain better representation of documents in text classification. The proposed approach consists of feature selection and feature transformation stages. The first stage is carried out using the state-of-the-art filter-based methods. The second stage employs latent semantic indexing (LSI) empowered by genetic algorithm such that a better projection is attained using appropriate singular vectors, which are not limited to the ones corresponding to the largest singular values, unlike standard LSI approach. In this way, the singular vectors with small singular values may also be used for projection whereas the vectors with large singular values may be eliminated as well to obtain better discrimination. Experimental results demonstrate that GALSF outperforms both LSI and filter-based feature selection methods on benchmark datasets for various feature dimensions.The goal of text classification, or categorization, is to classify texts of interest into appropriate classes. Along with the increase in the number of electronic documents, text classification has received more attention to be able to organize these documents appropriately. A conventional text classification framework mainly consists of feature extraction, feature selection and classification stages. Feature extraction stage simply extracts numerical information from raw text documents. For this purpose, most of the studies use bag-of-words technique ( At the end of the feature extraction stage, hundreds or even thousands of features are obtained depending on the size of the document collection. Excessive numbers of features not only increase computational time but also degrade classification accuracy. Therefore, dealing with high dimensionality of the feature space is one of the most critical issues in text classification. Various feature selection methods are usually employed to overcome this issue. Feature selection methods can be divided mainly into three categories: filter, wrapper and embedded ( As an alternative to feature selection, feature transformation approaches are also used to reduce feature dimension. However, these approaches project the original feature space into a new lower-dimensional subspace rather than selecting from the original set of features. Although there exist many feature transformation methods, majority of the text classification studies prefer latent semantic indexing (LSI) due to its proven performance ( While either feature selection or feature transformation methods can be individually used for dimension reduction, combinations of these methods are also possible. Moreover, these combinations may provide even better performance. As an example, a two-stage feature selection strategy consisting of various feature selection methods and LSI is proposed for text classification in ( Considering the feature transformation, there are also several efforts projecting the data in a different way than that of LSI or PCA. For instance, selection of the best subset of principal components among all rather than using those with the largest eigenvalues are found as an efficient method to determine the optimal multivariate regression model in ( Inspiring from the abovementioned approaches; in this paper, genetic algorithm oriented latent semantic features (GALSF) are proposed for text classification task. The proposed method consists of two stages, namely feature selection and feature transformation. The feature selection stage is carried out using the state-of-the-art filter-based methods. The feature transformation stage employs LSI empowered by genetic algorithm (GA) such that a better projection is attained using appropriate singular vectors, which are not limited to the ones corresponding to the largest singular values, unlike standard LSI approach. In this way, the singular vectors with small singular values may also be used for projection whereas the vectors with large singular values may be eliminated as well to obtain better discrimination. Effectiveness of the proposed method is comparatively evaluated against feature selection, and the combination of feature selection and transformation on two-class and multi-class text collections, namely Enron1, Ohsumed and Reuters-21578. For all collections, GALSF surpasses the other methods in terms of classification performance in almost all cases. Moreover, it is proven that the singular vectors providing better discrimination contain the ones corresponding not only to large but also small singular values rather than the largest singular values alone. Rest of the paper is organized as follows: feature selection approaches used in the study are briefly described in Section In this paper, two state-of-art filter methods are employed for the feature selection task. These are namely distinguishing feature selector (DFS) introduced by DFS selects distinctive features while eliminating uninformative ones considering the following term characteristics ( A term frequently occurring in single class and not occurring in the other classes is discriminative. A term rarely occurring in single class and not occurring in the other classes is irrelevant. A term frequently occurring in all classes is irrelevant, too. A term occurring in some of the classes is relatively discriminative. DFS score of a term in a given text collection is simply computed as In statistics, the CHI2 test is used to examine independence of two events ( As mentioned earlier, LSI is known as one of the most representative feature transformation approaches which transforms the original data to a more discriminative lower-dimensional subspace ( In this phase, LSI reveals hidden concepts such as synonym and polysemy. Therefore, Genetic algorithm is a suboptimal search method stimulated from biological evolution process ( When GA is used for attribute or feature selection task, chromosome length is set to the dimension of the original set of features. The chromosomes are then encoded with binary (0, As explained previously, The fitness value in GA is defined as the well-known Micro-F1 measure ( All the steps in GALSF approach can be listed as follows: Perform filter-based feature selection to obtain relevant features among all and to reduce dimension. Utilize GA oriented LSI method on the selected feature subset in previous step to obtain a new projection providing better discrimination and further dimension reduction. Project the selected features in Step 1 into the new semantic subspace computed in Step 2 to obtain GALSF. Feed GALSF to a pattern classifier for the recognition of the given document. In the experimental work, efficiency of the proposed method is thoroughly investigated and compared to the current approaches in the literature. Experimental settings including the utilized datasets, classification algorithm, and GA parameters are first briefly described. Then, the profile of singular vectors, which are used to obtain GALSF, and the accuracy analysis are provided. In the experiments, three distinct datasets with varying characteristics were used for the assessment. The first dataset consists of top-10 classes of the celebrated Reuters-21578 ModApte split ( During feature extraction from text documents, two preprocessing tasks, namely stop-word removal and stemming, were carried out. Also, TF-IDF ( For classification task, support vector machine (SVM), which is one of the state-of-the-art pattern classification algorithms, were employed. SVM basically aims to obtain maximum-margin hyperplane in a transformed feature space using the kernel trick ( GA parameters of GALSF method were defined as follows: population size is 100, number of generations is 20, probability of crossover is 0.8, and probability of mutation is 0.08. As indicated before, the fitness value is defined as the Micro-F1 score obtained from classification of the test samples in the datasets. The provided experimental results are the best of 10 runs. GALSF were obtained by applying filter-based feature selection first and then GA oriented LSI. In standard LSI procedure, the singular vectors constituting the feature transformation matrix always corresponds to the largest singular values. On the contrary, GA oriented LSI has no such limitation depending on the idea that the singular vectors corresponding not only to large but also small singular values may form a transformation matrix that provides more discrimination. As an example, in In this part, contribution of GALSF to the classification performance is comparatively evaluated against the existing approaches. The evaluation is carried out using various numbers of the selected features to observe efficiency of the proposed method in each case. Specifically, 1%, 2.5%, 5% and 10% of the whole feature set, which are initially selected by the filter methods (DFS, CHI2), are forwarded into GALSF to obtain the proposed features (DFS+GALSF, CHI2+GALSF). Those features are finally fed into SVM for classification. GALSF are compared (in terms of the attained Micro-F1 scores) to the features directly selected by feature selection methods (DFS, CHI2), and to the features obtained by the combinations of feature selection and transformation (DFS+LSI, CHI2+LSI) for each dataset. In case of (DFS+LSI, CHI2+LSI), various numbers of singular vectors, ranging from 10% to 100% of all vectors, were used to form the transformation matrix. Obviously, if 100% of the singular vectors are used, no further dimension reduction is applied. Hence, the resulting transformation will yield the same feature subset selected by the regarding feature selection method in previous step. The results of the conducted experiments on Reuters dataset are presented in One can note from these figures that the proposed framework (either DFS+GALSF or CHI2+GALSF) outperforms the others in almost all cases. The only exception is for Ohsumed dataset when 10% of the features selected by CHI2 are used. Even for this case, the performance of GALSF was still better than the performance of LSI at the same dimension. The amount of singular vectors selected by GA algorithm was always around 50% of all vectors. The runner-up of this analysis was the combination of feature selection and transformation obtained by standard LSI (DFS+LSI or CHI2+LSI) with just a few exceptions where DFS beat (DFS+LSI) and (CHI2+LSI) on Ohsumed and Enron1 datasets if 1% of the features are selected. Thus, individual feature selection methods (DFS and CHI2) took the last place in this analysis. Based on this analysis, it can be stated that the proposed method not only provide improved accuracy over both feature selection and combination of selection and transformation but also offers further dimension reduction with respect to individual feature selection methods. Since both (DFS+GALSF) and (CHI2+GALSF) are superior to the other approaches, one can also state that the efficiency of the proposed framework is independent from the utilized feature selection method. These statements are valid for all datasets. In this study, genetic algorithm oriented latent semantic features (GALSF) are proposed to obtain better representation of documents in text classification. Specifically, GALSF aims to find out a better projection using appropriate singular vectors, which are not limited to the ones corresponding to the largest singular values, unlike standard LSI approach. GA is employed in the selection of appropriate singular vectors. GALSF is obtained by applying filter-based feature selection first and then GA oriented LSI. One of the key differences of the proposed approach is to use a recently developed feature selection method (DFS) within a hybrid (selection