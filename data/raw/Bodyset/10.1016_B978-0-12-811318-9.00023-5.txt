Reliability and risk analysis concerns analyzing and predicting the state transition of engineered systems given historic information. With an increasing volume of available data, conventional machine learning algorithms may fail to capture hidden patterns behind the reliability engineering problems due to the curse of dimensionality. Autoencoders, a class of deep learning frameworks, have been reviewed in this study. They are a powerful tool in extracting hidden representations and producing a robust reconstruction for further predicting tasks. We have developed autoencoders and applied them on four data sets. Results showed that autoencoders can not only classify the states at a good accuracy, but also help to discover the failure mechanism.Reliability and risk analysis is engineering means that studies the stability and risk of a system or component, emphasizing the prediction of the evolving state of this system under given conditions. For example, product or maintenance engineers need to estimate the ability of a product or a facility to function so that the service life can be accurately predicted. By studying the historic trend of stocks, financial engineers make trading decision for maximizing the return. Despite the different domains, reliability and risk analysis involves the analysis of the historic data, and a quantified prediction or a decision is then made, which can be the probability of failure of one system Thanks to the development of various data collection devices, the available data grow to a degree that it is difficult to process them manually. Decades ago a civil engineer might have to check the structures every week and combine his previous experience to estimate the safety factor. Nowadays one can easily get different sources of data from sensors at a frequency of 1 million Hz. Traders are not limited to the open and close price provided by the stock market. They also use mass media and social websites to analyze others' attitudes towards the future economics for making better decisions. With the explosion of available data, data-driven methods, particularly machine learning based methods, have been developed and achieved a considerable success. Reliability and analysis is one of the most widely studied topics in Civil Engineering and Mechanical Engineering. Many case studies in this domain involve complex failure mechanism and the unpredictable failure process. For example, stress corrosion cracking (SCC) is one of the most dangerous failures due to the fast crack propagation without any obvious signs. In addition, the mechanism and the variables affecting the cracking process For industries requiring real-time quality monitoring, reliability and risk analysis can be conducted for early defect identification to avoid the potential loss. For example, semiconductor foundries often use hundreds of processes; an undetected failure during one process can lead to a considerable reduction in the yield rate. Kim et al. For some reliability engineering problems, particularly some structures which have a spatial distribution on the failure or error patterns, it is more useful to use the related spatial information. For example, an underground tunnel might have some areas where failure occurs frequently. Similarly, a mine site may have some areas where the chance of collapse is obviously higher than in the other places. Due to the limitations in conducting in situ experiments, most of the available information can only be obtained from observations. In this case, spatial data can be a valuable source of information. Spatial data is often topological or distance information conveying some quantified observations or events like failure frequency, stress level, or environmental variables. Cross-sectional data are also a common type, collected by recording many features of a system or components at the same point of time. The opportunity to observe or conduct experiments is scarce in some situations, so the engineers have to collect enough data at one time. This type of data can lead to a lack of useful information, since reliability problems are often time dependent. However, a reasonable reliability and risk analysis A number of challenges exist in reliability and risk analysis of engineered systems. First, complex working mechanisms make it difficult for an inexperienced engineer to monitor engineered systems. To solve this problem, automatic monitoring systems, e.g. the distributed sensor networks, have been applied to real-time monitoring and diagnosis to collect health information of systems at relatively low cost. Secondly, it is difficult to interpret the raw data collected from monitor instruments. In general, domain-dependent feature engineering is needed to reconstruct the raw monitoring information. If the distributed sensor networks are employed, then signal processing techniques are required to denoise electronic monitoring signal. Moreover, some threshold values need to be set based on domain-dependent knowledge Another challenge lies in developing algorithms for conducting reliability and risk analysis of engineered systems. In general, a real value, e.g., remaining useful life (RUL) and health state obtained from algorithms, is needed for planning maintenance. Algorithms play a critical role in mapping monitoring information to such values. To conclude, monitor instruments, feature engineering, and algorithms are three challenges in reliability and risk analysis of engineered systems. With the development of smart sensors, the available information increases to a degree that traditional domain-dependent feature engineering no long works  As a representation-learning based method, deep learning consists of multiple layers of non-linear representation, exempting one from complex and hand-engineering features. Generally, machine learning problems can be divided into supervised learning and unsupervised learning. The former requires that the data include the targets or labels, which can be predicted through mapping the An autoencoder is an artificial neural network attempting to reproduce the original input by encoding and decoding. A simple autoencoder consists of an encoder and a decoder, as shown in By adding the penalty or changing the architectures of the encoder and the decoder, one can obtain the regularized autoencoders to cope with different tasks. It is noted that the hidden representations produced by the encoder can be used as the input for other machine learning methods, helping to initialize a network. In addition, autoencoders are often used to compress the input data, which is also known as dimension reduction, through restricting the number of output neurons of the encoder. Since the MLPs are often employed as the basic framework of autoencoders, it is natural to perform function estimations to describe the relationship between the original input and the latent variables. We can also define the autoencoders from a probabilistic perspective by using distributions Though the encoder is often designed to compress the data by reducing the dimensions of the input, assigning a large number of output neurons for the encoder (probably larger than the input dimensions) can also extract useful information, particularly from the spatial information. This type of autoencoder is known as a sparse autoencoder (SAE), which was originally implemented by imposing a sparsity constraint on the encoder: L1 penalty or L1 norm, which is the sum of the output of the encoder Proposed by Vincent et al. During the denoising procedure, the model needs to sample an observation It is noted that in DAE, the loss function should be For many unsupervised learning methods, probabilistic modeling and maximum likelihood estimation (MLE) are employed due to the efficiency and consistency. Recently there has been an increasing interest in interpreting learning as a construction of an unnormalized energy surface. Based on this definition, encoding is actually a procedure of finding the local minima of this energy surface. Score matching was then introduced to learn the parameter Based on a more generalized definition of autoencoders introduced in DAE, the encoding procedure can be viewed as an inference procedure Proposed by Kingma and Welling As was mentioned, VAE learns to give rise to a probabilistic distribution of the input data through the encoder's inference, which is an approximation on the true posterior For each input data point For many real-world problems, the data may represent a strong spatial correlation or temporal correlation. For example, image processing and object recognition tasks involve the spatial information, but the conventional autoencoders are not designed to capture this 2D image structure, or represent the dynamic temporal relationship. In this sense, some advanced autoencoders, which can combine with interesting deep architectures like convolutional neural network (CNN) The autoencoders we described above contain only one encoder and one decoder. However, it is possible to build a deep autoencoder, which can bring many advantages. By stacking multiple layers for encoding and a final output layer for decoding, a stacked autoencoder, or a deep autoencoder, can be obtained.  Three autoencoders introduced above, SAE, DAE, and VAE, were applied on four data sets consisting of multivariate time series signals obtained from an aircraft engine run-to-failure simulation Three autoencoders, SAE, DAE and VAE, are employed for feature reconstruction. The original features involve multiple sensor collecting variables, and domain-dependent knowledge is needed to conduct reliability and risk analysis with physical models  Two methods were adopted to label the data. Method 1 assumes that the first 20% observations in each series represent a healthy state and the last 20% represent a degraded state. Method 2 adopts the same ratios for the healthy state and degraded state, except that the middle 20% (between 40% and 60%) were labeled as the degrading state. Each data set was split into a training set (70%) and a test set (30%). The former was used for constructing the model and the latter was for testing the performance. In addition, 10-fold cross validation was conducted to obtain the optimal parameters during training: nine of them were used to build a model and the remaining one then used to evaluate the predicting performance of this model. Bayesian optimization Three classification metrics, accuracy, precision, and area under and a receiver operating characteristic curve (AUC), have been calculated for the four test sets, as shown in To explain the drop in the accuracy of 3-state data set, we applied VAE on the FD001 data set and obtained the mean ( One important result VAE uncovered is that the states are not distributed evenly in the time-series data. By defining the top 20% data points as one state (healthy) and the bottom 20% data points as the other state (degraded), our models can classify them well. But adding the center 20% observations as one state (degrading) weakened the classification performance of the model, because the degrading state and the healthy state seem to be from the same cluster. Based on the latent feature representations produced by VAE, the degraded states might occur and accumulate quickly, leading to a sudden failure of an engine. By showing the latent distribution of the input data, VAE could help gain some insights into the failure mechanism of this system. In this study, we reviewed the application of machine learning in reliability and risk analysis. Autoencoders, an unsupervised deep learning framework producing latent feature representations, are believed to be a useful tool in this domain. We mainly introduced three types of autoencoders, SAE, DAE, and VAE, from a probabilistic perspective. Then we