Background
                  Although classification systems of olecranon fractures are important to help choose the best treatment and to predict prognosis, their degree of observer agreement is poorly investigated. The objective of this study was to investigate the intraobserver and interobserver reliability of currently used classification systems for olecranon fractures. Our hypothesis is that the Colton classification presents an acceptable agreement because it is simpler to use; on the other hand, considering the AO classification's complexity, we expect it to reach a lower level of agreement.
               
                  Methods
                  Radiographic images of elbow joint fractures were classified according to Colton, AO, Mayo, and Schatzker classification systems. The raters were 8 orthopedic surgeons split into 2 groups with 4 participants each, one with specialists in upper extremity surgery and the other with orthopedic surgeons without a specific focus on upper extremity surgery. This first procedure was the pretest training, aimed at calibrating participants' judgment. Image classification was conducted after all training was completed. After 30 days from the initial rating session, the test was conducted once again following the exact same procedures.
               
                  Results
                  The Colton classification has substantial intraobserver and interobserver agreement for specialists and nonspecialists. The Schatzker classification revealed a fair agreement for both specialists and nonspecialists. A fair concordance was also found for the Mayo classification. The AO classification demonstrated a moderate rate of agreement for specialists, whereas nonspecialists presented slight intraobserver agreement.
               
                  Conclusion
                  No classification system is widely accepted because it can be affected by interobserver variability, which can raise questions about its use in a research as well as in a clinical context.Olecranon fractures represent about 10% of all upper limb fractures. Fracture classifications are useful to characterize a problem, to suggest a prognosis, and to assist in choosing the most appropriate treatment. Given this gap in the literature, the objective of this study was to investigate the intraobserver and interobserver reliability of currently used classification systems for olecranon fractures. Our study included 8 orthopedic surgeons who work in the same department. Orthopedic surgeons were split into 2 groups with 4 participants each, one with specialists in upper extremity surgery and the other with orthopedic surgeons without a specific focus on upper extremity surgery. To the best of our knowledge, there is no consensus regarding power analysis for observer agreement studies. Therefore, we did not conduct a sample size calculation. Eighteen cases of elbow joint fractures with anteroposterior and profile (lateral view of the elbow) radiographic images were selected from the records of the Hospital Municipal São José (Joinville, SC, Brazil) from July to December 2012. Images were retrieved by 2 third-year orthopedic residents and by 1 orthopedic surgeon specialist in upper extremity surgery who were aware of the classification systems. Images were chosen to be representative of a wide range of fracture patterns according to the Colton, Mayo, Schatzker, and AO classification systems (see later section on classification). Any signs that could lead to patient identification were removed. The severity of open fractures and the final outcome of each patient were not disclosed to evaluators. Radiographic images with incorrect olecranon position that could cause any misunderstanding in image classification were excluded. Images with low quality or with artifacts or other technical defects during image acquisition were also excluded. Although dynamic imaging can be used in some classifications, in our study we restricted the evaluation to static radiographs. This system is based on degree of displacement and fracture pattern, ultimately aiming to provide treatment decision support. This system classifies fractures on the basis of 3 factors: stability, displacement, and comminution. Type I: Undisplaced fractures. In an undisplaced fracture, it matters little whether a single fragment or several fragments are present; thus, noncomminuted (type IA) and comminuted (type IB) fractures may be considered to be essentially the same lesion. Type II: Displaced, stable fractures. In this pattern, the fracture fragments are displaced more than 3 mm, but the collateral ligaments are intact and the forearm is stable in relation to the humerus. The fracture may be either noncomminuted (type IIA) or comminuted (type IIB). Type III: Displaced, unstable fractures. The type III fracture is one in which the fracture fragments are displaced and the forearm is unstable in relation to the humerus. This injury is really a fracture-dislocation. It also may be either noncomminuted (type IIIA) or comminuted (type IIIB). This system focuses specifically on fracture morphology and the biomechanical considerations related to each type of internal fixation. This system classifies olecranon fractures as a subdivision of elbow fractures. This first procedure was the pretest training, aimed at calibrating participants' judgment so that all could start at a similar level. A collection of 4 images was presented to participants through multimedia projections. Images were projected one at a time. Each image remained on the screen until the last participant had classified it. Each participant was asked to independently classify each image according to the classification systems described before. During the whole process, participants were able to consult all printed classification schemes. After the last participant had finished, all of them discussed their answers for each classification system. The discussion was moderated to establish consensus and was mediated through a detailed coverage of each classification. The orthopedic resident responsible for this study mediated the discussion. Subsequently, a new set of 4 images was presented, and all steps were repeated. Image classification was conducted after all training was completed and consisted of 10 new images (i.e., different from those presented in pretest training). They were also projected through a multimedia projector, one at a time. Once the last participant classified each image according to all classification systems, another image was projected. As in the pretest training, participants had access to the written classification schemes at the moment of image classification. Participants were not allowed to review other participants' ratings or to review their own ratings after they had completed the image classification. After 30 days from the initial rating session, without pretest training, the same 10 images were presented in a different sequence, and the test was conducted once again following the exact same procedures. Analysis of categorical items was performed with the Fleiss κ coefficient method to calculate the agreement. The following scales were used to evaluate scores: 0.81 to 1.00, almost perfect; 0.61 to 0.80, substantial; 0.41 to 0.60, moderate; 0.21 to 0.40, fair; 0.00 to 0.20, slight; and <0.00, poor. This paper followed the framework for reproducible research reports. Overall, the Colton classification was associated with a substantial interobserver agreement (κ = 0.67), the Schatzker and AO classifications presented a fair interobserver concordance (κ = 0.33 and κ = 0.21, respectively), and the Mayo classification found a slight interobserver agreement (κ = 0.19) ( Intraobserver agreement for the Colton classification was associated with a substantial agreement for both specialists and nonspecialists (κ = 0.60 and κ = 0.67, respectively).The Schatzker classification was associated with a fair intraobserver agreement for both specialists and nonspecialists (κ = 40). The Mayo classification was associated with a fair intraobserver agreement for specialists (κ = 0.18), whereas nonspecialists were associated with a moderate intraobserver concordance (κ = 0.51). Regarding the AO classification, specialists demonstrated a moderate intraobserver agreement (κ = 0.45), whereas nonspecialists presented a slight intraobserver agreement ( To our knowledge, this is the first study assessing the intraobserver and interobserver reliability of classification systems for olecranon fractures. Although numerous classification systems have been described for olecranon fractures, none has been universally accepted. Although there are no previously published studies on intraobserver or interobserver agreement in classifications of olecranon fractures, our findings are consistent with those studies that evaluated the agreement on other bone fractures. Specifically, our findings corroborate studies evaluating the classification of distal radius fractures by AO and Mayo systems, in which a fair concordance was obtained for these classifications. The Mayo classification system showed low reliability, which could be explained by the lack of familiarity of the surgeons in our institution with the Schatzker and Mayo systems. Nevertheless, the Mayo classification is easy to memorize, is easily reproducible, and infers prognosis by indirectly describing the situation of the elbow's ligament. On the other hand, the Schatzker classification is more difficult to memorize and purely descriptive, based on the pattern of fracture. As it considers the type of internal fixation required according to the fracture lines, it has the possibility of inferring prognosis. The Colton classification is simple and descriptive, and the prognosis is indicated according to injury pattern. Although our study is unique, image standardization limits the generalizability of our results. Our effort to exclude radiographs with poor contrast or positioning does not reflect the way in which films are used in actual practice and can have artificially improved the overall observer agreement. Among the classification systems compared in this study, Colton was the one with the best agreement. The AO classification presented better agreement among specialists, probably because of its complexity. Therefore, no classification system is universally accepted, and every classification system is subject to interobserver variability, ultimately raising questions about the suitability of its use in a research as well as in a clinical context. Learning scales that are more reliable and reproducible can ensure more predictable treatment results. Evaluating the reproducibility of the classifications can help surgeons choose one that fits better to their needs and help them to follow a standardized model to diagnosis and treatment. These can indeed help in the selection of treatment, improving outcomes, and can also establish better prognosis. The development of a more thorough method for evaluating all components of a classification system is therefore one of the suggested next steps in the field. The evaluation of changes in planned treatment secondary to reclassification is also a suggestion to future research. The authors, their immediate families, and any research foundation with which they are affiliated have not received any financial payments or other benefits from any commercial entity related to the subject of this article.