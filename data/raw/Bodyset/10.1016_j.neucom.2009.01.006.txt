Nonlinear dimensionality reduction is a challenging problem encountered in a variety of high dimensional data analysis, including machine learning, pattern recognition, scientific visualization, and neural computation. Based on the different geometric intuitions of manifolds, maximum variance unfolding (MVU) and Laplacian eigenmaps are designed for detecting the different aspects of dataset. In this paper, combining the ideas of MVU and Laplacian eigenmaps, we propose a new nonlinear dimensionality reduction method called distinguishing variance embedding (DVE). DVE unfolds the dataset by maximizing the global variance subject to the proximity relation preservation constraint originated in Laplacian eigenmaps. We illustrate the algorithm on easily visualized examples of curves and surfaces, as well as on the actual images of rotating objects, faces, and handwritten digits.Dimensionality reduction Recently, several manifold learning algorithms have been proposed for dealing with high dimensional data that have been sampled from a low dimensional manifold, such as Isomap Laplacian eigenmaps is based on the graph Laplacian that can be viewed as a discrete approximation to the Laplace Beltrami operator on continuous manifolds. Laplacian eigenmaps is a local algorithm for nonlinear dimensionality reduction. The algorithm imposes a natural cluster of the data. However, not all the datasets necessarily have meaningful clusters. The global algorithms such as PCA and Isomap might be more appropriate in that case. MVU is based fundamentally on the notion of isometry, a smooth invertible mapping that looks locally like a rotation plus translation. MVU attempts to “unfold” a dataset by pulling the input patterns as far apart as possible subject to the constraints that distances and angles between neighboring points are strictly preserved, i.e. local isometry. MVU is a global algorithm for nonlinear dimensionality reduction, in which all the data pairs, nearby and far, are considered. The final optimization of MVU is reformulated as an instance of semidefinite programming (SDP). Large-scale application is a particular challenge to MVU due to the expense of solving SDP. Combining the ideas of MVU and Laplacian eigenmaps, in this paper, we present a new algorithm for nonlinear dimensionality reduction, called distinguishing variance embedding (DVE). DVE aims to seek the low dimensional representations which maximize the global variance and simultaneously preserve the local neighborhood relations. The main optimization of DVE involves an eigenvalue problem. Compared with MVU, the computational complexity is dramatically reduced. DVE can be viewed as a variant of MVU that relaxes the strict distance-preserving constraints. These relaxations can lead to improved solutions for sparsely sampled or noisy data. The organization of this paper is as follows. In Assume that high dimensional dataset has been sampled from a low dimensional manifold Both Laplacian eigenmaps and MVU can be viewed as the graph-based methods, so they share some common properties: they both construct a weighted neighborhood graph Laplacian eigenmaps is a locality structure preserving algorithm which is based on a simple geometric intuition: nearby inputs in the high dimensional space should be mapped to nearby in the reduced space. It first constructs a neighborhood graph Let In the cost function MVU is the nonlinear counterpart of PCA The optimization Here we first reformulate Eq. The effective dimensionality reduction method can be expected that nearby points in the original space are mapped nearby in the reduced space and that far points are mapped far. Combining the ideas of Laplacian eigenmaps and MVU, DVE is designed for the purpose, which attempts to learn the low dimensional embeddings by optimizing Eqs. Notice that The algorithmic procedure of DVE is formally stated as follows:    The neighborhood graph The edge  Similar to MVU, DVE explicitly attempts to unfold data manifolds by maximizing the Euclidean distances between the low dimensional embeddings. DVE differs from MVU in the manner of neighborhood relationship preservation. Based on the notion of local isometry, in MVU the distances and angles between nearest neighbors are strictly preserved. While in DVE, inspired by the approximate locality preserving property of Laplacian eigenmaps, the sum constraint is employed, which can be viewed as a relaxation of strict local distance-preserving constraints in MVU algorithm. Compared with MVU, the evident advantage of DVE is its computational demanding. MVU needs to solve a SDP over To compare the computing time of DVE with MVU, we conducted the experiment on a synthetic dataset. In Each algorithm has its advantages and disadvantages. In MVU, the low dimensional embeddings are obtained from the spectral decomposition of the Gram matrix that the eigenvalue spectra provide a reliable estimate of the dataset's intrinsic dimensionality. Dissimilar to MVU, DVE solves a generalized eigen-decomposition that the eigenvalue spectra cannot reveal the intrinsic dimensionality of the dataset. We evaluated the DVE algorithm on several high dimensional datasets whose inputs were either explicitly sampled or believed to have been sampled from low dimensional manifolds. All the experiments were performed in Matlab on an AMD Athlon 2.0 Firstly, an easily visualized sample is shown in  We first compare our algorithm to MVU on the images of a three-dimensional solid object from the COLI-20 database The next experiment uses the face images dataset that has been used in   In this paper, combining the ideas of MVU and Laplacian eigenmaps, we propose a new nonlinear dimensionality reduction algorithm called distinguishing variance embedding (DVE). DVE is a global method which unfolds the data manifold by maximizing the global variance. In DVE, to preserve the proximity relations of the dataset, the sum constraint originated in Laplacian eigenmaps is employed to substitute for the strict local distance-preserving constraints in MVU. DVE can be viewed as a relaxed optimization of MVU. Experiments on artificial and real-world dataset demonstrate that DVE can find the manifold structure embedded in the high dimensional ambient space. There are two issues that need to be sorted out. First, the relaxation of distance-preserving constraints brings the undesired cluster on the boundary of the low dimensional representation. Second, appropriate choices for Compared with MVU, although the computational complexity of DVE has been largely reduced, it scales as Similar to MVU and Laplacian eigenmaps, DVE is also an unsupervised learning algorithm that does not take the label information into account. However, DVE exploits the sample variance in distinguishing manners: maximizing the global variance and simultaneously minimizing the local variance, which makes incorporating the sample labels to become natural. Hence, another direction for future work is the supervised and semi-supervised extension of DVE.