This is an update of K.H. Esbensen, L.P. Julius, 4.01 - Representative Sampling, Data Quality, Validation – A Necessary Trinity in Chemometrics, in Comprehensive Chemometrics edited by Steven D. Brown, Romá Tauler, Beata Walczak, Elsevier, 2009, https://doi.org/10.1016/B978-044452701-1.00088-0.General symbol for “analyte” (component of interest) Concentration of fragment/increment (grade) Average lot concentration (grade) Mass of unit Grade of sampling volume Sampling constant (material dependent) Top particle diameter, also known as Average group mass Group with index Individual heterogeneity contributions Heterogeneity of unit Heterogeneity of group Lag parameter Mean squared Average fragment mass Mass of fragment Lot mass Average unit mass Number of fragments in lot Number of group in lot Number of units Representativeness Variance Variogram function Distance between unit pairs Minimum distance between unit pairs This is an update of K.H. Esbensen, L.P. Julius, 4.01 - Representative Sampling, Data Quality, Validation – A Necessary Trinity in Chemometrics, in Comprehensive Chemometrics edited by Steven D. Brown, Romá Tauler, Beata Walczak, Elsevier, 2009, https://doi.org/10.1016/B978-044452701-1.00088-0.  Chemometric data analysis and modeling requires appropriate data quality (accuracy and precision) with respect to what data are supposed to represent, irrespective of whether acquired in the field, in the laboratory, or in an industrial (process) setting. The following trinity must be recognized and respected in order for chemometric data analysis and modeling to succeed. These demands follow a logical order as follows: Data must be based on representative sampling, ensuring accuracy with respect to the lot (material) to be represented and precision with respect to the totality of all sampling errors incurred. All sampling errors must be minimized before submitting the analytical aliquot to analysis. Aliquots must subsequently also have an appropriate quality (analytical accuracy and precision); this is usually the least of a problem as analytical laboratories strive hard to be competent, certified or accredited. Chemometric data models must finally respect a demand for reliable performance validation, for example, regarding prediction, classification, time forecasting. Without initial representative sampling, the entire chain of evidence versus the lot characteristics is compromised because of ever-present heterogeneity of lot materials (heterogeneity at all scales). It is not possible to correct for sampling deficiencies in the subsequent data analytical modeling. As analytical errors are typically one or two orders of magnitude All naturally occurring materials, commodities, technologically manufactured materials a.o. are Any sampling process (following the principles of TOS, or not) Thus there are always two main contributors to the total sampling error (TSE): the heterogeneity of the lot material and the sampling process itself. For stationary lots, this generates six principal types of sampling errors, to which must be added two errors specific for the dynamic case, that is, process sampling. Caveat: There exist two classes of materials that are exempt from the stringent principles and rules regarding naturally occurring materials delineated in this chapter, namely “fine” or “pure” chemicals, reagents, powders, and liquid solutions. Fine powders can be of so-called uniform composition and sampling variability less than any predetermined threshold, say, 1% (rel.) or less, either because they have been manufactured for this specific purpose (production of reagent grade fine/pure chemicals inevitably includes the most thorough mixing conceivable) or because of exceedingly thorough comminution and mixing of technologically or naturally occurring materials. Liquid solutions are very much easier to mix as they practically only show compositional heterogeneity at the molecular level. Thus, analytical chemistry often considers sampling of “homogeneous” liquid solutions, prepared by mixing chemicals in the laboratory, as a non-issue. But although both of the above classes of materials may give rise to some comfort within the analytical laboratory, they are in reality “Uniform materials” are the only case in which the devastating sampling bias problems dealt with below are practically absent. Sampling of such materials does not give rise to specific problems and can be accomplished on a basis of conventional statistics, in which the precision follows an inverse square root of the number of samples, etc. There exist scores of statistical textbooks and numerous textbooks on analytical chemistry, which deal comprehensively with this ideal situation; references can be found in Chapters 1.01–1.08. However, it is imperative to recognize that this constitutes but a very minor part of the gamut of materials arriving in the analytical laboratory, which are all characterized by significant heterogeneity … for which this chapter applies in full. Heterogeneity can be divided into two fundamental conceptual aspects: constitutional heterogeneity (CH) and distributional heterogeneity (DH). CH describes the heterogeneity dependent on the physical or chemical The distributional heterogeneity, DH DH is always smaller than CH, and CH can never be strictly zero. Homogeneity is defined as the (theoretical) limiting case of zero heterogeneity. Indeed if such a thing as a homogeneous material did exist, sampling would not be needed—as all sampling errors would be zero, all “samples” would be identical. Alas, all materials display an effective heterogeneity that should never be neglected, hence the need for a modicum of TOS competence. The heterogeneity between individual fragments is of interest here. TOS’s most fundamental theoretical achievement comes in the form of the heterogeneity contribution to the total heterogeneity of a stationary lot defined by focusing on an individual fragment: TOS characterizes all fragments according to the component of interest (the analyte, A), described by the component proportion (or grade), This definition of heterogeneity contribution is dimensionless and hence can be used for any intensive unit used in characterizing the material, for example, concentration, %, size. Delineating the compositional deviations of each fragment in this manner, the heterogeneity contribution also compensates for variation in the fragment masses; larger fragments result in a larger influence on the total heterogeneity than smaller ones. This viewpoint constitutes a major distinction from “classical statistics” where all units contribute equally (i.e., with equal statistical mass). The total CH of the lot, CH For the above derivations as well as the one given below, whenever increment/sample masses are not identical, estimation of the total lot mass, Ascending one hierarchical scale level—from the scale of fragments, grains, etc. to the operative level of one sampling unit (sampling scoop), the scale of the sampling increment—one moves into the realm of DH of the full lot, DH Other than this operative scale difference, the focus is identical, namely quantitative description of the differences in the composition (concentration) of the analyte, A, between these sampling volumes (index The DH for the entire lot can similarly be calculated as the variance of all group heterogeneity contributions: By observing that the sum of all (virtual) groups in practice constitutes the physical lot in its entirety, it can be appreciated that DH This alternative scale-dependent understanding at the two levels—fragments versus group of fragments (sampling unit)—constitutes a most elegant and effective theoretical concept in TOS which was the defining issue that opened up for the world’s first fully comprehensive, necessary and sufficient description of heterogeneity of all types of materials, at all scales, which in turn allowed the founder of TOS Pierre Gy to develop a practical theory for sampling under all circumstances (both stationary and dynamic lots). DH It is now possible to ascertain the quantitative effect of the lot heterogeneity Increments may be used either for making up a Unlike CH TOS has much to say (all negative) regarding the universal futility of grab samples, which are never representative in practice for all realistic heterogeneous lots and materials. Grab samples are thus never reliable and accordingly should never be used. Full details can be found in all of the available TOS literature. Full analysis of the phenomenon of heterogeneity, for example, CH Grouping (depends on the size (volume/mass) of the extracted increments). Segregation (depends on the spatial structural distribution of fragments in the lot; while segregation is very often due to a gravity-induced influence, there are occasional exceptions which may give rise to non-horizontal segregation a.o.). Both grouping and segregation effects can be quantified; methods and equations are described in detail in the pertinent literature, ibid, and further references herein. In order to extract samples from heterogeneous materials with sufficiently low inter-sample variation, it is necessary to minimize the effective DH Decreasing the size of the individual increments, thereby increasing their total number in use over the entire lot (or increasing the sampling frequency in process sampling); the increased number of increments are then combined to form a given composite sample mass, Forceful mixing/“homogenizing” the lot (reduces macroscale lot segregation). If these measures are insufficient for a given sampling process and total error manifestation, it is then necessary to reduce CH itself, which necessitates physical reduction of the fragment sizes, comminution (grinding or crushing), and/or increasing the total sample mass, It is important to note that the quantitative sampling effect of DH All analytical results are associated with some non-zero uncertainty, often taken to be the analytical uncertainty sensu stricto, that is, the variance of the total analytical error (TAE). Following the analysis of the primary sampling process delineated above, TOS in addition collects all other sources of error from sub-sampling a.o. in the laboratory and sums up all of these in the TSE; TAE and TSE together form the global estimation error (GEE) (see Very often TAE is under competent and strict control in the analytical laboratory, and is usually of no significant concern in comparison to sampling, as TAE is always significantly smaller than the sum of errors stemming from sampling (TSE). In fact, TSE can be 10–50–100 times TSE has many sources. Indeed the primary objective of representative sampling is to identify, eliminate (where possible), or reduce all contributing sampling errors. The compound sampling variance, as delineated above, is particularly influenced by the heterogeneity of the material as well as the sampling procedure; as will be clear below there are no less of eight possible sampling errors that can be incurred by non-competent sampling itself. Although much of the effort to reduce sampling errors wherever possible is to a large extent under the control of the sampler, the part from CH is dependent on the material properties only. This error is termed the fundamental sampling error (FSE), as it cannot be altered for any given system (lot, geometry, material, state, size distribution). In fact altering FSE necessitates active physical intervention, in the form of crushing or comminution. On the contrary, the contribution from the spatial distribution of the material is not fixed and The crucial rule to respect in regard to selecting smaller increments is that all possible extractions from the lot (all possible virtual increments) must have the same probability of being selected. This is called the Fundamental Sampling Principle (FSP)—and FSP must never be compromised, otherwise all possibilities of documenting accuracy of the sampling process are abandoned. FSP implies physical access to all geometrical units of the lot. TOS contains many practical guidelines of how to achieve compliance with FSP. TOS employs a strict terminology, in which all aspects of non-compliant sampling can be specifically identified and named. Thus, TOS specifies as “correct” only those features that will contribute toward the ultimate goal of being able to demonstrate unbiasedness of the particular sampling process employed, ibid. The sum of FSE and GSE is termed the “Correct Sampling Errors” (CSEs), as they are not due to erroneous sampling or wrong procedures; in fact, CSEs occur even when the sampling procedure is “correct”, hence perhaps at first sight somewhat peculiar naming. Errors that are connected to erroneous sampling procedures are contrarily summed up as the “Incorrect Sampling Errors” (ISEs). ISEs comprise three parts: one stemming from not delineating correct increments from the lot, the second from not extracting physically exactly what was intended (what was geometrically delineated), and a third form of error which can be induced after the extraction of the increment (or sample). The increment delineation error (IDE) can be avoided by always selecting (delineating) an increment that completely A third incorrect error arises when the sample is altered after extraction, for instance by absorbing moisture, by spillage, cross-contamination, or some similar phenomenon. Sample tampering and downright fraud is also a type of “error”, which is likewise collected under the term incorrect preparation error (IPE). Finally there are occasionally also a fourth type of ISE, the Incorrect Weighing Error (IWE), which will give rise to a sampling variance-inflating effects if proper care is not taken to secure equal-weight increments, or to correct appropriately by using weighted-means when estimating a All these errors The above analysis of heterogeneity, types of sampling error, and criteria for practical representative sampling holds for all lots/types of materials that are stationary, that is, non-moving. Although the physical and geometrical configuration of lots may vary, 3-D, 2-D, and 1-D, sampling from all non-moving lots is governed by the above features. TOS recognizes this invariant issue by defining a generic, stationary type of lot, termed a 0-D lot ( As mentioned earlier two additional errors are found when dealing with the dynamic case, that is, in process sampling. These are rather easily dealt with after the basic framework of TOS has been internalized, and for that reason are left for later. Very often focus is on selecting the final sample mass all too early in the sampling process. Contrary to this (erroneous) objective, it is necessary, and far more beneficial, to focus only on “how to” make the primary sample be representative (unbiased and precise)—even if this means procuring an apparently “oversized” sample, for example in order to overcome problems with lot heterogeneity that cannot be dealt with satisfactorily in situ. This approach of course necessitates that sample masses of any size can be significantly reduced prior to analysis, also in a representative manner. This section delineates all necessary principles and equipment types for representative mass reduction after primary sampling from the original lot. Any and all mass reduction steps (whether singular or in series) will generate sampling variation, no matter how well it is performed. Hence, it is also necessary to be able to minimize these inevitable contributions to the variation of the final result. In other words, “sampling in the laboratory” is principally just as critical as primary sampling and although the quantitative effects are less influential, say 10–20% of TSE, so this stage cannot definitely not be neglected. Any mass reduction in the laboratory must therefore also be in compliance with FSP (now a suitably scaled-down stipulation commensurate with the relevant volume/masses involved); that is, all parts of the primary sample, or a sub-sample hereof, must have an equal probability of ending up in the next sub-sample, or in the final aliquot for analysis. This means that a correct device or method must ensure complete randomness at the lowest possible scale, ideally at the fragment level, which is only very rarely obtained in practice however, and only with very small lots comprised by relatively large fragments. An extensive benchmark study has been carried out in order to identify the optimal mass reduction principle(s).  The 17 mass reduction techniques and methods investigated covered: Grab sampling (a single increment used as Fractional and alternate shoveling (dividing sequentially into two (alternate) or more piles, scoop by scoop). Riffle splitters, with varying number of chutes, chute width, feeding mechanism, dust minimizing systems. A widely used circular riffle splitter for seeds and grains (Boerner divider) (no mechanical parts). Mechanical “rotational dividers” in which samples are distributed over radial chutes by a rotating feeding nozzle. An elaborate manual technique recommended in international seed and grain communities (the “spoon method”), which involves a rudimentary form of bed blending, followed by selecting a few spoons in a composite sampling context. In Petersen et al., The conclusions and recommendations regarding mass reduction are clear: It is evident that any method involving manual shoveling, grabbing, or similar simplistic selection of some material is by far the worse, indeed unacceptable: grab sampling, fractional shoveling, alternate shoveling, or the “spoon method”. This group also includes the popular “coning-and-quartering” method, which likewise fails for the same reasons, see special documentation: The simplest riffle splitting method forms a transition from these unacceptable methods to a suite of 12 acceptable methods. Optimal under all conditions are only riffle splitters or methods based on the same underlying principle (Boerner and rotating devices), namely that all material is distributed uniformly over an equal number of equally sized chutes facilitating a representative split into well-specified portions of the original material (lot). Standard riffle splitters end up with two identical 50% portions, necessitating an iterated procedure to achieve larger reduction ratios (all multiples of ½). This is easily evaded using the slightly more technical rotating “Vario” divider principle, where the total sample mass can be reduced to a lower proportion sample and a larger proportion (“waste” or bypass), as all chutes are used many times, yielding a large total number of effective chutes; this approach is a kind of composite sub-sampler in practice. The Vario device principle is comparable to using a static, but significantly cheaper, riffle splitter in sequence, but many more times over. Furthermore, it is crucial that the equipment is operated correctly. There are a number of practical rules to be respected when using any of the acceptable devices—neglecting one or more of these can end up in the entire mass reduction being biased, or result in unnecessary large variation. The complete benchmark report A 1-D object is a lot in which two transverse dimensions are almost negligible in size, compared to the third, and where a distinct spatial or temporal correlation exists along the singular elongated dimension. A 1-D lot for example can appear as an ordered series of units from a production line (succession in time or space) or as a flowing stream of material. For 1-D sampling the imperative operative demand is that all increments must be in the form of complete, planar-parallel cross-stream In order to sample a 1-D lot correctly, one of only three possible A One-dimensional sampling requires understanding and acknowledgement of the non-random heterogeneity fluctuations The heterogeneity contribution, The A short-range fluctuation component, describing the heterogeneity A long-range fluctuation contribution that describes the longer term A cyclic term that describe any To characterize the heterogeneity of a 1-D lot, the chronological order of the units must be included in the analysis; enter the variogram. A variographic analysis requires a complete set of analytical results based on representative increments, extracted equidistantly over a relevant, sufficiently long interval so as to cover, to represent the relevant process variations well. Clearly, some first-hand knowledge of the process to be characterized/analyzed/sampled is advantageous, but ab initio variogram analysis is always also possible; both options need to be Usually, 50–60 increments are considered minimum for a thorough variographic analysis; in general 100 is recommended. A dimensionless, relative lag parameter, If N The variogram function is calculated iteratively corresponding to a series of lags running in the interval [1,2,3, …, N Interpretation of the resulting variogram is the most important step in a variographic analysis. The variogram level and form provide intricate information on the process variation analyzed. Normally, only three primary types of variograms are encountered: An increasing variogram (normal variogram shape). A flat variogram (no autocorrelation along the defining lag dimension). A periodic variogram. It is also possible to find a periodic The above variograms are outlined in When the variogram type has been identified, information on optimized 1-D sampling can be derived. The increasing variogram ( Variograms are not defined for lag One of TOS’ major understandings is that MPE represents a sum that includes all potential error components, sampling + analysis, that is, both CSEs (FSE, GSE), and When the increasing variogram becomes more or less flat, the “sill” of the variogram has been reached. The sill in practice represents the observable maximum process variability sampling variation as expressed with the current sampling procedure (if the existing autocorrelation is not taken into account). The “range” of the variogram is found as the lag beyond which there is no autocorrelation. These primary characterizing variogram parameters are illustrated in If a significant periodicity is observed, the sampling frequency must never be similar! Also the specific sampling mode (random sampling (ra), systematic sampling (sy), and stratified random sampling (st)) becomes critically important. Full details can be found in Gy, Avoid extracting increments with a frequency coinciding with a process period. Doing so will grossly underestimate the true variation of the process. In general, the stratified random sampling mode (st) will always lead to optimal results (absolute lowest TSE), but systematic sampling (sy) will in many cases be marginally just as effective, and very often much easier to implement. Systematic sampling option is the overwhelmingly most often selected option. Random sampling (ra) is never used in the process realm, ibid. Sampling with a frequency Use A series of recent illustrations of the powerful versatility of variogram process data analysis is compiled in “additional reading” reference section. The data in this example are simulated, but patterned from many real-world examples and therefore fully realistic. A factory produces a powder mixture with three components in a continuous process. The normal sampling scheme consists of extracting one increment (1 A variographic experiment was performed to investigate whether the process is able to yield a product with satisfactory constant composition across 1  The corresponding heterogeneity contributions of the individual increments can easily be calculated, as displayed in From the variogram in From the variogram, a minimum at This example only illustrates how to identify process behavior (trends/periods) from the variogram. A full analysis on how to identify optimal number of increments to form a composite sample, how to zoom in on an optimal sampling interval, and sampling strategy can be found in Petersen and Esbensen Variographic analysis, in the form of It is now possible to summarize all errors defined by TOS, governing both the stationary 0-D and dynamic 1-D sampling scenarios. The concept of 0-D errors identifies and summarizes every sampling variation source associated with sampling of stationary lots: FSE: FSE is related to CH GSE: This error arises because of meso- and macroscale grouping and segregation effects in the lot material. IDE: When the geometrically outlined increment cannot be made to comply completely with the ideal increment requirements, this error arises. It can be completely eliminated based on diligence and TOS competence. IEE: When the material extracted is not 100% coinciding with the geometrically delineated increment, this error results. It can also be completely eliminated. IPE: IPE is the error that sums up all sources of non-stochastic variation IWE: Where appropriate, Minkkinen For 1-D sampling scenarios, a three-part error due to process variation is added: The so-called continuous selection error (CE) is made up of three parts (only two are new in addition to the 0-D case): 1CE: Random fluctuation ( 2CE: Time fluctuation error (TFE). Error contributions due to 3CE: Cyclic fluctuation error (CFE). Error contributions from All the above errors form the maximum TSE: The role of TOS is to educate, guide and provide rules and principles for minimizing TSE. There are many recent introductions to TOS, at several levels from first overview initiations to in-depth textbooks. Below follows a systematic brief commensurate with the present objectives and focused context. The founder Pierre Gy (1924–2015) developed the Theory of Sampling (TOS) over a period of 25 Today, based on more than 60 The eight sampling errors presented above originate from only three sources: the lot material (always heterogeneous, it is only a matter of degree), the sampling equipment (which can be designed either to promote a representative extraction, or not) and the sampling process (even correctly designed equipment can be used in a non-representative manner). Sampling is also defined by whether the lot is stationary or moving when sampling takes place, a distinction that is well-known within many application fields.   The first task on any sampling agenda is to reduce maximally, or eliminate completely, the Incorrect Sampling Errors (ISE), mainly an issue regarding the design, installation, operation and maintenance of the sampling equipment. Subsequently, what remains, are the Correct Sampling Errors (CSE) which can be dealt with by standard means, that is, by increasing the number of composite sampling increments, Q, with respect to the empirical heterogeneity encountered (always honoring the Fundamental Sampling Principle, FSP) and always involving the pertinent GP’s. TOS logically demands that all pre-aliquot steps are supervised and governed by a unified sampling responsibility, which is not tantamount to compartmentalizing. Indeed, whether company president, vice president of science and technology, operations manager, process technician, laboratory supervisor, quality assurance and quality control manager (see e.g., Esbensen and Ramsey 2015 Data quality is a very broad, often only loosely defined, term; a suite of problem- and discipline-related definitions can be found in the literature. Here we shall not try to define data quality in any comprehensive sense, but shall be content to specify that any definition that does not include the specific aspect of sampling and data The term “data” is often equaled with “information”; however, it should be obvious that this can only be in a latent, potential form. Only data analysis + interpretation may reveal “information” and furthermore in a particular problem-specific context only. Such issues have not been without adverse effects in chemometrics, where, in general, issues pertaining to the prehistory of a data table (“data”) usually receive but very little attention. One relevant major exception is the work of Martens and Martens, Inasmuch as the present TOS exposé has specified that “reliable analytical information” derives from “reliable analytical results (data)” only, meaning analytical results exclusively stem from representative samples and adequate sampling plans only, it stands to reason that any definition of data quality per force must include reflections/reference to representative sampling. Thus, it is always crucial to contemplate the specific origin(s) of any data set. From the present perspective, dealing with “modeling of data” without any considerations as to their representativity (strictly speaking, the representativity of the samples analyzed) cannot be considered safe and can easily become unreliable or even irrelevant, depending upon the degree of “incomprehensible variability”. Without following the basic rules delineated by TOS, a continued focus on “data” alone at best constitutes a sin by omission, at worst amounts to a lack of due diligence. In the context of process analytical technologies (PATs) for example, most aspects related to process analytical sampling (0-D and 1-D TOS) are still very much only at a rudimentary level: TOS constitutes a veritable missing link in PAT. The type of errors known as “measurement errors” in chemometrics typically only relate to the X-data (“instrumental signal errors”), and the magnitude of this type of error is inherently only of the same order of magnitude as the analytical errors—making this type of error almost as irrelevant as TAE compared to the physical sampling errors (TSE and its breakdowns) treated here. Section “Data Quality” has a direct bearing on the evergreen issue of proper validation in data analysis/data modeling in general and in chemometrics in particular. Validation is concerned with validating the performance of a specific data analytical model, be it for prediction, classification, time-series forecasting, or similar procedures. In statistics, data analysis, and chemometrics, a much favored method of validation is cross-validation, in which a subset of the training set apparently performs as an “independent test set” in a sequential manner. Depending on the fraction of training set samples (totaling Against this background, Esbensen and Swarbrick The main lesson from TOS’ some 60 Therefore, it is always necessary to It is interesting to note that this highly compressed argumentation is made infinitely easier to express, and comprehend, by invoking the heterogeneity characteristics from TOS. Within process sampling, there are many opinions on the market as to the optimal definition of the training set versus the test set. In a 5-year data series for example, would the first 3