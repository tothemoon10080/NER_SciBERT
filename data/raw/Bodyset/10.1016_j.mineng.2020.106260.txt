This paper examines the hypothesis that the manual selection of rocks for inspection, testing or analysis is invariably biased towards the heavier (larger) particles in the population being sampled. If the property of interest, such as assay or breakage potential, is size-related then such a bias would lead to systematic errors in the estimation of this property. To test the hypothesis, human volunteers were asked to select a sample of 10 rocks from a tray of 100 rocks of known weights, with and without a blindfold, in duplicate. This was repeated for a number of different rock size ranges in the range −50 + 19 mm. A statistical analysis of the results confirms the hypothesis that in almost all cases the samples were of larger weight than that expected from the known weight of the population of rocks. The magnitude of the bias depended on conditions but was highest for the widest size range. It is also shown that the volunteers produced different results to each other. The blindfold reduced the bias in the narrow size ranges but increased it for the wide size range. These effects are likely to be less important for populations of narrow size range, but where a truly unbiased sample is required strategies are proposed using randomisation processes. Relying on unmoderated human selection will lead to samples which overestimate the weight of the population.Mineral engineers and geologists often select rocks by hand, for inspection, testing or analysis. The rocks may be specimens for visual inspection, or they may comprise a sample of a larger population of rocks for breakage analysis using a drop-weight tester, for example. This paper explores the hypothesis that such samples are biased towards the larger (heavier) particles. If the property of interest is size-related, such a bias would lead to systematic errors in the estimation of the property. Size-related trends in coarse particle properties are well known in mineral processing. For example, Such a bias has already been reported. As long ago as 1936, the famous statistician Frank Yates, then working at the Rothamsted Agricultural Research Station, had 12 observers each select 20 ‘stones’ from a population of 1200 flints of varying sizes which were then weighed ( The same effect has been used by teachers of statistics to illustrate the perils of sampling, using not rocks but sweets (‘candy’ in American parlance). A mixture of sweets of varying sizes is placed into a bag and each student asked to put their hand in and withdraw say five sweets and weigh them, replacing them afterwards. The mean weights of the student samples are then compared with the total weight of the sweets in the bag and again the sample weights are invariably found to be biased high ( Because of the importance of manual sampling of rocks in mineral processing, we decided to run a similar experiment to test the prevalence of this bias, in a procedure mimicking the process used to select rocks for breakage testing in a drop-weight tester or similar device. Volunteers were asked to manually sample a population of rocks of known weights. A preliminary experiment was conducted in 2010, and a more comprehensive experiment was conducted in 2018. This paper describes the results of both these experiments. The questions we aimed to answer were: Are the samples of rocks taken by the volunteers biased in weight when compared to the population of rocks? If so, in what sense? Do the volunteers differ from each other in their sample weight? Does it make a difference whether they are blindfolded or not when taking the sample? Do these conclusions depend on the size distribution of the population being sampled? Are there volunteers that are consistently better than the rest? Do any of these conclusions matter in practice? If so, what is the remedy? In neither the 2010 nor 2018 experiments were the volunteers told the purpose of the experiment, to seek to prevent conscious attempts to change their normal selection habits. However in both cases they were asked to select a representative sample. 101 pieces of quarry rock in the approximate size range −50 + 38 mm were assembled and individually weighed. The rocks were placed on a metal tray in a random arrangement and 16 volunteers were asked to ‘randomly’ select 10 rocks from those on the tray. Each sample of ten rocks was weighed and returned to the tray and the rocks then re-arranged to form (hopefully) a new random arrangement, before the next volunteer selected his/her sample. The expected weight of each sample, assuming no bias, would be 10 times the average weight of the rocks. The entire experiment was repeated, that is, each volunteer sampled the population twice in order to provide an estimate of within-volunteer experimental error. This was similar to the 2010 experiment but 100 pieces of quarry rock in each of three different size ranges were used as populations for sampling by 20 volunteers, comprising fine and coarse narrow size ranges (−22.4 + 19 mm and −45 + 38 mm) and a single wide size range (-45 + 19 mm). The volunteers were asked to randomly select their sample of 10 rocks both visually (seen, as in the 2010 experiment) and blindfolded (unseen). Again, the entire experiment was repeated. This provided 12 data points per volunteer (3 size ranges, 2 visual conditions and 2 repeats).    Had the selection of rocks by the volunteers been truly random, we would have expected the weights of the samples to have been normally distributed around the expected value of 1581 g. A 1-way analysis of variance was conducted to test the null hypothesis that there was no statistically significant difference in the sample weight obtained by the different volunteers (based on the duplicate samples taken by each volunteer). This generated a P-value of 0.26, indicating that the variance due to differences between the volunteers was similar to that of the within-volunteer repeatability, suggesting that the hypothesis can be accepted. This conclusion survived the removal of volunteer 6 which was an outlier. We may therefore conclude that there is no evidence in this experiment that magnitude of the bias was different across the volunteers. The residual standard deviation of the ANOVA, measuring the repeatability of the experiment was 235 g, or 11% of the mean. With only two repeat samples to generate the variances for individual volunteers, there was wide variation in variance between volunteers, which may account for the apparent lack of a statistical difference between them. The 2018 experiment allows us to assess the additional effects of rock size range and whether the volunteer was blindfolded or not (seen/unseen). The mean sample weight was again found on average to be larger than the expected sample weight for all size ranges and both seen and unseen conditions. Paired t-tests showed that the observed difference was statistically significant with >99% confidence for all three size ranges and for both ‘seen’ and ‘unseen’ conditions (P(t) wts. in However the magnitude of the effect was smaller than in the case of the 2010 experiment, and some individual values were close to or even below the expected values ( The experimental design allows us to use a 2-way analysis of variance (ANOVA) to test statistically whether the sample weights varied between volunteers, and simultaneously whether being blindfolded made any difference to the sample weight, based on the basic experimental uncertainty measured by the duplicate samples taken by each volunteer. Taking P = 0.05 as the conventional level of significance (implying a 95% confidence in the reality of a difference) it is clear that the observed differences between both the individual volunteers and between seen and unseen conditions were statistically significant. The interaction was also significant for the – 45 + 38 mm and – 45 + 19 mm size ranges, but not for the fine – 22.4 + 19 mm size. The interaction effect implies that the seen/unseen effect depends on the volunteer, that is, some volunteers will take samples that differ between seen and unseen, and some will not. An example of this effect is apparent in Although there were overall differences in the sample weights for the seen and unseen conditions, the direction of the difference was not consistent. Comparing The error mean square (MS) in Unsurprisingly, perhaps, the largest standard deviation is associated with the widest size range. The coarse material has a larger standard deviation than the fine. Between 17 and 19 out of 20 volunteers selected sample weights greater than the expected sample weight for all experiments, except the – 45 + 19 mm ‘unseen’ experiment in which only 13 out of 20 selected greater sample weights. A chi-squared test was used to test the hypothesis that on average half the volunteers will select a larger sample weight than the expected and half a smaller sample weight, which would imply no selection bias. The test returned a total chi-square value of 31.6 for the six conditions (5 degrees of freedom), with a P-value of 7.1 × 10 An alternative way to visualise the data is to plot it as first sample weights versus repeat sample weights for each volunteer, as in The data can also be plotted as cumulative curves as seen in As the individual rock weights are available (see Appendix) simulated results can be generated using random numbers to select ten rock weights as experimental data, and then combined as sets of 32 or 40 values corresponding to the experimental data. By simulating a large number (say 10,000) of ten rock samples and sorting them, the distribution expected from a random process can be generated, and these are plotted as the black curves in A statistical test of whether the difference of the mean of the experimental curves is consistent with the mean from the simulated (expected) distributions gives very low probabilities for each data set (as expected from examining the graphs), less than 10 Even if the general trend is for volunteers to select biased samples, it is of interest to determine if some volunteers are able to select accurate random (unbiased) samples. RMS w Subscripts 1,2 = repeat samples 1 and 2 taken by volunteer, n E m = 1 for 2010 and 3 for 2018 (the number of size ranges) The larger the value of RMS Zero bias in all volunteers would be represented by a vertical line at an RMS accuracy of zero (all volunteers take samples of exactly the expected weight). The extent to which the line departs from the ideal is a measure of the degree of inaccuracy amongst the volunteers. An approximately straight line plot implies that the accuracies are log-normally distributed. Outliers (very good or very bad volunteers) will be reflected in departures from a straight line at the ends of the plot. Similar lines (which overlap) imply similar degrees of accuracy (e.g. seen vs unseen). The lower the gradient of the line, the more different are the volunteers from each other. In each case these are near to straight lines and there is no indication that the lowest (more accurate) values are outliers from the distribution. In fact these values are a little larger than might be expected. This implies that none of the volunteers was unusually accurate. The 2018 volunteers 3 and 16 produced more accurate results than most (see The highest value in the 2010 line is probably an outlier (large bias) because it departs clearly from a straight line (log-normal distribution). This is volunteer 6 (see The answer to the first question listed at the beginning of this paper is clear. The weights of samples of rocks selected by individuals are biased consistently high, on average. The effect was stronger in the 2010 experiment (average 32% high) than in the 2018 experiment (average 16% high), for reasons that are not clear, but the bias was present in both cases. One might speculate as to why this effect exists. Heavier rocks of similar density are by definition larger in one or more dimensions so the volunteers are perhaps more likely to see (and thus select) the larger rocks. When blindfolded and ‘scanning’ by hand there is a higher probability of encountering a larger rock than a smaller one. However this is more the domain of the behavioural psychologist than the mineral engineer. The magnitude of the bias increases with size range, which is unsurprising if one considers that in the limit the bias for particles of identical size must be zero. There is good evidence from the more comprehensive 2018 experiment that individuals differ in the weight of sample that they take under some circumstances, so the process is also subject to the vagaries of individual bias. This was not true in the 2010 experiment for reasons not understood but may simply be a statistical artefact (more degrees of freedom were available for the comparison in the 2018 data than in the 2010 data), compounded by the large difference in variances between the individual volunteers. No volunteer could be said to be unusually unbiased in terms of RMS accuracy (Eq. There are differences in the sample weights depending on whether the volunteer is blindfolded or not. For the narrow size ranges blindfolding reduces the bias, presumably because the bias effect is based on visual selection which is not possible when blindfolded. This reduction occurs only in the coarser (heavier) part of the range; there is no difference between seen and unseen in the finer sizes. However the difference between seen and unseen is reversed for the wide size range, over the whole range; it is not clear why this should be so but it may be that the wider range of sizes is more prone to a ‘feeling’ bias as volunteers select each rock without visual aid. There is also evidence that the effect of the seen/unseen condition varies with the individual. So blindfolding the sampler is not a universal solution to the problem of bias. The dominant conclusion from this study is that the unmoderated manual selection of rock particles for inspection or analysis is a flawed process, leading to non-random samples that are not representative of the population from which they have been drawn. Whether this matters in practice will depend on the extent to which particle size/weight is correlated with other properties such as metal content or breakage potential (both have been observed, as noted earlier). It follows that these effects can be minimised by using narrow size ranges, as is usually practiced for example in drop-weight breakage testing ( Should a truly representative sample be required it would be necessary to randomise particle selection, for example by numbering each particle and selecting based on a table of random numbers. Alternatively the rocks could be distributed into numbered squares in a physical grid of squares (possibly multiple ten by ten grids), and random numbers then used to select the grid positions and thus the rocks. Also Relating the properties measured to the actual size of the rocks selected rather than the nominal size range from which the rocks came will also reduce the bias in results. These strategies were not tested in the present work, and should therefore be investigated to confirm that they are effective in removing the bias. It would also be interesting to use the coefficient of variation of the sampled lot (standard deviation of mass divided by mean mass) as a measure of the ‘degree of difficulty’ in manual sampling and to determine how it relates to particular sampling conditions, lot particle size ranges and the magnitude of the bias. This measure is related to the width of the size range being sampled, but is tedious to determine in practice. Finally, as noted earlier, some commonly used single-particle ore breakage testing approaches, such as the JKMRC drop-weight test (  The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper. We thank the 28 staff and students of the JKMRC who acted as volunteers in the experiments described in this paper. Their participation was of course central to the work. We also thank the reviewers for useful suggestions which have been incorporated into the paper. Expected sample weight 1580.6 g Expected sample weight 1252.1 g Expected sample weight 140.4 g Expected sample weight 688.8 g