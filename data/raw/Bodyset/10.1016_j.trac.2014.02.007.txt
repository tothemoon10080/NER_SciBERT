We assess current approaches to measurement uncertainty (MU) with respect to the complete ensemble of sources affecting the measurement process, in particular the extent to which sampling errors as set out in the Theory of Sampling (TOS) are appropriately considered in the GUM and EURACHEM/CITAC guides. All pre-analysis sampling steps play an important, often dominant role in the total uncertainty budget, thereby critically affecting the validity of MU estimates, but most of these contributions are not included in the current MU framework. The TOS constitutes the only complete theoretical platform for dealing appropriately with the entire pathway from field sample to test portion. We here propose a way to reconcile the often strongly felt differences between MU and TOS. There is no need to debate terminology, as both TOS and MU can be left with their current usages.The purpose of sampling is to extract an amount of material from a ‘lot’ (also termed the ‘sampling target’), which can be documented to be representative of the lot. It is evident that sampling should be optimized before analysis, as it is always preceding, and no longer has any interaction with the test portion when this is analyzed. However, a non-representative sampling process will always deliver an invalid aliquot for MU characterization. A specific sampling process can either be representative (full definition below), or not; only the first results in representative samples, the latter results in mass-reduced undefined lumps of material without provenance (‘specimens’ in Theory of Sampling (TOS) parlance). Only analytical results pertaining to representative aliquots reduce the measurement uncertainty of the full sampling-and-analysis process to its minimum (based on an analytical process in full control). Sampling process correctness (full definition below) and representativity are therefore core elements of concern to both the sampling process and for minimum measurement uncertainty objectives in analysis. The TOS has been established in the past 60 This study points out the main discrepancies between TOS and MU and presents reasons why there is a strong need for reconciliation and how this can easily be achieved. For readers not well versed in the TOS, a comprehensive introduction can be found  The pathway, from sampling target to MU, and its implicit estimate of the sampling-process-error effects (MU There is no need for worry about possible confusion stemming from the different terminologies in these two approaches; this state of affairs is unavoidable, since it evolved in two distinct scientific communities with very little interaction (so far). By analyzing the existing, crucial differences, we reach the conclusion that a call for structured reconciliation is timely and mutually beneficial, and that there is not much danger of a terminology debacle, as both TOS and MU can be left with their current usages. Following the pathway in In the TOS, lot dimensionality is characterized by specifying the operative number of dimensions to be ‘covered’ during the sampling process, defining one-, two- and three-dimensional (1-D, 2-D and 3-D) lots and the special case of a zero-dimensional (0-D) lot, reflecting the effective number of dimensions involved in the sampling process. (A 0-D lot refers to a lot that can be effectively, mixed, moved and sampled throughout with complete correctness (see below). Usually, these are comparatively small lots, which can easily be manipulated). The concept of lot dimensionality becomes clear, e.g., when considering an elongated material stream, as is the case for material on conveyer belts. According to the MU definition, this sampling target should be termed 1-D, since one dimension of the physical geometrical aspect dominates, while, according to the TOS, it is essential to consider the applied sampling method as interacting with an effective number of dimensions during the sampling process. Employing grab sampling (full definition below) on such an elongated material stream, a widely-applied, but fundamentally-flawed, extraction method, would make this lot effectively 3-D (and not 1-D), since singular grab samples are most likely only taken from the top surface part of the moving material flux, so far from covering both the transverse lot dimensions fully (i.e. width and thickness). By contrast, a cross-stream cutter (a sampling device especially designed for elongated material fluxes) would cover the entire depth and width of the stream, thereby fully reducing the sampling lot to one dimension (i.e. the longitudinal dimension of the material stream). According to the TOS, 1-D lots present the optimal sampling situation, preferring that 2-D and 3-D lots (e.g., industrial, geological or environmental strata, stacks, stockpiles, silos) should, where possible, be transformed to comply with a 1-D sampling situation Lot-dimensionality transformation constitutes one of the governing principles (GPs) of the TOS, described further below ( Below we present a sufficient minimum of the TOS tenets to allow full understanding and appreciation of deficiencies inherent in the current MU approaches. Before defining these concepts theoretically, For well-mixed materials {e.g., The theoretical analysis of the TOS of the phenomenon of heterogeneity leads to the recognition that the total material heterogeneity in a lot must be distinguished as two components [i.e. the constitutional heterogeneity (CH) and the distributional heterogeneity (DH), respectively], which are conceptually and mathematically defined in full only in the TOS. CH describes the heterogeneity depending on the chemical and/or physical differences between individual “constituent units” in the lot (e.g., particles, grains, or kernels), which are generically termed “fragments” in a subtle, ingenious coverage also of the situation in which the sampling procedure accidentally or unavoidably fragments original particles. Note that each fragment (particle) can exhibit any analyte concentration in the range 0–100%. When a lot (L) is sampled, CH DH It is essential to understand, to acknowledge, and to act appropriately upon DH Perhaps paradoxically at first encounter, the TOS focus is not with ‘the sample’ but exclusively with the sampling process that produces the sample. Without specific qualification of the sampling process, it is not possible to determine whether or not a particular sample is representative. Loosely speaking of ‘representative samples’ without fully describing, fully understanding and fully documenting the lot provenance and the sampling process is but an exercise in futility (this includes massive confusions, such as ‘more representative’, or ‘less representative’). Only a sampling process designed according to the rules of sampling correctness can produce representative samples. There is thus no declination of this attribute, a sampling process is representative or it is not representative. The primary requirement in this context is sampling correctness, which means elimination of all bias-generating errors [termed ‘incorrect sampling errors’ (ISEs), see According to the TOS, a sampling process is representative only when it is both accurate and precise The relation between bias-generating errors, ISEs and CSEs, is depicted in The TOS has analyzed the concept of heterogeneity in full, especially its manifestation in the sampling bias – and, by fully acknowledging these objective characteristics of all lots in science, technology and industry, the TOS reaches the conclusion that the ISEs must be eliminated and, for that, it describes all necessary countermeasures (see The term ‘error’ in the TOS denotes a specific source that generates, or contributes to, the total MU. An important duality: while it is qualitatively essential to understand the origin and the circumstances influencing the source of specific sampling errors, it is only their manifestations (i.e. variances, or standard deviations) that can be estimated quantitatively. The ISEs are three-fold: ‘Increment Delimitation Error’ (IDE), ‘Increment Extraction Error’ (IEE) and ‘Increment Preparation Error’ (IPE). IDE relates to variations of the geometrical outline of the physically to-be-extracted increments, which can be avoided by stringently identical delineation of each increment. IEE manifests itself, e.g., when particles that belong to the delineated increment do not end up here. This principle is also referred to as the “center-of-gravity rule”, which states that particles with their center of gravity inside the delineated increment when intersected by the sample cutter edge(s) must end up in the final sample IPE occurs when increments/samples are altered after extraction (which they should never be able to). In order to avoid effects, such as contamination, moisture absorption, evaporation, misidentification, loss of material or even fraud and sabotage, all samples require the utmost care in handling, correct sealing and storage. IPE is one sampling error, which can be completely controlled, but it critically depends upon strict, professional quality assurance/control of all processes, instrumentation and personal competence. The TOS deliberately introduces the ISEs in order to signify that these errors, if not eliminated, always cause a significant sampling bias and are therefore the source of unpredictable, high sampling uncertainty. We do not deal in this article with the ‘Increment Weighting Error’ (IWE) and ‘Point Selection Error’ (PSE, which only affects process sampling) – two further bias-generating errors, which are not included in The TAE is identical to the total MU A recent unified approach for valid estimation of the GEE in the form of a new international standard, termed ‘DS 3077 Representative Sampling – HORIZONTAL standard’ The systematic framework in the TOS of principles and operations for representative sampling enables us to evaluate the representativity of all types of sampling methods and equipment. The TOS is comprehensive and complete, in the way that these 10 SUOs summarize all principles and practical procedures needed to ensure a correct (bias-free) and variance-reduced sampling along the complete lot-to-aliquot pathway, including all sample handling, mass reduction and sample-preparation steps in the analytical laboratory. More theoretical background to each SUO can be found in the TOS literature. First, it can only be rated representative when a given sampling procedure is correct (unbiased), and GSE and FSE have subsequently been minimized in order for the sampling procedure also to be sufficiently precise, as depicted in the TOS pathway in The MU approach is discussed below following the pathway indicated in All sampling targets are very nearly always characterized by significant heterogeneity (i.e. deviating from the ideal homogeneity, which is defined in the EURACHEM guide The process of experimentally obtaining quantity values for a measurand is defined as ‘measurement’, requiring specified procedures and conditions. The MU “includes components arising from systematic effects, such as components associated with corrections […], as well as the definitional uncertainty” EURACHEM points out eight main sources that effect MU, of which the first two refer to sampling and sample preparation Notable ‘gross errors’, such as involuntary mistakes (e.g., lack of knowledge, spillage, contamination, mixing of sample numbers), or deliberate faults are specifically excluded from these uncertainty estimates For estimating the MU caused by sampling, the EURACHEM guide introduces two approaches – i) empirical and ii) modelling. These approaches can also be used in combination, if desired. The empirical method, also termed the ‘top-down’ approach, quantifies uncertainty by determining the effects caused by “factors such as the heterogeneity of the analyte in the sampling target and variations in the application of one or more sampling protocols” using “repeated sampling and analysis, under various conditions” The latter postulate of the analytical bias and an alleged analogous sampling bias constitutes the singular theoretically (and practically) most important difference between TOS and MU, as outlined fully below, which cannot be overemphasized. Random analytical effects are normally estimated by ‘duplicate measurements’, using conventional, effective statistical approaches; here, MU and TOS are in full agreement. Within MU, the analytical bias is to be estimated based on comparisons based on certified reference materials (CRMs, see Explanation 5). However, the situation may, or may not, be different when it is realized that the only relevant definition and configuration of ‘duplicate measurements’ must be realized by full ‘duplicate sampling’ from the primary sampling stage (always including the first sampling stage). This understanding forms the basis for the elaborate ‘replication experiment’ imperatives laid out in the new standard, DS 3077 Representative Sampling – HORIZONTAL standard For estimating the sampling bias, the EURACHEM guide suggests the use of a reference sampling target (RST) as “an equivalent to reference material(s)” to estimate the bias, or alternatively to compile measurement results from “inter-organizational sampling comparison trials” (see Explanations 4 and 6). Depending on which type of method is used (duplicates sampling plus analytical precision (duplicates); sampling precision plus bias in-between different protocols, as well as analytical precision (protocols); sampling precision plus bias in-between different samplers, as well as analytical precision and bias (CTS); and, sampling precision plus bias in-between samplers and protocols, as well as analytical precision and bias (SPT). These experimental plans are all analyzed by ANOVA. Below follows our critique of the notion of RST, which clashes with a full understanding of heterogeneity following the TOS. If the RST concept cannot stand up in its alleged role as a direct analogue (regarding sampling) to CRM (regarding analysis), EURACHEM options iii) and iv) are compromised. The above assessment points out that the more complex EURACHEM experimental scenarios are not based on a fully comprehensive heterogeneity concept in relation to the many manifestations met with in science, technology and industry. They are specifically unable to cover the full range of challenges in sampling all types of heterogeneous materials and lots under the limited specifications offered. The key issue in the present critique concerns the lack of inclusion of the effect of ISEs, without which there can only result unnecessarily inflated MU estimates. Such MU estimates can therefore never be considered valid (or even fit-for-purpose if ISE effects remain in the definition of FFP). We here advocate a much simpler, direct approach: the replication experiment (stationary lots) or variographic characterization (1-D lots), each of which works directly on the lot to be sampled and each of which captures the combined effect from the specific sampling procedure/material heterogeneity interaction without any of the excessive RST complications demonstrated above, see DS 3077 The EURACHEM guide refers, correctly, to variography for estimation of the combined MU from analysis and sampling in the case of process sampling and process monitoring, in full agreement with the TOS (see Explanation 7). Composite sampling is a fundamental issue on which TOS and MU agree substantially. In order to ‘cover’ lot heterogeneity appropriately, the logical approach is clearly by the use of composite sampling [i.e. by deploying an optimized number, Q, of correctly sampled increments covering the entire spatial geometry (volume) of the lot as well as possible within a set of given conditions]; it is manifestly not enough to specify only the number of increments to be used without this spatial coverage imperative. Note, however, that only the TOS enables the sampling operator to establish correctness (un-biasedness), which is absolutely not an automatic attribute of any sampling equipment or procedure by itself (design, operation and maintenance of procedures and equipment must be so that ISE effects are eliminated – not a trivial task, but a necessary task nevertheless). These conditions are often not fully understood. The TOS is the only framework that furnishes ways and means with which to optimize Q in relation to the empirical heterogeneity met with, either via replicate experiments or by variographics {see, e.g., DS-3077 The above evaluation of GUM and the EURACHEM guide shows that MU is not fully comprehensive, and is not a universal, guaranteed approach to estimate an uncompromised total MU from sampling. Sixty years of theoretical development and application of TOS practice has shown that sampling, sample handling and sample-preparation processes are associated with significantly larger uncertainty components than analysis (measurement) itself, multiplying MU While GUM focuses on MU Furthermore, in EURACHEM’s four empirical approaches to MU, the scope of the MU estimate depends on the method applied. Only the sampling proficiency test (SPT) approach considers analytical precision and bias, and sampling precision and bias, albeit in abridged form only. Otherwise, the sampling bias is considered to only a severely limited extent. It is tacitly assumed, but incorrectly so, that a sampling bias can be likened to a systematic effect in the standard statistical understanding. However, the physical nature of the sampling bias is most emphatically not of this simplistic nature – the main feature of the sampling bias is its very violation of constancy. For these reasons, the only scientifically acceptable way to deal with any and all sampling bias is to eliminate it. This, then, is where the major distinction between TOS and MU becomes clear: the TOS notion that a sampling bias is a reflection of the ISE effects interacting with a specific heterogeneity The different realms of MU GUM: MU considers (only) TAE EURACHEM – empirical approach (SPT): MU considers (only) TAE EURACHEM – modelling approach: MU considers (only) TAE GUM focuses overtly on uncertainties related to analytical measurement (i.e. weighing, preparation, dilution, filtration, handling and similar issues for reference materials), disregarding key elements governing all prior sampling and laboratory sub-sampling, among other issues, and their uncertainty contributions. The scope in the EURACHEM guide varies depending on the applied approach. The empirical approach, based on repeated sampling and analysis, includes FSE and GSE, since both of these will always be reflected in repeated sampling and analysis. {A subtle point here is that, whenever “re-sampling” is involved, it must always be replicated from the primary stage (see e.g., On this basis, it is clear that none of the stated MU approaches is able to estimate the full total GEE (GEE Representative TOS approach: GEE Complete MU approach: MU Regarding (5), it falls to the TOS to take responsibility for the estimate of MU We here draw the logical conclusion to the above analysis and assessment. We call for integration of the TOS with the MU approach, easily illustrated based on the widely-used fishbone flow-path diagram. In order to prevent structural underestimation of the full complement of active uncertainty sources, it is necessary to integrate the effects related to all sampling stages involved with this standard MU The TOS stipulates that all ISEs must be eliminated (sampling correctness), followed by reduction of the remaining CSEs (and PSEs, if relevant) (sampling precision), until compliance with representativity and/or until a fit-for-purpose criterion. Note that the Point Integration Error PIE A critical assessment of GUM and the EURACHEM guide shows that not all influential uncertainty sources are considered as to their full MU impacts. In particular, effects caused by ISEs are insufficiently defined and integrated. While GUM exclusively focuses on estimating the analytical MU, the EURACHEM guide indicates and incorporates some error sources related to sampling, but detailed analysis of the scope also here revealed several deficiencies compared to the full sampling-error framework of the TOS. While the EURACHEM guide acknowledges the existence of the CSEs, it stays with the assumption that all other sampling-uncertainty-error sources have been eliminated by other parties – which gives no help to the sampler/analyst. By excluding both the concept of, and the risk incurred by, the inconstant sampling bias, the sampler/analyst may well not even beware of the risk that the effective MU estimate will be principally different each time that it is re-estimated. The user is left without the crucial understanding that ISE effects will unavoidably result in uncontrolled and unquantifiable, inflated MU Only the TOS offers complete theoretical and practical understanding of all key features related to heterogeneity and full practical insight into the intricacies of the sampling process when confronting the gamut of heterogeneity manifestations. Closing this gap between TOS and MU necessitates a certain minimum TOS competence, and confidence, that all sampling processes can indeed become correct (sampling free of bias), opening up for them also to become representative, or fit-for-purpose, where appropriately defined. This minimum competency has recently been outlined in a new international standard, DS 3077 A detailed analysis of MU and formulation of the requirements for a universally optimal MU concept outlined the critical deficiencies in MU and pointed out that the TOS can simply be inducted as an essential first part in the 
complete measurement-process framework, taking charge and responsibility of all sampling issues at all scales (i.e. along the entire lot-to-aliquot process). We here call for a constructive integration between TOS and MU, allowing reconciliation of these two frameworks that all too long have been considered only antagonistically. One could perhaps conceive of a potential terminology debacle in the wake of the present proposal. For one thing, MU denounces with extreme prejudice the notion of “error” and “true value” (as in “sampling error” and “true average lot concentration, a A scientific concept and terminology skirmish is also uninteresting in view of the separate histories and the complementary practical roles of TOS and MU. Neither framework can win such a battle in view of their hitherto individual histories, achievements and their present status. The only constructive way forward lies with the proposed integration and reconciliation. Heterogeneity, introduced above, is the prime characterization of all naturally-occurring materials, including industrial lots, intermediate materials and products, processed and manufactured materials, and all materials in the natural world. Rocks could serve as an example of significantly heterogeneous materials in the natural world (also mineralizations, polluted sediments, toxic wastes, mineral-processing streams, commodity raw materials) – the range of examples from all of science, technology and industry is legion. Moreover, heterogeneity manifests itself It is much more than a quibble, to point out that heterogeneity should be defined as the degree to which a property or a constituent deviates from an assumed uniform distribution throughout a quantity of material, instead of the degree to which it conforms to an unrealistic ideal concept of random distribution. It is counterproductive to keep to the ideal notion of a uniform distribution, because such is never the case for the very many, very different types of materials and lots that are to be sampled. Uncritically taking on the notion of a random distribution, which can then be considered fully with traditional statistical tools, is a very dangerous endeavor. An example of quantitative analysis of genetically-modified organisms (GMOs) provides a poignant case. Esbensen et al. The MU definition of heterogeneity is incomplete in that it specifically only addresses one of the two aspects of heterogeneity, spatial heterogeneity (DH). This is unspecific (e.g., regarding analytes that may reside wholly or partly inside certain types of particles but not in others), and particles may obviously have widely different concentrations of a dispersed particular property. Particles may also be broken up during sampling (or they may not), partly or fully ‘liberating’ the “property of interest”, as a function of the sampling process. The compositional heterogeneity concept is not defined in MU. The situation is somewhat more relaxed concerning the definition of “practically homogeneous materials”, termed “uniform materials”, which are defined as materials with a “repeated sampling reproducibility lower than 1%”. Claims have also been made that “small items” (presumably meaning “small lots”) are also not in obvious need of elaborate sampling instructions. However, such materials and cases only occur naturally in but the rarest of instances (e.g., exceptions are gases, well-mixed solutions, and manufactured pure or ultra-pure materials). However, the most important characteristic from such cases is that generalizations based on them with respect to sampling can never be valid for the gamut of all other types of materials and lots. It is by far the simplest always to treat all types of lots, including such marginal cases, all materials and sampling targets as examples of materials displaying significant heterogeneity The TOS defines the Global Estimation Error (GEE) as the sum of the Total Analytical Error (TAE) plus the Total Sampling Error (TSE). TAE is identical to the total analytical MU, MU The TOS considers the first type of ‘gross error’ as part of the ‘Incorrect Preparation Error (IPE)’. This definition also allows inclusion of the effects of ‘gross errors’ in the overall Global Estimation Error (GEE), if they can be quantified, (see One of the major issues of dissent between the TOS and the MU concerns this twilight status and deliberate neglect of the incorrect sampling error (ISE) (the second type of ‘gross error’ in the MU). In the TOS, this would be unthinkable, if for no other reason than these dominate the total uncertainty budgets if not heeded properly, but also because they are indeed, and manifestly, subject to directed action: the TOS actively reduces, and seeks to completely eliminate, the effects from these critically important errors as part of a reconciled TOS/MU. Systematic error effects caused by sampling heterogeneous lots, termed ‘sampling bias’ in the TOS, are not constant, and therefore not “predictable”. A specified sampling procedure interacting with a given heterogeneous material will, if replicated, never result in an identical bias estimate precisely because of the nature of the material heterogeneity. Lot heterogeneity is a complex spatial and compositional feature characterizing the lot volume at all scales above the sampling-tool size, and it is transient (i.e. varying if/when the lot is manipulated in connection with sampling, or resulting from transportation). Sampling procedures that compromise the ‘correct sampling imperative’ (GP 3 in While this difference at times may be negligible or small (small lots and/or uniform materials), and therefore perhaps ultimately only constitute an acceptable MU contribution, it may equally well deviate to a significant degree, depending on both the nature/magnitude of the lot heterogeneity in question and the sampling procedure used, either way leading to an unacceptable, unnecessarily inflated MU Above all, this principal uncertainty can never be used as justification for deviating from the strict rules of the TOS formulated to guarantee representativeness. Unfortunately, a varying, ‘inconstant sampling bias’ cannot be compensated for by any known means (e.g., data, analytical, statistical, equipment, or procedure), which all presumes a ‘predictable’ (i.e. constant) bias. The TOS allows all samplers the easy and full understanding that unrecognized, or uncontrolled, the ISEs create the inconstant sampling bias, so there is only one conclusion: ISEs must be eliminated from the specific sampling process involved. This solution is both logical and practically achievable. Occasionally, a fit-for-purpose (FFP) version of this imperative may suffice, provided that the TSE The approach to estimating the analytical bias is designed to work in the analytical laboratory, where every systematic effect can, in principle, be brought under control. However, it is only fair to point out that this is critically dependent upon ‘TOS-correct’ sub-sampling from batches of certified reference materials (CRMs), as received from relevant suppliers. But laboratory sub-sampling representativity is very often assumed without proper validation, despite sub-sampling always critically depending upon the “effective heterogeneity” of the CRM sachets supplied (e.g., containers, or vials). Such sachets are lots in their own right, albeit small, the only difference is in scale. As such, the critical issue here is, as always, how sampling is performed, in this case how the relevant sub-sampling is performed This issue is far from trivial, as witnessed by numerous discussions and focused CRM heterogeneity studies (e.g., in journals, such as To the degree that a CRM sachet is heterogeneous at the scale of a few test portion masses, say 5–15 or so, there is a very real danger of sampling errors also affecting even this ultimate sampling step producing the analytical CRM aliquot. This is why many calls have been made to supply CRMs with an effective “sampling constant” specifying a minimum sampling mass (sometimes augmented by a demand for a representative grain-size distribution documentation) { Analysis of Variance (ANOVA) is a statistical approach for variance decomposition proportioned along a set of experimental factors in the experimental design employed. The defining issue is that each factor is controllable by the fashion that the experimenter is able to set the specific levels desired in a design of experiment (DOE), or by a random factor. Different sampling procedures or sampling plans can, with a stretch, be codified as ‘levels’ on a sampling-mode factor, or different between-sample-distances may also be viewed as ‘levels’. However, there would appear to be little or no possibility of ‘different degrees of heterogeneity’ (and its interactions with alternative sampling procedures) to be similarly codified on an experimental factor, at least not without a truly staggering amount of work. More importantly for significantly heterogeneous lots in the real world, it is unrealistic to contemplate that an RST can ever be constructed precisely because of the compositionally complex and varying spatial heterogeneity involved (amongst others, even laying up the RST would, e.g., be subject to inconstant segregation effects). Above all, the RST approach is extraordinarily difficult and prohibitively laborious because one would first have to try to estimate the effective heterogeneity of the target lot reliably (indeed this itself must involve extensive sampling, not yet documented representatively) and then to try to construct a reference-sampling-target lot with identical heterogeneity characteristics, from which to try to obtain insight as to how to best to sample the original target. There appears to be no way such an endeavor can ever come close to simulating the entire target lot, without taking this apart The variogram is a powerful tool with which to characterize 1-D variations and which benefits from the inherent auto-correlation between units (increments, or single samples) sampled with different ‘between pairs-of-samples’ distances, termed ‘lags’. Variography is particularly relevant for process sampling (or, equivalently, stationary 1-D lot sampling), both instances referring to lot configurations for which one elongated dimension in time or space dominates completely, because the other two dimensions are eliminated by the TOS stipulation that all increments (samples) must cover both these dimensions completely – hence the rigid demand in the TOS only to use correct increment delineation and extraction. The extensive approach of the TOS to both stationary and dynamic 1-D sampling addresses, e.g., moving streams of matter on conveyer belts or in pipelines, units transported as truckloads, railroad cars or tank vehicles, and manufactured or produced units, such as containers, vessels, or bags. Depending on their intrinsic heterogeneity characteristics, such streams are characterized by various degrees of 1-D auto-correlation, as manifested by the variogram. There exist numerous, in-depth descriptions, illustrations and very many case histories involving variograms in the TOS literature A comprehensive description of sampling streams of extremely irregularly distributed trace concentrations, including a thorough exposé of the versatility of variographic characterization as a general approach for designing “fit-for-purpose” sampling plans, commensurate with the empirical lot heterogeneity, can be found