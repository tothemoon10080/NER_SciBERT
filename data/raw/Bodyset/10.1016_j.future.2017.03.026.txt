Inducing adaptive predictive models in real-time from high throughput data streams is one of the most challenging areas of Big Data Analytics. The fact that data streams may contain concept drifts (changes of the pattern encoded in the stream over time) and are unbounded, imposes unique challenges in comparison with predictive data mining from batch data. Several real-time predictive data stream algorithms exist, however, most approaches are not naturally parallel and thus limited in their scalability. This paper highlights the Micro-Cluster Nearest Neighbour (MC-NN) data stream classifier. MC-NN is based on statistical summaries of the data stream and a nearest neighbour approach, which makes MC-NN naturally parallel. In its serial version MC-NN is able to handle data streams, the data does not need to reside in memory and is processed incrementally. MC-NN is also able to adapt to concept drifts. This paper provides an empirical study on the serial algorithm’s speed, adaptivity and accuracy. Furthermore, this paper discusses the new parallel implementation of MC-NN, its parallel properties and provides an empirical scalability study.The 4 main aspects of Big Data are The growing importance of data stream classification techniques is reflected through many commercial applications, such as: sensor networks; Internet traffic management and web log analysis Few attempts have been made to combine parallelism and real-time data stream classification. Parallel binning is used by the SPDT This paper proposes an inherently parallel adaptive data stream classifier termed MC-NN. The classifier is based on Nearest Neighbour (NN) classification and statistical summaries of the data and recency. The statistical summary is structured in the form of a set of variance based Micro-Clusters (MCs). Micro-Clusters continuously adapt to concept drifts through absorbing new data instances (updating statistics). An empirical evaluation A parallel implementation of MC-NN is presented, along with a critical appraisal of implementation mechanisms that can be used to support parallel analysis of real-time data. A scalability evaluation is also carried out, identifying insights, difficulties and solutions in implementing parallel real-time data stream classifiers. This paper is organised as follows: Section In the more general area of data mining an algorithm would iterate over the data several times in order to generate a model that fits the concepts (patterns) in the data. In each iteration the model is altered in order to better fit the concepts. However, as data streams are inherently infinite in length, iterative processes cannot be used. If left un-monitored, the algorithms would try to fit the concepts encoded on the whole stream and not account for Other techniques such as Hoeffding bound based techniques A number of systems exist to support parallel stream processing, the most notable of these include Esper In the authors’ previous feasibility study The notation used for Micro-Cluster has been taken from The centroid of the Micro-Cluster can be calculated by  As more labelled instances are received for learning they will change the distribution of the Micro-Clusters. According to Algorithm 1 two scenarios are possible after the nearest Micro-Cluster has been identified when a new training instance is presented to the classifier:   If over time a Micro-Cluster’s error count At any point in the data stream the Triangular Number (Eq. For example, consider the two Micro-Clusters (MC1 and MC2) as depicted in Calculate the Triangular number for Time Stamp 11.          If either of these percentages drops below the performance threshold ( The complexity definitions used in this section are given in During training the distances of new instances are calculated against each of the Micro-Clusters’ centroids, and the nearest Micro-Cluster’ class label is used for classification. This is equivalent to computing the Euclidean distance between 2 vectors, where each vector value corresponds to a data attribute and there are In the best case, where there is only one Micro-Cluster per class label this will be For the evaluation of this serial MC-NN implementation, an ‘Intel core’ I5 processor with 8 Gb RAM was used. All synthetic data generators and algorithms evaluated in Section Four data streams have been utilised: The MC-NN was compared against two state-of-the-art data stream classifiers, Hoeffding Trees Naïve Bayes and Hoffeding Tree classifiers were chosen as they are widely covered in the literature and often considered as the best all round classifiers in data stream mining, providing a benchmark for comparison and evaluation. Although dated, both are still widely used, as they often deliver an exceptional classification performance in many applications. The Multinomial Naïve Bayes version was used recently as an ensemble to win the 2014 data mining Kaggle competition      For example, decision trees will adapt to a constantly changing concept by growing more subtrees from leaf nodes. As the Hyperplane streams are constantly changing, leaf nodes will receive conflicting data and thus the tree grows constantly. This leads to increased complexity and a high computational cost. A tree using the oscillating hyperplane will cause the tree to grow in multiple directions; whereas the rotating hyperplane will cause the tree to grow constantly one sided in the direction of the drift. Contrary to decision trees, MC-NN aims to retain its Micro-Clusters by updating their statistical properties. Thus, continuous concept drifts naturally get absorbed by the model without additional computational cost for growing the model. Data streams with only occasional drifts, such as the ones illustrated in  Overall the experiments in this section showed that MC-NN clearly outperforms its predecessor in terms of classification accuracy and computational efficiency. MC-NN achieves a similar performance compared with well established data stream classifiers in terms of accuracy and runtime. However, it is more robust in terms of adaptation to concept drifts, especially complex continuous concept drifts. Moreover MC-NN is naturally parallel and thus has the advantage to be scaled up to high speed data streams as will be discussed on greater detail in Section This section presents a parallel implementation of the MC-NN classifier highlighted and evaluated in Section Parallel MC-NN is implemented using the In the parallel MC-NN implementation each Mapper in the cluster computes its own MC-NN cluster set. Predictions from individual Mappers are aggregated in Reducer nodes, which assign the final class label based on the majority vote. This paper is primarily concerned with the The parallel MC-NN implementation can be described in three steps, the Micro-Cluster initialisation, the training/adaptation of Micro-Clusters and the prediction of newly arrived unlabelled data instances (testing). Sections MC-NN requires a set of parameters for Micro-Clusters to adapt to data patterns and data stream variations, as discussed on Section Training of individual nodes only requires a single ‘send’ operation and no ‘round trip’ response time for returning predictions. Utilising a computer cluster incurs communication and management overheads. The impact of these overheads can be reduced by batching data instances together in single messages when sending them to the computer cluster. The instances in the batch are then distributed evenly inside the computer cluster across the individual Mappers ( Once a labelled training instance arrives at a Mapper, the Mapper’s Micro-Clusters absorb the new data instance as it is described for serial MC-NN in Section Testing of a data instance requires each Mapper to be sent a copy of the unlabelled instance. A broadcast message is created containing the instance to be tested. This is depicted in The architecture utilises multiple open source technologies to handle real-time data stream processing. The cluster consists of 17 rack mounted physical servers. Each node consists of a quad core machine running CentOS 5. The architecture is centred on the use of Hadoop, which was initially designed to work with files and batch processing on large parallel processing tasks (e.g. distributed file searching and word count problems), the later versions of Hadoop (2.0 — YARN) can accept data from different sources such as MPI and Kafka  The message system used in the experiment was  The complexity definitions used in this section are given in MC-NN Memory Size is The setup of the proposed MC-NN algorithm over the cluster was implemented in a traditional MapReduce setup, which is depicted in Each of the Mappers (Hadoop nodes in the cluster) was initialised with a Samza container connecting them to a specific partition (distributed sub-section) of a Kafka topic stream. Each node when idle, waits for a message to arrive. Upon arrival of a message the node performs the required action (training, predicting, etc.) of the message and data, then forwards the results to another Kafka Output stream; as depicted in The SEA data stream A 10 million data instance stream was created in parallel across the Stream Generators. To highlight the parallel performance of the system ‘batching’ was utilised to reduce the latency communication costs. Training messages created were batched into individual 10 thousand blocks for the Kafka messaging streams. The HAR data is looped instance by instance from each of the stream generators, to mimic the presence of a much larger (infinite) data stream for performance evaluation. For each parallel Mapper configuration an equally parallel Kafka distributed Topic was created for the Samza containers to join to on a 1:1 basis. In addition to each Samza container joining to a specific Topic partition, a shared broadcast Topic was used for distributing the test instances to all Mappers at the same time as depicted in  It can be seen in In In Overall, the pattern we can see is that the more MC-NN nodes that are used, the faster the processing of the 10 million data instances from the streams. What can also be seen is that a larger number of Stream Generators is also beneficial, which can be explained by a more distributed data communication load between the MC-NN nodes. The speedup of the algorithm has been calculated by the following formula:   For better readability the speedup shown in The current implementation has been Re-read the entire data stream (limited to Kafka logs). Start a completely new job from this point and accept that the previous data was lost. Re-reading the entire data stream will depend on the pre-set Kafka configurations and how long the data stream has been running for (perhaps, weeks or months). Kafka only retains messages for a pre-set period of time or until messages are overwritten on a ‘rolling log’ format of fixed hard disk size, whichever happens sooner. Starting a new job from the point of failure makes the classification analysis with other configurations incomparable. Only a subset of the data stream is processed by the new job. Accuracies and processing times are no longer comparable. For the purpose of this paper the Kafka nodes only kept messages for a maximum of 1 h. This allowed any old data to be naturally flushed out of the system in a reasonable time, for other experiments to be executed. The development of a new parallel adaptive data stream classifier for data streams, termed MC-NN, is presented. This research is motivated by the fact that very little work has been conducted on the development of real-time scalable parallel data stream classification, even though many applications with high throughput data streams exist. MC-NN is naturally parallel and has been implemented on a computer cluster. MC-NN realises real-time adaptation of data stream statistics using a novel implementation of Micro-Cluster and a Nearest Neighbour classification approach. Loosely speaking, Micro-Clusters adapt quickly to concept drifts through splitting into new Micro-Clusters based on variance and misclassification errors. Adaptation and learning within the cluster is performed through the deletion of Micro-Clusters with a low participation. A Micro-Cluster is considered participating if it regularly absorbs new data instances by being correctly situated over a data pattern. An empirical evaluation of MC-NN showed that it is competitive in terms of classification accuracy and adaptation to concept drifts with other existing popular data stream classifiers, i.e. Hoeffding Trees, adaptive Naïve Bayes and real-time KNN. The results show that MC-NN has a similar or better overall classification accuracy compared with its competitors and better adaptability to concept drifts. Furthermore, the results show shorter runtime of MC-NN compared with its competitors. Parallelisation of MC-NN is achieved by distributing Micro-Clusters to computational nodes in a computer cluster. Adaptation and training is achieved by concurrently distributing training instances from the data stream evenly among the computational nodes in the cluster. Each node then trains and adapts to concept drift in the same way as the serial implementation of MC-NN. Classification is achieved through the use of a voting mechanism by each computational node. An architecture that allows the parallel processing of data streams has been realised and implemented. The architecture is based on integrating various distributed Open Source technologies such as Hadoop, Samza and Kafka. The paper describes the use of these technologies for parallel data stream processing and highlights issues and experiences. An empirical evaluation of the parallel MC-NN implementation utilising multiple data streams of varying attribute and class label sizes, shows that parallel MC-NN scales well with respect to the number of computational nodes utilised and the amount of Data Stream Generators used. This research has been supported by the