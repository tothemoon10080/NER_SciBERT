Predictive maintenance (PdM) is applied to monitor a system’s life cycle to provide current diagnostics, prognostics and provide information capable of guiding maintenance related decisions. Often, an asset’s life cycle is monitored using multiple measurements which translate to high-dimensional (multivariate) data. The large volume of data used to describe an asset’s life cycle has led to current state-of-the-art data-driven PdM relying on machine learning (ML). As research shows, high-dimensional data diminish ML algorithm performance. Generally, high-dimensionality is managed by feature engineering, except asset data characteristics differ from characteristics managed in typical feature engineering problems. In data-driven PdM, information regarding observed faults in an asset is important. Such information is often misinterpreted or lost when general feature engineering is performed on asset data. This work proposes a correlation and relative entropy (C-RE) feature engineering framework specific to asset data. C-RE, applies correlation based hierarchical clustering and relative entropy through the measure of Kullback–Leibler divergence to generate a lower-dimensional feature subset of the original data. The resulting feature subset has minimal redundancies and the highest content of domain-specific information relating to the influence of faults observed during an asset’s life cycle. The utility of C-RE is demonstrated on the Commercial Modular Aero-Propulsion System Simulation (C-MAPSS) dataset which describes the run-to-failure life cycles of multiple aircraft engines.Predictive maintenance (PdM) is the current state-of-the-art choice for an effective maintenance strategy that provides an enterprise the ability to better anticipate or address unscheduled downtime due to both preventable and unforeseen failures ( In this domain, complex systems are monitored through multiple sensing instruments, which often translates to high-dimensional multivariate time series data ( In While ML algorithms are applicable to data-driven PdM capabilities, their performance and reliability is often limited by the quality of data representation used to train and test algorithms ( Unfortunately, raw asset sensor data is rarely usable by data-driven methods, including machine learning algorithms, due to characteristics such as variations in interpretation of domain specific information such as fault influences, and difficulties choosing a representative sample from large datasets ( High-dimensional datasets highlight the limitations of ML algorithms ( The Also, redundant variables (features) and variables with minimal variance change (i.e. low information content) highlight the limitations of ML algorithms ( As systems become more complex and require increased volumes of multi-sensor measurements to perform accurate ML-driven PdM, a useful data representation of a system’s life cycle data is required ( In other fields of study such as computer vision and robotics, data is transformed using dimension reduction methods such as feature selection to generate a data representation conducive to accurate ML algorithm performance ( Information regarding observed faults in an asset is important in data-driven PdM ( Generally, ML-driven PdM tools are applied to identify fault modes and capture their effects within an asset’s life cycle ( The concept of relating faults and their implications as domain knowledge in asset data is further developed based on entropy ( Feature selection methods are commonly applied to circumvent the General feature selection methods are acceptable for general data. However, the conditions and criteria for the effective selection of a useful feature subset do not always translate well for asset data in the PdM domain for two main reasons: the interpretation of feature characteristics vary between general data and asset data (for example, in asset data, low variance can be an indicator of a nominal state, while a high variance feature could be an unreliable sensor); standard feature selection methods do not provide a method to measure retention of information pertaining to domain knowledge ( Feature selection methods are often criticized due to their interpretation of ideal features and how they are selected. For example, depending on a method’s interpretation, it may either select an excessive number of features, or redundant features in which many of the features may be correlated. These variations in data interpretation have also been a prohibitive factor in the acceptance of general feature selection methods in the PdM domain. In asset data, a feature’s importance should be attributed to its ability to describe domain specific information. Where, in the application of standard feature selection, per their selection criterion, may dismiss features that contain high content of domain specific information. Thus, the feature subset generated by standard reduction methods, may be useful for general ML application, but this does not ensure that they are relevant for ML techniques when applied to data-driven PdM. Given the standard definition of an ideal feature, as well as the requirements of retaining domain knowledge during feature selection, an amendment to the current definition provided in This work proposes a feature selection framework that uses feature based hierarchical clustering in combination with relative entropy (Kullback–Leibler divergence) to provide feature subset with minimally correlating features which maintain the highest content of discriminatory information within samples per feature. With respect to PdM, the selected features will hold the highest content of domain specific information conducive to predictive analytics. This work aims to prove that feature selection based on domain knowledge, can still be as useful as the original dataset. This is due largely to the proposed method’s efforts to retaining domain specific information necessary in other analysis specific to PdM for asset management. This paper is organized as follows: Section Feature selection methods can be broadly classified into three categories: Filter methods are often based on variable ranking methods ( Alternatively, using the mutual information criteria, the variable ranking method evaluates the measure of statistical dependence between any feature and the target feature. The ranking is administered such that less dependent features are ranked higher and the more dependent features are ranked lower. Thus, the selected feature subset will consist of highly independent features. Wrapper methods use ML algorithms to select feature subsets based on their usability for prediction. Simply, Does the feature or feature subset contribute to a good measure of accuracy for the applied ML algorithm? Often, Although there are various feature selection methods, they can often be categorized as either a In the information theory domain, the measure of As For instance, suppose a sensor Given two probability distributions Within the PdM domain, provided the life cycle condition of a system is continuously sensor monitored, a continuous relative entropy analysis starting from the initial normal state until failure should have a monotonic trend. This trend would describe the difference in entropy between observations, in relation to the normal state, as the system experiences various forms of general wear and degradation. The influence of a fault is related to the moment where the observed trend begins to deviate from entropy measures relating to a normal state at an increased rate.  The application of Kullback–Leibler divergence (KLD) is widely accepted in the predictive maintenance domain. In The authors in The growing interest in artificial intelligence have led the progression of standard applications of machine learning to deep learning where inference is based on the use of deep artificial neural network (DNN) architectures ( The initial success of DNNs in fields such as computer vision, robotics, finance and medicine, have motivated the adoption of DNN in the PdM domain ( It is widely accepted that DNNs were developed to better manage high dimensional complex data ( While the main function of the optimization is to ensure the DNN architecture is generalizable, the optimization is heavily dependent on the number of features contained it the raw data. In a DNN, the number of parameters increase as the number of features increase. For example, in a convolutional neural network (CNN) (DNN architecture for ML-based image analysis) applied to RBG images of Regularization methods are often applied to manage the increase in parameters due to feature size by controlling the significance of each feature ( As regularization tuning in a DNN requires user input and interpretation, this process of feature selection can be considered arbitrary and case specific to the practitioners experience level ( This work believes that the development of DNNs and their applications in PdM are exciting and should be further developed. Furthermore, this work believes the performance DNNs applied in PdM can improve when applied with prior data manipulation methods specific to asset data. The succeeding section details a feature selection framework dedicated to achieving a more useful representative feature subset of asset data. As experiments in this work demonstrates, the use of proper feature selection, prior to a DNN-based PdM analysis, improves the DNN architecture’s performance. The proposed Correlation and Relative Entropy (C-RE) feature selection method is broadly summarized in The selected feature subset is defined as a reduced dimensional form of the original data that is representative of the information contained in the overall dataset. Also, the selected feature subset hold the following properties:   With the following properties, the selected feature subset of the proposed framework is conducive to requirements of both the PdM and ML domain. In this process, hierarchical clustering was applied to define the correlative relationships between features. A standard form of hierarchical clustering is presented in This form of hierarchical clustering is often referred to as unweighted pair group method with arithmetic mean (UPGMA) ( In In the proposed method’s adaptation, the observed feature set is transposed in order to compare features as opposed to the traditional sample comparison. To cluster the features based on correlation, a form of In the form presented in The objective of the In this work, the relative entropy analysis is applied as described in For each feature Then, the relative entropy analysis is performed on all observation in each feature The Within each correlation cluster Continuing with the example feature set used in Generally, any distribution that can interpret real-world values, and used for random sampling can be applied for relative entropy analysis of asset data. Note, the relative entropy evaluation does not focus on specific sample characteristics. Alternatively, relative entropy aims to characterize the differences in sets of observations, through a measure of differences between distributions. In this work, the This section presents the performance and utility of the proposed feature selection framework on Datasets, C-MAPSS-FD001 and C-MAPSS-FD003 are selected from the NASA’s PCoE ( Engine fleets in C-MAPSS-FD001 and C-MAPSS-FD003 both contain 100 engines of varying sample lengths. The only differentiating factor between the two fleets are the fault modes experienced. The C-MAPSS-FD001 engines have experienced one A mutual information filter (VR-MI) and decision tree wrapper (W-DT) were selected as comparative feature selection methods ( The VR-MI method evaluates dependencies between variables, and will issue a higher scoring to more independent features. For this work, the selection threshold was set such that features that fall within the 30 percentile of the most independent features were selected. Although in a normal use, one would apply a VR-MI threshold based on application. A generic threshold was selected to mitigate over parameterization of the selection method, and allow a fair comparison. In W-DT, the grid search algorithm was applied. The features selected were those which generated the best model accuracy score in a 3-folds cross validation approach. The feature subsets that generated the most accurate classifier performance were included in the selected feature subset. The proposed framework (C-RE) was parameterized to Prognosis is used to estimate the remaining life cycles of a system prior to failure ( A convolutional neural network remaining useful life (CNN-RUL) prognosis algorithm described in When applying the prognostics algorithm from Root Mean Square Error (RMSE) and Mean Absolute Error (MAE) are used to evaluate the accuracy performance. Each metric provides a measure of error between the actual RUL and the CNN-RUL prognosis algorithm predicted RUL. Lower scores correspond to an accurate performance of the tested algorithm. To limit algorithm bias, and ensure generalizability, in each dataset, 70% and 30% life cycle observations are selected for training and testing respectively. In PdM anomaly detection is the ability to detect unfamiliar operating conditions that vary from known nominal operating conditions. Thus, providing the ability to know when a fault has occurred within an asset. Within ML, anomalies are observed data samples that do not contain the same characteristics as the majority of the dataset which are deemed nominal samples ( The OCSVM is a machine learning anomaly detection algorithm trained with unlabeled observations. Newly observed data is then measured against learned data using a distance measure to determine level of similarity. Observations that strongly deviate from learned data are identified as outliers. As all engines in both datasets are run-to-failure, it is assumed that the first set of In data preparation, an engine’s life cycle The For anomaly detection performance evaluation, the training set The metrics of false positive (FP) (undetected failure observations), true positive (TP) (detected normal observations), false negative (FN) (undetected normal observations), true negative (TN) (detected failure observations) were used.   The features of FD003 per Also this supports C-RE’s selection of 6 sensors in FD003, while only selecting 3 features in FD001 which has one fault mode. Although, this should not imply a ratio of 1 fault mode to 3 features, one can concluded that more features are required to describe the complexity introduced by the second fault mode present in FD003 engines.  Although the main differentiating factor between the two datasets are their fault modes and number of faults, one can observe that VR-MI’s ranking of the features can be inconsistent. Also, the final number of features selected are equal between both datasets. Although, they could be different features, there appears to be no clear method or reason as to why certain features were prescribed specific rankings. Also, VR-MI’s ranking does not seem to imply that it is considering information content within each feature. When observing features This further emphasizes the necessity to consider a feature’s ability to observe the totality of an asset’s life cycle when considering which features to select during dimension reduction. For asset data, a feature selection method must be able to contextualize the sample variance of a feature. This is provided in C-RE through the application of relative entropy measure per feature. Standard methods such as VR-MI only consider the features importance based on comparison to other features. Using such a method would consider a feature with high variance due to noise as a high ranking feature without considering the meaning of the sample variance. In W-DT based feature selection, a grid search algorithm, based on a 3-fold cross validation, was used to select the feature subset that provided the best accuracy score for a decision tree classifier. Similar to VR-MI, W-DT’s feature selection process can be considered inconsistent. Although W-DT can guarantee a selection of sensors that improve algorithm performance, one cannot ensure features within the selected subset are not redundant. As the selection of features considered are based on the order of observation, if a useful feature was observed further in a higher index in a series of features, features indexed prior to the useful feature are grouped in the final selected subset. In a classification problem, as the algorithm is based on, this redundancy might not have negative implications. In other ML methods such as regression or unsupervised learning, redundancy may skew the performance of the applied algorithm ( Alternatively, a useful feature can be observed in an earlier index of a feature set and generate a high classification accuracy. When an another important feature is observed in a higher index position, but does not increase the accuracy, that feature may not be selected. This occurs while redundant or less useful features remain in the subset of a feature that generated a high accuracy score in an earlier index order.  The results of Prediction accuracy results in The increased selection of features by C-RE between dataset FD001 and FD003 show its sensitivity to increased information content in datasets. Recall, the engine fleet in FD003 are described by more complex data due to their additional experienced fault mode compared to the engines in FD001. It is expected that the additional fault mode HPC, required more features to generate an accurate and useful lower-dimension feature subset. C-RE was able to identify such features, thus the increased feature subset size to describe the FD003 dataset. This further proves that the C-RE framework only selects features that are necessary to describe the dataset of interest. C-RE’s sensitivity to fault implications show that C-RE is selecting features based on their ability to capture such domain specific information as described in this work. Also, this further emphasis that measured faults in asset data can be inferred as domain knowledge, and the influence of fault modes can be interpreted as domain specific information. In anomaly detection performance evaluation, C-RE’s feature subset usability in standard unsupervised ML methods is demonstrated by the performance of the OCSVM anomaly detector. Not only does C-RE allow for high anomaly detection rates, but also for strong rejection of false negatives (FN) and false positives (FP). Again, the performance of the original data in both dataset FD001 and FD003 show that high dimensionality can hinder the performance of ML algorithms. Results for dataset FD001 are shown in Also, one could interpret the close to  The described experiments further emphasis C-RE’s ability to use domain knowledge through measuring a feature’s ability to capture the influence of faults in data samples. Also, C-RE’s application of hierarchical clustering to eliminate highly correlated features with similar entropy deltas ensures limited redundancy of features. Thus, the C-RE feature selection framework is able to identity the optimal lowest number of features required, while ensuring the feature subset consists of features with domain knowledge contextualized variance. The C-RE framework is an applicable feature selection method ideal for PdM in the asset management domain. With respect to asset data, C-RE is capable of selecting features capable of measuring the influence of faults on an asset. Thus providing a feature subset, with high contents of decipherable domain knowledge, ideal for further ML-based PdM. The form of C-RE proposed in this work required limited parametrization. As presented, C-RE requires the parameterization of the allowable percent entropy change Experiments show that such minimal change in correlation limits, could result in the difference of selecting one more or less feature that could contribute to improved algorithm performance. Although such sensitivity from selection parameter limits were also observed in VR-MI and W-DT, further research is required to develop a parameterization that manages the sensitive generated by Experiments have proven C-RE is capable of measuring the influence of faults in asset data. Thus, C-RE has the potential to be further extended as a method to identify faults or occurrence of faults during the observation of an asset’s life cycle. For example, a known fault mode could be characterized by its influence on a known asset’s data by attributing the fault mode to a specific entropy delta C-RE is a robust feature selection method. C-RE is applicable as a general feature selection framework and a feature selection framework dedicated to asset management analysis. Through further research C-RE can be applied as a feature selection and fault identification method in the domain of PdM for asset management. To ensure the full robustness of the C-RE framework, datasets of different machine types should be applied. Further development of the C-RE will be based on datasets that differ from the C-MAPSS data set applied in this work.  This paper proposes a general and asset data dedicated feature selection framework (C-RE). C-RE accurately selects the lowest dimensional subset of features, while retaining the highest content of domain specific information pertaining to asset data. First, C-RE applies hierarchal clustering to measure the correlation between features to create groups of similar features. Within each cluster of similar features, C-RE uses relative entropy through the measure of Kullback–Leibler divergence to identify features capable of measuring influences of faults on a dataset. Features within each cluster group that achieve or exceed the minimal required entropy delta are selected. Then, the selected features are used to form a lower-dimensional feature subset of the original data. The resulting feature subset has minimal redundancies and the highest content of domain specific information pertaining to asset data. The utility of the C-RE framework is demonstrated in a predictive maintenance (PdM) case study based on turbofan jet engine datasets. The datasets described jet engines’ degradation trends due to observed faults during their life cycle. Information regarding observed faults is clearly important to retain when performing data-driven PdM analytics. Thus, C-RE feature subsets were the optimal feature subset as they: (a) addressed the problem of “ This work is presented as a feature selection framework that is conducive to ML dependent predictive maintenance analytics. As asset data in the PdM domain does not observe general characteristics, more advanced feature selection methods which preserve critical domain specific information are required. We find that C-RE was able to retain this crucial information while identifying the lowest dimensional feature subset required to describe each engine’s underlying degradation trend when applied to PdM dedicated ML algorithms. Experiments were conducted to test the utility of the C-RE feature subset on the This work provides evidence that performing feature selection with a criterion focused on preserving information specific to domain knowledge can generate a feature subset useful to the applied domain as well as general ML algorithms. C-RE shows promise in PdM for asset management via the application of feature selection, and as an ideal tool for dimensionality reduction with a generalizable framework for other applications with datasets containing domain knowledge related sample variance.  The authors would like to thank the