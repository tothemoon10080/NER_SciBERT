Latent variables models used in principal component regression (PCR) or partial least squares regression (PLSR) often use a high number of components, and this makes interpretation of score and loading plots difficult. These plots are essential parts of multivariate modeling, and there is therefore a need for a reduction of the number of components without loss of prediction power. In this work, it is shown that such reductions of PCR models with a common number of components for all responses, as well as of PLSR (PLS1 and PLS2) models, may be obtained by projection of the X modeling objects onto a subspace containing the estimators bˆ
                     
                        i
                      for the different responses y
                     
                        i
                      . The theoretical results are substantiated in three real world data set examples, also showing that the presented model reduction method may work quite well also for PCR models with different numbers of components for different responses, as well as for a set of individual PLSR (PLS1) models. Examples of interpretational advantages of reduced models in process monitoring applications are included.Solutions of ill-posed regression problems using principal component regression (PCR) or partial least squares regression (PLSR) based on latent variables (LV) models, offer the added advantage of score-loading visualizations. However, for good predictions such models often require so many components that interpretation of the plots is difficult, and this calls for model reduction methods without loss of prediction capability. This is the main results of the orthogonal signal correction (OSC) methods In the present paper, the 2PLS theory is extended to show that any PCR or PLSR (PLS1 and PLS2) model with an In the multiresponse case with The theory behind these model reductions is given in Assume an ill-posed linear regression problem with modeling data The loading matrix In PLSR, we may use the orthogonalized LV model The loading weights matrix Finally note that in the case of collinear responses, the matrix In PCR and PLSR, all objects in  See Note that The main point with the examples below is to indicate the possibility to obtain approximate results according to The data in this example are provided by the Wentzell group at Dalhousie University ( From The following example uses multivariate regression data from a mineral processing plant After centering and standardization of the data, and using the first 120 samples for modeling and samples 181–240 for validation, the results in The conclusions drawn from In order to illustrate the interpretational advantages, the PLS1 model ( The 2PLS scores and loadings for  Using original score plots and projections of This data originating from the Cargill company is found on the Eigenvector home site ( In The main idea behind the model reduction results presented above is that models for prediction and interpretation should not necessarily be the same, and the practical result of model reduction is increased interpretability. An example of that is the 2PLS algorithm, where a single response PLSR model is reduced by projection on a plane containing It has been shown that a PCR latent variables model with a common number of components for all responses, as well as a PLSR model for one or several responses (PLS1 or PLS2), can without any change in the predictions be reduced by projection of the The theoretical results are substantiated by use of three real world data set examples, also showing that the presented model reduction method may work quite well also for a set of individual PCR latent variables models with different numbers of components for different responses, as well as for a set of individual PLSR (PLS1) models. Some interpretational advantages are also exemplified, and more details of this are found in the references. The proof is given for the PCR estimator We first prove the theorem for the special case of Note that For the general case allowing for