Model-based fault diagnosis tends to be too expensive or time-consuming to apply in the mineral processing industries, owing to the complexity and variability of operations. In contrast, data-based methods are inexpensive, but do not exploit the availability of first principle knowledge of plant operations. In this investigation, the use of process causality maps in conjunction with data-based fault diagnosis is considered as a hybrid methodology that can leverage the advantages of both approaches. Extreme learning machine algorithms are used to implement the data-based component of the approach. These algorithms can be deployed rapidly on large-scale systems and have the ability to deal with highly nonlinear systems. Two different variants are considered, viz. one used in combination with principal component analysis, as well as one with a bagging algorithm for fault diagnosis and applied to an industrial concentrator circuit in South Africa. The use of process causality maps led to significantly more effective fault diagnosis, while the use of extreme learning machines in combination with principal component analysis likewise allowed markedly better fault detection and diagnosis. In contrast, fault diagnosis with the bagging approach did not perform particularly well, owing to the high degree of correlation between the variables, which made it difficult to isolate individual causal variables.Key to remaining competitive within the mineral industry is ensuring that processes are operated as close as possible to their optimal settings, for as long as possible. These mineral processes can be complex, dynamic and non-linear, among other owing to the involvement of multiphase systems subject to highly variable feedstock. Multivariate process performance monitoring is therefore essential to ensure process operation with as few faults as possible, as well as improved understanding of the contributors to the success or failure of the process operation. In the mineral process industries, statistical process control (SPC) is often used to monitor key performance indicator (KPI) variables, such as the concentration of valuable metals in concentrate or tailings streams or the determination of particle size distributions in comminution systems. These KPI’s are usually not measured online, but are based on laboratory assays that are subject to sampling errors and comparatively long turnaround times. In contrast, operational data associated with process conditions are usually collected online at high frequency. Since the collection of KPI and process data are not synchronized, uncovering the deep underlying or root causes of processing problems giving rise to deviations in the KPI variables may be difficult. Typically, the analyst would need to find the primary deviations in the process data that can be associated with those in the KPI data, which may not be a trivial task, owing to recycle streams and varying or unknown time lags in the process circuit. Approaches to the problem of root cause analysis can be broadly grouped as either based on fundamental process models ( Unlike model-based approaches, data-based approaches require little modelling effort, making them relatively easy to implement and more popular for large-scale industry applications ( Although root cause analysis has been researched extensively and comprises a large variety of tools, techniques and methodologies, no single method has been identified which has all the desirable features required by fault detection and diagnostic systems ( In this paper, a novel approach to root cause analysis is proposed, based on the integration of process causality maps with data-based systems employing extreme learning machines (ELM). Two methods for the use of ELMs are proposed, namely the use of ELMs with principal component analysis (ELM-PCA) and ELMs used with bagging methods (ELM- Process causality maps can be seen as consisting of process flow diagrams with unit specific causality maps dangling from each process area/unit ( The process is further decomposed into areas and units in subsequent layers of the process causality map, following the general layout of the process flow diagram, accounting not only for the forward material paths, but also recycle streams. Process performance measures are attached to the various functional nodes within the process causality map, allowing for top-down root cause analysis to be performed, as well as interaction or intervention with the model at any level. At the lowest level of the process causality map, individual instrument units are found with process performance measures for data validation and instrument failure detection. The aim of structuring the process as such is not to overanalyse the process, but rather to treat the process as a series of interconnected major process units with each process unit being functionally described by the fundamental physics of that process unit. Division of the process into areas and units are based on their location within the process flow, the interactions and interconnections between the areas and units, the function and purpose of the major units and their mutual dependence during operation. All process data, including high frequency online process data and low frequency offline laboratory data, are thus grouped logically into functional nodes and combined with the relevant metadata. At the various functional nodes, process performance measures, in the form of data-based criteria, are attached to suit the fundamental drivers of the process units. Process performance measures can be anything from single values or simple time series plots to complex customized statistical data analytical techniques used to monitor process performance. These measures are not only used to describe the performance of the various process units, but they are also used to monitor the process equipment and control solutions affecting process performance. When combining relevant functional nodes hierarchically, a unit specific causality map is formed ( At any arbitrary system level, a KPI on a particular process unit can be a function only of process data flow at that level, viz. inputs and outputs across the unit boundary plus internal states and parameters of that unit. By extension, a KPI of an entire, unpartitioned system, for example a concentrator section or circuit, should be defined in terms of input and output, internal states and parameters defined on the section as an unpartitioned meta-unit. KPI’s themselves are not aggregated across system levels, but appropriate KPI’s are recalculated at each level, with the source data being rolled up across system levels. This means that partitioning of the causality graph takes place as it traverses system levels from root to leaf. E.g. in the case of a concentrator circuit, the total power is aggregated across units of the concentrator in order to calculate the ratio of the total power consumed to the total ore feed rate. If it is not possible to define a relationship between two functional nodes, it is impossible to determine the root cause at the lower functional node, owing to a shift in the process performance at a higher functional node in the unit specific causality map. Therefore, constructing such a unit specific causality map not only requires a sound knowledge of how the various process variables relate to one another, but also a fundamental understanding of how the various process units operate and interact with each another. Unit specific causality maps are a culmination of process, equipment and control knowledge, where the latter could be explicit knowledge from first principles or heuristics, but could also include tacit knowledge in the form of operator experience not explicitly recorded. Analysis, monitoring and diagnosis of process operating performance based on the use of principal components are well established. The basic theory (e.g. Instead of monitoring the original variables, only the Extreme learning machines (ELMs) are learning algorithms for single hidden layer feedforward neural networks (SLFNs), in which the weights associated with the hidden layer are chosen randomly and the weights associated with the output layer are determined analytically ( More formally, given In this model, In the hidden layer output matrix of the network ( Assignment of random input weight vectors Calculation of the hidden layer output matrix Calculation of the output weights, Whereas traditional learning algorithms employed by feedforward neural networks are limited in their learning speed, mainly gradient descent-based methods getting stuck in local minima, as well as iterative parameter estimation, ELMs cannot only achieve extremely fast learning speeds, but also tend to provide good generalization performance, owing to the learning algorithm not only typically reaching the smallest training error, but also the smallest norm of the weights ( Furthermore, for conventional feedforward neural networks there is a dependency between the different layers of weight and bias parameters that requires all parameters to be tuned. For SLFNs with The ELM-PCA methodology is based on the neural network PCA (NNPCA) technique developed by As with the neural network PCA methodology, the ELM-PCA methodology can be considered to consist of two core stages, viz. residual generation and residual evaluation. Residuals are generated by comparing the actual behaviour of the process to be supervised with that of an ELM trained on the same observations. The residuals derived from the difference between the actual process and the SLFN predictions are subsequently evaluated using PCA. The residuals are expected to be close to zero under the NOC. Under abnormal operating conditions, the zero point of the residual variable would drift away. Comparing the residuals with a decision function or predefined threshold from the NOC, statistical analysis is done in order to determine if the new process behaviour can be classed as in-control or out-of-control. Multivariable control charts are used to monitor the residuals due to the correlations between the residual variables. Therefore, the SLFN acts as a nonlinear dynamic operator used to remove the nonlinear and dynamic characteristics, whereas PCA is applied to generate monitoring charts based on More formally, the process can be summarized as ( Although contribution plots are used widely with principal component analysis to identify faults, they are known to suffer from a number of drawbacks. For example, the maximum number of variables that can be fully reconstructed cannot exceed the number of principal components retained ( An alternative to statistical process control contribution plots for root cause analysis lies with variable importance analysis (e.g. ELM- In the classification approach for ELM, a 1-of- With From the data observations, Train a classification model, using { Assess the model performance, using { Store performance of the model and repeat from (a) until Repeat from (a), each time randomizing a different variable from the data observations, until all variables have been randomized, one at a time. For variable importance analysis, the effect that randomization of a variable has on the classification accuracy of the model, is considered to be a direct measure of the influence of the model. Unlike the contribution plots arising from the PCA models, the ELM- The ELM-PCA and ELM- The detection delay is simply the time taken to detect a fault once it has occurred, the reliability index is the fraction of faults detected and the false alarm rate is the fraction of false alarms generated by the monitoring schemes. For the industrial concentrator case study, the mineral processing plant under investigation concentrates UG2 ore containing platinum group metals. The process consists of three grinding circuits with their associated flotation circuits ( Having identified a steady decline in the measured recovery of the concentrator ( Class 1 – reference data (NOC) – data records 1–100 Class 2 – changeover data – data records 101–219 Class 3 – fault data – data records 220–341 For the final tail grade and grind, Classes 2 and 3 were combined and some data were lost owing to missing values. This resulted in two classes, with the reference Class 1 having 100 records and the combined Classes 2 and 3 having 235 records. For the silica circuit grind, Classes 2 and 3 were also combined. This yielded a Class 1 (NOC data) with 75 records and the combined Classes 2 and 3 with 39 records. Further analysis of the process data is based on these three classes according to the methodology outlined in From a process causality map (   From these variable contributions on average over all three classes ( These findings are supported by the fact there was indeed a significant shift in both the fine silica circuit tails grind component and the coarse chrome circuit tails grind component. In addition to this, a shift in the silica circuit tails grade was also noted. Furthermore, there seems to have been a certain amount of mismatch between the initial recovery classes and the subsequent silica and chrome circuit tails grade and grind classes. It may be possible to improve the reliability of the results by recategorizing the data into more distinct silica and chrome circuit tails grade and grind classes that still align with the recovery classes. Although it was found that the shift in the chrome circuit performance was more significant than that of the silica circuit, it was markedly smaller in magnitude. This could be indicative of the fact that the shift in the chrome circuit performance was found to be significant, owing to incidental correlation, whereas the less significant shift in the silica circuit performance could have been owing to systemic variation or causation. With the aim of finding the root cause of the shift in recovery, the investigation will follow the path of causation and thus understanding the shift in the silica circuit performance. Unfortunately, no reliable data are available regarding the silica circuit flotation process and the assumption has to be made that the increase in the silica circuit tails grade is due to the coarsening of the silica circuit grind. From a process causality map ( As before, for the reliability index (  With no significant shift in the primary flotation (rougher) tail grind and a significant increase in the power drawn by especially Dividing the chrome classification mass split data into classes similar to the recovery data and comparing these classes of data based on their distributions as graphically portrayed in This would seem to confirm that a shift had occurred in the chrome classification performance, which would have resulted in a shift in recovery. Further investigation revealed that an increase in the primary mill feed rate and inlet water ratio resulted in an increased feed flow to the cyclone, albeit at a lower feed density, thus overloading the chrome classification cyclone and causing it to underperform. In this paper, the combined use of process causality maps and extreme learning machine algorithms for root cause analysis is proposed and compared to fault isolation with conventional principal component analysis. Hierarchical system decomposition was used as a framework for deriving process causality maps from a process system. This not only facilitated retention of the process flow topology and connectivity information via structural decomposition, but also the use of causal and fundamental process and unit specific information. It was found that the functional decomposition of the process, based on the fundamental characteristics of the process units, into unit specific causality maps significantly contributed to simplifying the practical challenge of variable selection ( Extreme learning machines were proposed as a replacement for the more traditional neural network (multilayer perceptron) found in the neural network PCA algorithm of ELM-PCA derived metrics typically outperformed the linear PCA metrics, attributed to its nonlinear form and good generalization performance. As an alternative to statistical process control contribution plots for root cause analysis, the problem was approached from a classification perspective through the use of the ELM- Even so, it should be noted that when using supervised methods, such as the ELM- From a practical perspective, the application of process causality maps was found to not only save time during the analysis through their reusability, but it also greatly simplified the challenge of monitoring the process. This meant not only improving the ability of the techniques through a better focussed application, but also improving the interpretability of the results owing to the reduction in complexity. Whereas the fault detection models allowed for the process to be continuously monitored for fault conditions in future, their integration with process causality maps allowed not only known cause and effect relationships to be confirmed, but it also ensured that known relationships that may not have been reflected in the plant data, could also be identified. In principle, this simultaneous use of both explicit and potentially also tacit knowledge altogether provides a more powerful framework for fault detection and identification on process plants.