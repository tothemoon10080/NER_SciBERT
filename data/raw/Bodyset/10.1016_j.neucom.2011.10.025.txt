Stability and periodicity of neural networks is important behavior in biological and cognitive activities. In order to better simulate a biological genuine model, a special kind of discrete Hopfield neural networks (SDHNNs) in which every neuron has only one input is considered. By applying permutation theory and mathematical induction, we prove that the SDHNN always converges to a stable state or a limit cycle. The SDHNN is extended to the discrete Hopfield neural networks with column arbitrary-magnitude-dominant weight matrix (DHNNCAMDWM) in which there only exits a magnitude-dominant element in every column. Some important results, especially the periodic stability of the DHNNCAMDWM, are obtained. And the XOR problem is successfully solved by the results.Hopfield neural networks, as a signpost that is the second development of neural networks, have been extensively studied in past years because of their wide applicability in solving constraint satisfaction, optimization, associative memories, pattern classification, image processing, etc. These applications heavily depend on the dynamical behavior of the networks. So, the analysis of dynamical behaviors is very important in the design and application of Hopfield neural networks. Previous works mainly consider that the dominant effect of every neuron is the connected weight from itself to itself With a common assumption that the weight matrix is symmetric, From above we know that the dominant effect on every neuron is the connection weight from itself to itself. Applying MDDM, QDRDM and QDCDM, they mainly analyze the stability of Hopfield neural networks whose connected weight matrixes are main-diagonal dominant in these literatures In addition, periodic oscillation in recurrent neural networks is an interesting behavior since many biological and cognitive activities require repetition On other hand, in most references Motivated by the previous discussion, the aims of this paper are to study the stability and periodicity of DHNNCAMDWM by permutation theory and mathematical induction. The method of this paper is as following. According to permutation theory, firstly, SDHNN This paper is organized as follows. In A discrete Hopfield neural networks The network is updated asynchronously, that is, only one neuron In order to facilitate description, we denote that neuron set Let We know that a permutation of a set An extended permutation When The corresponding entry In DHNN, each neuron only has one dominant-input neuron. That means the connected weight matrix of networks is a column arbitrary-dominant matrix in which every column only has one dominant element. Based on this, we define DHNN with column arbitrary-magnitude-dominant weight matrix (DHNNCAMDWM) as following: Let In model In CAMDWM, when Let Let According to  Then neuron set Prior to the stability analysis for DHNNCADWM, in this section we establish the following results about SDHNN. Firstly, we discuss the all states of one neuron of SDHNN. In SDHNN we suppose that one neuron When If If If If When If If When If If Then based on the all states of one neuron we can obtain the following results. (   (    (    (  In this paper the energy function method is more conservative. But when the weight matrix is not a dominant matrix, the method of this paper is more conservative. In the following corollary,  When the all dominant connected weights are nonnegative, we can easily obtain the similar results corresponded to Lemma 2, Lemma 3, In this section, the stability of DHNNCAMDWM is obtained by the stability of SDHNN.  Suppose that the dominant-input neuron of neuron Because the weight When When When If If When If If When If If When If If So, the network  Based on the proof of Theorem 2, we know that the network If  Starting with the arbitrary initial state Then the weight  Based on the proof of Theorem 2, we obtain that the network When the all dominant connected weights are nonnegative, we can easily obtain the similar results corresponded to Theorem 2, Corollary 2, Let When we use the Back-Propagation (BP) neural network and radial basis function (RBF) neural networks to classify, they are difficult to know the number of hidden nodes and deal with the curse of dimensionality, respectively. Based on the SDHNN, a simple method of classification is introduced as fellows: firstly, the classifying space is divided into some subspaces; secondly, the subspaces, which are in the same class, are joined to form a whole by the SDHNN. In this section we use the XOR problem to explain the application of SDHNN. The space  In this paper SDHNN that every neuron only has one input is considered for DHNNCAMDWM in which there only exits a magnitude-dominant element in every column. Simultaneously, the stability and periodicity of those modeles is studied. It is well-known that the Lipschitz function is difficultly constructed. So the stability and periodicity of those modeles are proved with the permutation theory and mathematical induction. According to permutation theory, SDHNN The authors would like to thank the editor and the anonymous reviewers for their detailed comments and valuable suggestions which greatly contributed to this paper. This work was partially supported by the Program for New Century Excellent Talents in University of China, Proof of Lemma 1: When If the relation between (1) When network is operating in the fully parallel mode, if there has not a relation between (2) When network is operating in the serial mode, we prove it with mathematical induction. We orderly select from neuron 1 to neuron If there has not a relation between Suppose that there is the neuron set  Proof of Lemma 2: When Then the structure of If there exits the relation between We suppose that there exits a neuron When there have many neurons satisfied Otherwise, we prove those with mathematical induction. Suppose that there are When network is operating in the serial mode, similar to the proof of Lemma 1, we know that in the serial mode, when When network is operating in the fully parallel mode, we prove it with mathematical induction. The initial state is   Firstly, we prove that when Secondly, we prove that when Based on the above, when Proof of Lemma 3: When Then the structure of So, the state of neuron Similar to the proof of Lemma 2, we know the result. Proof of Theorem 1: According to In a network, which structure is a link, we naturally obtain that the states of the neurons of a directed link are determined by the self-weights and the state of the head neuron of the directed link. According to When network is operating in the fully parallel mode, similar to the proof of According to Lemma 1, Now we consider the combination of In We consider the combination of When network is operating in the serial mode, the provement is as following: When the network is updated orderly from 1 to Imitating the proof in the fully parallel mode, we know that