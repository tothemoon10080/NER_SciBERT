The ability to predict the long-term impact of a scientific article soon after its publication is of great value towards accurate assessment of research performance. In this work we test the hypothesis that good predictions of long-term citation counts can be obtained through a combination of a publication's early citations and the impact factor of the hosting journal. The test is performed on a corpus of 123,128 WoS publications authored by Italian scientists, using linear regression models. The average accuracy of the prediction is good for citation time windows above two years, decreases for lowly-cited publications, and varies across disciplines. As expected, the role of the impact factor in the combination becomes negligible after only two years from publication.Scientific publications encoding new knowledge have different values, depending on their impact on future scientific advancements and ultimately on social and economic development. As a proxy for such impact, bibliometricians adopt citation-based indicators. The underlying assumption is that when a publication is cited, it has contributed to (has had an impact on) the new knowledge encoded in the citing publications ( The questions for evaluative scientometrics are: i) which indicator (or combination) best predicts future impact; and ii) what is its predictive power. The answers to these queries are seen to vary, depending on the time elapsed from date of publication to measurement of accrued citations. There is general agreement on the fact that late citation counts (as proxy of long-term impact) serve as the benchmark for determining the best indicator or combination of (and its predictive power), for each citation time window ( What becomes clear is that there is an embedded tradeoff between level of accuracy and timeliness in measurement, and also clear is that the scientometrician has the responsibility of communicating this in their relations with decision-makers. Indeed, one of the critical issues in reliability of citation indicators of impact concerns the rapidity with which citations accumulate: citations accrue with time, and no one can know for sure for how much time. Citation count can only serve as a reliable proxy of the scholarly impact of a work if observed at sufficient distance from the date of publication, or applying what is called a “citation time window” of adequate length. Yet, given a performance assessment aimed at informing policy and management decisions, no reasonable decision-maker could wait the necessary decades for completion of the citation life cycle. Hence, in designing a research assessment, the question becomes what length of citation time window should be selected in order for early citations to qualify as an accurate proxy of impact. Inevitably, finding an answer entails addressing the tradeoff between level of accuracy and timeliness in measurement. Previous literature signals that the “best trade-off” would differ across disciplines, since the life cycles of citations and peaks in the citation distribution curves vary with this factor ( Some bibliometricians have investigated the possibility of using alternative metrics, or “altmetrics”, to increase accuracy in predicting impact - an area of research now attracting considerable interest. Since reading a publication occurs before citing it, then particularly for the problem of recent publications, it could make sense to count readers (through on-line views, downloads, tweets or other digitally traceable behaviors), rather than relying on citations; it might also be possible to use altmetrics to supplement indicators based on citations ( Several years earlier, In this work, we try to identify the combination of IF and early citation counts that best predicts long-term citations of publications in each discipline, by using two alternative prediction models, one based on the rescaled citation counts and another utilizing the log-transformed citation counts. We also analyse the error distribution for the prediction as a function of the number of early citations. Differently from previous contributions in the literature, we also provide: i) the weighted combinations of citations and IF, as a function of the citation time window and field of research, which best predict future impact; and ii) the predictive power of each combination. These two main contributions of the current work feed into the ultimate aim: providing scholars and practitioners with a tool supporting effective design and implementation of research assessments. Our reference framework is the Italian national research assessment exercise, intended to assess the performance of research institutions through the evaluation of their research products from a period of time. Bibliometric evaluation can be applied to research products indexed in such repositories as Web of Science (WoS) and Scopus. The publications under evaluation are issued in different years, and belong to different fields or subject categories (SCs). We approximate the long-term impact of an article by the number of citations counted nine years after publication. For example, for papers published in 2004 (2005, 2006), we measure the long term impact with the number of citations received up to year 2013 (2014, 2015). In the first model, we rescale the number of citations by year and subject category, an approach frequently used by bibliometricians when comparing publications of different years and fields. In line with       We adopt a linear regression model of the following form: The second model follows an alternative approach based on the logarithmic transformation of the citation counts. The underlying reason is that the distribution of citations to articles in a given year and SC is likely to be approximately log-normally distributed ( In both models, the obvious autocorrelation of the number of citations makes the role of To calibrate the regression models and answer our research questions, we consider a dataset consisting of all Italian publications indexed in WoS over 2004–2006 in the sciences and social sciences (only articles, reviews, conference proceedings and letters, totaling 123,128 items). We exclude the Art & Humanities SCs because of the limited coverage of outputs by WoS. We assign each publication to the SC of the hosting journal, according to the WoS classification scheme. Publications in multi-category journals are assigned to each of the SCs. We run regression models for In some cases, the regressions models for specific subgroups resulted positive to heteroskedasticity tests (such as the Breusch-Pagan test). All regression coefficients presented in the following were thus obtained by a robust standard error approach for the computation of the p-values (heteroskedasticity consistent variance-covariance matrix HC3, see We tested also more complex quadratic regressions and non-parametric classification models (decision trees) but without significant improvement of fitting with respect to the linear regression shown in To start, as an example, we apply the OLS regressions to the 1113 Italian publications falling in the SC “Engineering, chemical” ( Differently, early citations coefficients are large and steadily increasing in the first three years after publication, further weakening the weight of the IF in predicting long-term citations. We can conclude that with a citation time window of three years the role of the IF becomes negligible in both models, which exhibit very good fit (R Regression results for each of the 170 SCs can be found in Appendix A, for a three-year citation window (or in Supplementary Material-SM_1, for all citation time windows).  As for rescaled citations ( As expected, the two models may perform differently at individual SC level; no specific patterns of performance emerge at a preliminary analysis. In order to better appreciate differences across macro-areas, Economics shows (on average) the maximum relative weight of IF with respect to early citations in both models; Psychology is on the opposite side in both models; Life science areas (biomedical research, chemistry, biology, clinical medicine) have varying average early citation coefficients but very similar IF average coefficients, typically very small; Law and political sciences seem to experience a strong effect of early citations in both models; similarly, Engineering (in the left panel) and Multidisciplinary sciences (in right panel).  The accuracy of impact prediction depends anyway on the size of early citations. To further investigate this aspect, for each citation time window, we assign each publication to the relevant quartile of the distribution of cited publications (leaving aside the subset of publications that have received no citations at the citation time window We have deepened the analysis through an even finer classification of citedness, assigning each publication to the relevant percentile of the distribution of cited publications. Considering the impact prediction error we computed the median error and plotted it in As expected, the median of E is higher for publications in the initial percentiles by citations, and then decreases for the subsequent ones, for citation time windows of one year and more (for rescaled citations) and of two years and more (for log-transformed citations). For the three-year citation window the overall median error is 0.40 for rescaled citations, ranging from 0.237 for most cited publications to 0.515 for the least cited (purple line in the left panel). Similarly, we have an overall median error of 0.340, with 0.174 for the most cited publications and 0.407 for the least cited, in the model based on log-transformed citations (purple line in the right panel). Improvements in error may be obtained by limiting the regressions only to cited publications, but such improvements are indeed negligible. The analysis provides several interesting indications for the early assessment of impact of scientific output. First, we provide statistical evidence that, on average, the choice of a three-year citation window is sufficient to predict the long-term impact of scientific publications with acceptable accuracy, using a linear regression model. A complementary conclusion is that the IF has a non-negligible role only with very short time windows (0–2 years); for longer ones, the weight of early citations is dominating and the IF is not informative in explaining the difference between long-term and short-term citations.Second, statistical evidence is also present indicating that the long-term impact of publications with low early citations cannot be predicted with the same accuracy as for those with high early citations. This demands that special attention be taken when papers with a null or small number of early citations need to be evaluated, particularly considering their substantial shares: in our dataset, they are 76.7% for a zero-year citation window, around 13.9% for the reference three-year window, down to 7% for a nine-year window. The decision maker faces an uneasy choice: to apply bibliometrics for evaluation of all publications, which entails accepting high error rates for a noticeable share, or to recur to peer-review for uncited ones, with consequences of higher costs and longer execution times. To any extent, the IF does not help in any way in assessing the impact of such publications. A probably better choice is only to analyse the papers with a threshold level of citations and note the fraction that fall below the threshold. Some papers with good long-term impact will be omitted, but assuming that we are analysing a discipline at the university level there is not a reason to think that the fraction of low citation papers that eventually get high citations should vary systematically across universities. These idiosyncracies of individual papers should be averaged away at the department/university level ( Third, both the relative weight of regressors and the accuracy of prediction vary greatly across macro-areas, and across SCs within the macro-areas. In the future, alternative metrics or “altmetrics” (on-line views, downloads, tweets, other digitally traceable behaviors) could serve as covariates, to improve predictive power when the time window is too short and early citations are too few ( Our work draws inspiration from real practical issues, faced by all practitioners called to design a national research assessment exercise with a short citation time window. Given any such national framework, it is appropriate to use the relevant national distributions to assess the impact of research products, as we have done in the current example. However, given its utmost simplicity, the approach proposed in this paper can be replicated in various other contexts, accounting for the specific objectives of the individual assessment exercise. Giovanni Abramo: Conceived and designed the analysis; Collected the data; Contributed data or analysis tools; Performed the analysis; Wrote the paper. Ciriaco Andrea D’Angelo: Conceived and designed the analysis; Collected the data; Contributed data or analysis tools; Performed the analysis; Wrote the paper. Giovanni Felici: Conceived and designed the analysis; Collected the data; Contributed data or analysis tools; Performed the analysis; Wrote the paper. See See Supplementary material related to this article can be found, in the online version, at doi: The following are Supplementary data to this article: