With highly heterogeneous run of mine coal streams, physical segregation, using on-line quality measurements, can serve as an economically important first stage of the mineral beneficiation process. Segregation enables high quality fractions of the stream to bypass cleaning operations, thereby saving cleaning costs and avoiding the yield losses inherent in any downstream separation process. This paper develops control strategies that address the objectives of quality targeting of the no-wash coal to meet desired quality specifications while maximizing yield of the segregation process to the no-wash stock. It extends earlier work by the authors to deal with the case where the stochastic nature of the quality levels is non-stationary over time, a situation any practical segregation system should accommodate. Two techniques, involving the use of moving windows and auto-correlated statistical process control techniques, are employed to define intervals of time where the process might be considered as “locally” stationary. Subsequently, two different methods, involving the use of empirical and Gaussian distributions, are used to characterize the distribution of quality levels over these intervals. Given this characterization, a special decision criterion is applied to make segregation decisions that enables one to meet the control objectives stated above. Performance of different variants of these control strategies are compared with each other and to an existing industrial segregation algorithm using data obtained at a representative mine. The methods show dramatically improved capability to control quality targets and increase yields. Moreover, different variants of the algorithms are more effective in different application circumstances.Run-of-mine (r.o.m.) coal is frequently washed to meet customer specifications. At an incremental cost of $3-5/ton, it is a major expense of the coal mining process. To reduce/avoid the costs of washing and washing recovery losses, coal operators may physically segregate coal into On-line analyzers are central to such segregation strategies. Typically mounted on the main conveyor belt exiting the mine, they constantly monitor the r.o.m. coal Segregating r.o.m. coal, in real-time, into wash and no-wash, can be easy if one does not pay regard to the yield. For example, the algorithm could simply be that only r.o.m. coal that meets customer specification is accepted, i.e., the Present-day industrial segregation algorithms make cut-off adjustments to improve yield A more elaborate discussion of physical segregation of coal including its role in mine-to-market material flow is given elsewhere There also appears to be potential to employ segregation as a means for splitting the r.o.m. coal into distinct quality stocks, based on ranges of one or more quality parameters, and subsequently applying custom cleaning and blending to these stocks in order to maximize economic returns. This paper only considers segregation of coal from the simpler perspective of making wash/no-wash decisions. The decision to send any block of coal to wash or no-wash, i.e., the cut-off, should depend on two factors: the average quality level of the no-wash pile at the present time and the distribution of the quality of coal expected in the future. Using the segregation criterion reported in Frequent changes in the nature of the mining process or the quality of coal render prediction of the future difficult. Our field observations have shown that it is not uncommon for the distribution of coal quality to change substantially and unpredictably over time. Accordingly, a practical coal segregation system needs to view these observations as a realization of a non-stationary stochastic process. Instead of predicting the future, one might base segregation decisions on the This paper develops methods to determine the current state of the process that are used in conjunction with a previously developed segregation criterion to segregate coal. In contrast to the methods presented in our earlier work The algorithms developed in this paper are implemented in two steps: The present stochastic character of the process is first estimated. A cut-off is then computed, which is the best for the estimated process. If the block of coal is above the cut-off, then it is sent to the wash pile, else, it is sent to the no-wash pile. The computation of the cut-off (the second step), given the present state of the process, is described next. Assume that the distribution of quality The dashed lines, called In practice, the ultimate histogram is not known a priori. It develops over the production period dedicated to making that shipment of coal, changing its statistical nature over time. Therefore, over different periods, coal quality levels have different characteristics, and for any given instant in time they are characterized by a Since the ultimate histogram cannot be predicted, at any given time if a segregation decision is made that is the best for that instant, then one can expect reasonably good performance overall. This is done by truncating the local histogram such that the mean of the truncated portion is the Note that the procedures discussed in this paper do not directly address the issue of differences between the measurements made by the on-line analyzers and measurements made by the customer to verify quality of the coal. It would typically be the objective of the operator of such a segregation system that the latter measurement meets quality targets, whereas we control so that the quality as measured by the on-line analyzer meets a target. In practice, we expect a strong correlation between these two measurements. One simple strategy for dealing with this issue would be to use field observations of corresponding pairs of analyzer and customer measurements of no-wash quality to empirically establish the joint distribution of these two measurements (e.g., a bi-variate normal distribution might be appropriate). Let this distribution be given by Additionally, note that in our demonstrations of the procedures we have developed given below, for the sake of simplicity we have assumed that the mass of each “block” of coal associated with a given quality measurement is a constant. In reality, one need not assume that this is true. It is a simple matter to substitute distributions where the individual measurements are weighted by their mass for ones where they are weighted equally. Finally, we note that our procedure considers segregation control with only one quality variable (e.g., ash or sulfur) and does not consider multiple quality variables simultaneously. Although it appears that the approach might be extended to deal with the multiple variable case (one would need to model the joint distribution of these variables), in most practical applications there is one particular variable that causes the greatest difficulty in meeting quality specifications. If we control for this variable, other variables will also generally meet quality constraints. Further address of this issue is left for future work. To obtain the statistical description of the ash values being realized at the present time, it is first necessary to identify observations that are indicators of the present nature of the process. Obviously, observations from the immediate past are the best indicators of the process. At the early stages of development of the algorithm, it was decided to pick a constant arbitrary number of observations from the immediate past as relevant to the present state of the process. This constant number of observations was called the window width. For example, if the window width was 50 and the present time We present it here again because it has now been more extensively tested with additional field data sets and so that it can compare with a newer procedure. Data was collected from an underground coal mine in Ohio. The mine frequently ran two sections: a high ash section and a low ash section but would also run one section at a time. The on-line analyzer in the mine scanned the r.o.m. coal constantly and every 5 s gave the average ash for the coal scanned. For the belt speed and loading at the mine, each such reading corresponded to about 1 ton of coal. Thirteen data sets were collected from the mine. Each data set was of different length; however, each corresponded to about a single shift of production. Ten of the data sets were collected when the mine was running a single section (low ash or high ash), while three were collected when the mine was running both sections. As can be expected, the ash values varied considerably when both sections were running compared to when just one section was running. This is exhibited in To test for the effect of window length, the data sets were segregated at six different window widths: 10, 25, 50, 100, 150 and 200. Also, to maximize the use of the data sets, each data set was segregated four times to meet four different targets. Using the data sets in this manner resulted in segregation of a total of 90,756 tons. The targets were termed Target 1, Target 2, Target 3 and Target 4, with Target 1 being the smallest in magnitude and Target 4 the greatest. It will be pointed out here that Target 1 in one data set was not necessarily the same as Target 1 in another data set. For example, data set 1 may have been segregated to meet targets of 6.00%, 7.00%, 8.00% and 9.00% ash contents while data set 2 was segregated for targets of 5.00%, 6.00%, 7.00% and 8.00% ash. However, the percentile of the data set that averaged below a certain target level in one data set was approximately the same as in another data set. For example, if 25% of data set 1 could be segregated to meet a target of Target 1 (6.00 in the example), then in data set 2, approximately 25% of the data could be segregated to meet the corresponding Target 1 (5.00 in the example). The targets were set that way to allow comparison of results for various Targets. The algorithm was considered successful if the segregated coal met customer target. The following was concluded from the performance of MWE. ⋅ The method is robust in the sense that it would generally achieve target in both single and double section data sets. ⋅ Window width had little effect in the success of the method in meeting target, with the smaller windows working for about the same number of cases as large windows. ⋅ When the targets were small, small windows did not perform well. This is easily explained. In an empirical distribution fitted to a small number of observations, the tails are not properly estimated as they get clipped off ⋅ Larger window widths tended to give higher yield. To better estimate the tails, it was decided to use the normal distribution instead of the empirical distribution. Note that we are assuming the form of the distribution is normal. Such an assumption is not made in the MWE method. The normal distribution was estimated from the window, i.e., the mean and the variance were computed from the window. This method was called Moving Window Normal (MWN). The following was observed from its performance. ⋅ It was robust in both single and double section data like MWE. ⋅ Yield improved significantly compared to MWE when MWN was successful. ⋅ MWE exceeded the yield for MWN for some cases of large window widths. This is because when the windows are large, it is possible that they contain observations from several distributions, and therefore, forcing a single normal distribution causes errors. ⋅ MWN had difficulties in meeting target as window width increased. This might be expected because of forcing a single distribution to fit non-stationary data. However, when it did work, yields were high. This might be expected because the estimation of the local distribution would be better when wide windows are used if the process is stationary. ⋅ Like MWE, MWN was unsuccessful in meeting low targets. An obvious drawback of the above two algorithms is that the window width is kept constant. Depending on the window width selected, one could be right or very wrong. This is seen in Constant window widths do select the recent history of the process in order to estimate the current process, but given the unpredictable performance for any given window width, it makes sense to include a longer history if the process is stable and less if it is changing. A better algorithm would have variable window widths that respond to the changing ash characteristics. This was achieved, as explained below, by combining Statistical Process Control (SPC) techniques with the MWE/MWN algorithms. When several observations are grouped together into a single window, it implies that they belong to a homogenous group and the process is stable for all observations in that interval. When a new observation is realized, instead of arbitrarily discarding the oldest observation to make room for the new one, one could test to see if the new observation is a reasonable occurrence from the process represented by the window. If it is, then one could include the new observation into the existing window, thereby increasing its width by 1. Increasing the window width when the process does not change increases the accuracy of process estimation compared to discarding useful information in an effort to keep the window width constant. If the new observation was not a reasonable occurrence, then it might be assumed that the process had changed, and so the entire window is discarded and a new one built, starting with the latest observation. Thus, adjacent windows have different widths. In implementing SPC, one must make an assumption on the nature of the process. It was assumed in this work that all windows are ⋅ Estimate the parameters of the AR(1) process from the present window. An AR(1) process is represented by the equation ⋅ The residuals are computed from this model. For a time ⋅ Sequential ⋅ If a In the above procedure, when the old window is discarded, the new window has a width of one (the present observation). As no distribution can be estimated from one observation, one obviously cannot make segregation decisions. However, since segregation occurs in real-time and a decision Once the appropriate window is determined, the algorithm proceeds the same way as MWE/MWN. The method is called SPCMWE when an empirical distribution is fitted to the window and, SPCMWN when the normal distribution is fitted to the window. One might question the impact of spurious observations on yields, e.g., a case where a chunk of tramp iron on the conveyor belt impacts a reading. Such observations are likely to cause false signals that the process has changed. There is little one can do to control for such events. However, note that for this method and other methods given below that employ SPC, our results, which are based on real world data, reflect their impacts on yields and target control. The following was concluded based on the performance of SPCMWE. ⋅ SPCMWE with a MH of 5 is robust in two section data. ⋅ SPCMWE with MH of 5 worked in 46 out of 52 cases (13 data sets segregated to meet four targets each), yielding 51,551 tons out of 90,756 tons. SPCMWE with MH of 15 worked in 43 cases yielding 49,319 tons. ⋅ This method was more likely to fail for smaller targets. ⋅ The window widths were tracked as segregation proceeded to see how the window lengths varied. It was found that most window widths were small (less than 20). ⋅ The method failed in two section data when the MH was increased to 15. When the MH is increased, coals from two sections are forced into one large window causing errors, explaining the failure in two section data. On the contrary, when the coal is from a single section, larger windows should give better estimates of the process. This was seen in the improved performance with MH of 15 in single section data. Conclusions on SPCMWN were similar: The use of the normal distribution increased the yield to 55,035 for a MH of 5, and to 54,911 for a MH of 15. Hence, the normality assumption tends to result in higher yield than when using an empirical distribution. This trend is similar to that seen earlier. However, the number of cases where it worked reduced to 44 from 46 for MH of 5 and to 41 for a MH of 15. In large targets (Targets 3 and 4), there is not much difference in the performances of SPCMWE and SPCMWN (see The performances of the SPCMWE and SPCMWN algorithms were compared with that of the only known industrial algorithm. The industrial algorithm was applied to segregate the same 90,756 tons for the same targets. The industrial algorithm could only send a total of 13,921 tons to the no-wash pile without jeopardizing the target. It failed to meet target in many more cases than the algorithms in this paper. Moreover, in cases where it was successful, as is seen in Algorithms for segregation control were developed in this paper. The algorithms characterize the stochastic nature of the mining process for better segregation decisions in contrast to the industrial algorithm, which is strictly feedback in nature. The results can are summarized below. ⋅ The MWE/MWN methods are simple but robust segregation algorithms. However, success depends on the window width picked. No particular width resulted in consistently high performance and there does not seem to be a practical way to specify window width on the basis of experience or theoretically. ⋅ The SPCMWE and SPCMWN methods automatically adjust window widths. Therefore, there is no guessing involved. ⋅ Although yields for the ⋅ Use of the normal distribution improved yield relative to the empirical distribution. We believe that this occurs because a selection of form of the distribution makes it easier to estimate the distribution if that selection is appropriate. Most of the time at this mine, a normality assumption was reasonable. ⋅ A MH of 5 works better for two section data, while a MH of 15 works best for single section data. This is expected since more frequent updating is desirable in the two-section case. ⋅ All developed algorithms are robust in two section data. This is noted by the mining industry to be a difficult situation to apply segregation technology. For a given range of difference in quality levels among the selections, two-section mine would, in fact, tend to exhibit higher variability than three or more section mines. The authors would like to thank DOE/EPSCoR for funding this research. The help of personnel at the cooperating mine is also gratefully acknowledged.    Let Let Note that, in comparison with Now, for any random variable Substituting into We note that