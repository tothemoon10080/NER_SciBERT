This is the first ever attempt of application in a country other than Italy of the output-to-input indicator FSS, to assess and compare the research performance of professors and universities, within and between countries. A special attention has been devoted to the presentation of the methodology developed to set up a common field classification scheme of professors, and to overcome the limited availability of comparable input data. Results of the comparison between countries, carried out in the 2011–2015 period, show similar average performances of professors, but noticeable differences in the distributions, whereby Norwegian professors are more concentrated in the tails. Norway shows notable higher performance in Mathematics and Earth and Space Sciences, while Italy in Biomedical Research and Engineering.In their article “A farewell to the MNCS and like size-independent indicators” ( Most of the comments and differences in opinion on the validity of per-publication citation indicators related to the concepts of performance, impact, productivity and efficiency. In their “rejoinder” article, What we propose in this work is the first attempt ever to apply the FSS indicator of research performance of professors and universities to a country other than Italy, e.g. Norway. Similarly to Italy, accessing professors’ identity and their affiliation in Norway is quite straightforward. The main problem we encountered was the alignment of the research field classification of professors, as the classification schemes in the two countries are rather different. As we show in Section The principal aims of the work are to present all the difficulties we have encountered to achieve comparable measurements of performance in the two countries, and to show how we have overcome such difficulties. We have devoted then special care in describing the procedure to operationalize the measurements, and all the limits and assumptions involved. This should make hopeful future replications of the exercise in other countries more straightforward. In this study, we have measured and compared research performance of Italian and Norwegian professors and universities in the period 2011–2015. The rest of the paper is structured as follows. In the next section, we will briefly describe the characteristics relevant to our analysis of the higher education systems in the two countries. In Section The Italian Ministry of Education, universities, and Research (MIUR) designates a total of 98 universities throughout Italy with the authority to issue legally recognized degrees. Of these 30 are small, private, special-focus universities (of which 11 offer only e-learning), and 68 are public and generally multi-disciplinary universities. Six of the whole are In the overall system, 94.9 per cent of faculty are employed in public universities, and only 0.5 per cent in Despite interventions intended to grant increased autonomy and responsibilities to the universities (Law 168 of 1989), In keeping with the Humboldtian model, there are no “teaching-only” universities in Italy, as all professors are required to carry out both research and teaching. National legislation includes a provision that each faculty member must provide a minimum of 350 h per year of instruction including teaching, preparation to teaching, exams, thesis supervision, etc. At the close of 2018, there were 54,700 full, associate and assistant professors in Italy, and a roughly equal number of technical-administrative staff. Salaries are regulated at the central level and are calculated according to role e.g. administrative, technical or professorial, rank within role e.g. assistant, associate or full professor, and seniority. None of a professor’s salary depends on merit. New transparency provisions, and timely issue of regulations for the evaluation procedures, are all intended to ensure effectiveness in the faculty selection process. In reality, the systemic characteristics ‒ including a historically strong inclination to favoritism, structured absence of responsibility for poor performance by research units, and lack of merit incentive schemes ‒ undermine the credibility of selection procedures for hiring and advancement of university personnel. This lack of credibility is accentuated in the public eye by the high and growing number of legal cases brought by unsuccessful candidates, by continual critical reports in the social and mass media, and by specific studies of systemic problems ( The overall result is that of a system of universities almost completely undifferentiated for quality and prestige, with the exception of the tiny The Norwegian higher education system consists of several types of institutions, including universities, specialized university colleges and university colleges/universities of applied sciences. Traditionally, the main task of the university colleges has been to offer vocationally oriented education. The universities on the other hand have research as a main task, in addition to providing educational programs from Bachelor to PhD level. Most institutions are state owned. In order to achieve or retain status as various types of higher education institutions, each organization has to be accredited by NOKUT (National Agency for Quality in Education). This institutional differentiation has been changing in recent years. Both universities and university colleges as well as independent research institutes have merged, and several former university colleges have achieved status as universities. This process was accelerated by a structural reform in 2014 ( Currently, there are ten universities in Norway (two traditional universities, two traditional universities which merged with university colleges, one former specialized university institution, and five former university colleges), nine specialized university colleges and 14 university colleges/universities of applied sciences in Norway. The majority of the research in the higher education sector in Norway is carried out at the University of Oslo, the Norwegian University of Science and Technology, the University of Bergen and the Arctic University of Norway. In 2018, these four institutions accounted for 68 per cent of the total scientific and scholarly publication output of the sector. Since 1995, the academic career structure in Norway has consisted of two types of tracks: a research-oriented and a teaching-oriented ( According to the national regulation, research is neither an individual duty nor a right ( Measuring and comparing research performance of Italian and Norwegian professors and universities, requires access to the following data: i) professors’ name and surname, affiliation, field of research, and academic rank; 2) their research output in the period under observation. We extract data on the Italian faculty at each university from the database on university personnel, maintained by the MIUR. For each professor this database provides information on their name and surname, gender, affiliation, field classification and academic rank, at close of each year. We extract Norwegian data from a similar database, the Norwegian Research Personnel Register (providing the official Norwegian R&D statistics, compiled by the Nordic Institute for Studies in Innovation, Research and Education-NIFU). This database contains individual data for all researchers in the higher education sector and institute sector (institutions carrying out R&D which are not part of the higher education and industry sectors) in Norway. For reasons of significance, the analysis is limited to those professors who held formal faculty positions for at least three years over the 2011–2015 period. Furthermore, the dataset is limited to individuals with at least one publication during the time period (non-publishing personnel is not registered in Norwegian databases). As a consequence, performance measures at the aggregate levels concern only productive professors. The bibliometric dataset used to assess Italian output is extracted from the Italian Observatory of Public Research (ORP), a database developed and maintained by Abramo and D’Angelo and derived under license from the Clarivate Analytics Web of Science (WoS) Core Collection. Beginning from the raw data of the WoS, and applying a complex algorithm to reconcile the author’s affiliation and disambiguation of the true identity of the authors, in ORP each publication is attributed to the university professors that produced it. Data on publication output of the Norwegian professors is based on a bibliographic database called Cristin (Current Research Information System in Norway), which is a common documentation system for all institutions in the higher education sector, research institutes and hospitals in Norway. Cristin has a complete coverage of the scientific and scholarly publication output of the institutions. Publication data from professional bibliographic data sources (i.e. WoS and Scopus) are imported to the Cristin system, to facilitate the registration of publications by the employees. The publications are indexed as standard bibliographic references, which can be analysed bibliometrically. In order to obtain the Norwegian dataset, the Cristin publication database has been linked with the Research Personnel Register. Each individual has unique IDs in both databases. However, the IDs are not identical. The linking is based on data on the full name of the professors as well as their institutional affiliations. For a large number of individuals, there is a one-to-one correspondence, and homonyms (different people with identical names) do not represent a problem. In our study we have manually linked professors with identical names, using available data and information, such as field of research (department versus publication field), and CVs/personal web-pages. While the Cristin database contains a complete coverage of the publication output, this study has been limited to the subset of publications which are indexed in the WoS databases, more specifically to the Science Citation Index Expanded, the Social Science Citation index, and the Arts and Humanities Citation Index. As for document types, in the Norwegian system only articles and reviews count as scientific and scholarly publications. As a consequence, the dataset used for comparison contains only the above types of publications. Citations for both Italian and Norwegian publications were counted until 31 October 2018. As said above, this study is based on WoS data. WoS has a very good coverage of scientific literature within the natural sciences and medicine, but the coverage of technology, the social sciences and humanities has more limitations ( In the following, we present the common field classification scheme developed for professors from both countries, and the research performance indicator. Knowing the dominant research field of each professor is a prerequisite of any distortion-free comparative performance assessment. In fact, taking advantage of the Italian professors’ field-classification scheme, In the Italian university system all academics are classified in one and only one field, named scientific disciplinary sector (SDS), 370 in all. SDSs are grouped into disciplines, named university disciplinary areas (UDAs), 14 in all. In Norway, there is no such a fine-grained classification system. The publications are classified in 4 broader disciplinary areas and 86 fields, and in some analyses this system has been used to field classify the academic staff. Because of the remarkable difference in the graininess of the two field classification schemes, which makes it formidable to reconcile, we had to figure out a different common classification scheme. We then recurred to the WoS SC classification scheme, and adopted the following procedure to classify each professor in one and only one SC. First of all, we identified the WoS indexed publications of each professor under observation, over a period of time. We then assigned to each publication the SC or SCs of the hosting journal. Finally, we classified each professor in the most recurrent (dominant) SC in their publication portfolio. To exemplify, we consider the case John Doe, who in the period of observation produced eight articles published in four different journals ( A problem arises when the portfolio is limited to one or a few publications or when one observes more than one dominant SC, an event which in turn is more likely when the professor’s number of publications is low. Therefore, the larger the observation period of production the better. To reduce the probability of low number of publications, just for the purpose of identifying the dominant SC of professors, for Italian professors we extended the observation period of production to eleven years: 2006–2016. We adopted a similar procedure for Norwegian professors, extending the observation period to seven years: 2011–2017 (corresponding data for previous years is not available). Residual cases of professors with more than one dominant SC (14 per cent of Italians and 26 per cent of Norwegians), were solved as follows: For the Italian dataset, we measured the frequency distribution of SCs of the 2006–2016 overall publications by all Italian professors in each SDS. Knowing the SDS of each professor, we assigned to him or her the SC (among the dominant ones) with the highest frequency in the publications of professors belonging to the relevant SDS. For the Norwegian dataset, we randomly selected one of the dominant SCs attributed to the professor. Finally, after merging the datasets of the two countries, we included in the final dataset only those SCs (177 in all) with at least: i) one Norwegian and one Italian professor; and ii) ten professors in total. The final dataset consists of 93 Italian and 6 Norwegian We say that an individual or an institution performs better than another if, all others being equal, it produces more, i.e. it is more productive. Productivity is commonly defined as the rate of output per unit of input. In other words, it measures how efficiently production inputs are being used. Measuring productivity is a complex task, and it is even more so in research organizations. Generally speaking, the objective of research activity is to produce new knowledge. Research activity is a production process in which the inputs consist of human resources and other tangible (equipment, scientific instruments, materials etc.) and intangible (accumulated knowledge, social capital, reputation, etc.) resources, and where outputs have a complex character of both tangible (publications, patents, conference presentations, databases, protocols etc.) and intangible nature (tacit knowledge, consulting activity, etc.). Thus, the new-knowledge production function linking outputs to inputs, has a multi-input and multi-output character.. The calculation of research productivity requires a few simplifications and assumptions. It has been shown ( Most bibliometricians define productivity as the number of publications in the period of observation. Because publications have different values (impact), and resources employed for research are not homogenous across individuals and organizations, we prefer to adopt the definition of productivity extracted from the economic theory of production: the value of output per Euro spent in research (i.e. costs of labor and all other production factors, referred to as capital in the following). This definition recognizes that the publications embedding new knowledge have a different value (impact) on scientific advancement, which bibliometricians generally approximate with citations, or a weighted combination of citations and journal impact factor for short citation time windows ( Furthermore, research projects frequently involve a team of scientists, which is registered in the co-authorship of publications. In this case, we account for the fractional contributions of scientists to outputs, which is sometimes further signaled by the position of the authors in the list of authors. Depending on the objectives of an assessment exercise, it might be appropriate or not accounting for the costs of research (input). It is appropriate, when the objective is to reward best performers (i.e. those who produced more, all resources being equal), or to decide where to allocate funds to maximize returns. It is not appropriate, when the objective is to identify for example the most knowledgeable experts in a field. It is known that accounting for the different costs of capital and labor is a formidable task, because of lack of data at the individual level or, where available, of the assumptions and approximations required. When comparing productivity at the individual level, we proceed in two ways: one way neglects costs, and the other accounts for them. At the aggregate level, we always account for inputs/costs. If we neglect costs, the yearly productivity of a professor, termed fractional scientific strength (      The fractional contribution equals the inverse of the number of authors in those fields where the practice is to place the authors in simple alphabetical order but assumes different weights in other cases. For Biology, Biomedical research and Clinical medicine, widespread practice in both Italy and Norway is for the authors to indicate the various contributions to the published research by the order of the names in the bylines. For the above disciplines, we thus give different weights to each co-author according to their position in the list of authors and the character of the co-authorship (intra-mural or extra-mural). We calculate the productivity of each professor in each SC and express it: i) on a percentile scale of 0–100 (worst to best) for comparison with the performance of all professors under observation of the same SC; and ii) as the ratio to the average productivity of all professors of the same SC with productivity above zero. In the case it is appropriate to account for the cost of inputs, when comparing individuals’ productivity or measuring productivity at the aggregate level (SC, discipline, department, school, university), one should account for the different cost of labor and capital available for research, as research units are likely to embed professors with different ranks/salaries and provide them with different resources. Failure to account for the different production factors would result in fact in performance ranking distortions. The information on individual salaries is unavailable in Italy but the salaries ranges for rank and seniority are published. As said above, in the Italian university system, salaries are established at the national level and fixed by academic rank and seniority. Thus all professors of the same academic rank and seniority receive the same salary, regardless of their merit and the university that employs them. Also in Norway, the academic rank is the primary salary determinant. Therefore, we do not account for differences in salaries across countries, because a given salary can only “buy” a professor, regardless of her or his merit and country. Instead, we need to account for differences in salaries across academic ranks, because higher salaries can buy higher academic ranks, and it has been shown that full professors (more costly to society) are on average more productive than associate professors, and these more productive than assistant professors ( As for the cost of capital devoted to research per man/year, aggregated data at discipline level are available in Norway. In the absence of specific individual level data, we assume that each professor devotes the same amount of time (50 per cent) to institutional activities other than research (i.e. teaching, technology transfer, administration, etc.), and can count on the same resources, regardless of academic rank, university, and country. When accounting for costs of labor and capital, assuming that labor and capital concur equally in determining research output, the        We halve labor costs, because we have assumed that 50 per cent of professors’ time is allocated to activities other than research. As for the cost of labor, As for the cost of capital, As we said, the productivity of units that are heterogeneous for fields of research of their staff cannot be directly measured at the aggregate level. A two-step procedure is required: first measuring the productivity of the individual professors in their field (formula 2), and then normalizing individual productivity by the average in the field. At the aggregate level then, the yearly productivity    In this section, we present the results of the performance analyses in the two countries. We start with the individual level comparisons, both accounting and not accounting for costs. We then proceed with the aggregate level analyses, accounting for costs. We have measured the research performance of each professor in their relevant SC. As an example,  We further delve into the top decile performers at SC and discipline level, and measure in each SC, for each country, the total number of professors whose percentile performance is 99 or more (TS_1%) The As said above, at the aggregate level we always account for costs. In the following, we provide examples of results at the level of overall institutions, disciplines, and SCs. As noted in the Methods section, the dataset consists of professors affiliated with 93 Italian and 6 Norwegian universities (including one for other HE-institutions). The performance of these institutions varies significantly overall and across fields.     The analyses shown in Finally, as an example of institution ranking at SC level, The main aim of this study was to verify the feasibility of applying a research performance (output-to-input) indicator, namely the FSS ( The main contribution of the study, in view of hopeful future extensions to other countries, is the methodology developed to overcome the limitation and comparability of input data, and differences in professor field classification schemes. All solutions and assumptions adopted respond to the criterion of making performance comparisons as equitable as possible. The availability of the cost of capital per Norwegian professor made it possible to improve the measurement of individual productivity, that has been so far limited to accounting for the cost of labor only ( One interesting finding of this comparison is that overall there are hardly any differences at all in the average research productivity of Italian and Norwegian professors. Thus, despite representing countries which differ considerably along such dimensions as size, research profile and organisation of the research systems, the overall performance measured by the Especially for small-sized countries in research terms, the importance of comparing performance with large-sized countries is twofold. On the one hand, the comparison allows a strategic analysis of strengths and weaknesses at field and discipline levels, as compared to other countries, informing then research policies, organizational strategies, fund allocation, incentive systems, etc. In the case in point, results show the notable productivity advantage of Norway in Mathematics and Earth and Space Sciences, and that of Italy in Biomedical Research and Engineering. At field level, Norway notably outperforms Italy in Mathematics, interdisciplinary applications; Remote sensing; and Statistics & probability. The opposite is true in Information science & library science; Computer science, interdisciplinary applications; and Biodiversity conservation. On the other hand, the comparison with a larger country contributes to make the assessment more robust in those fields where the number of professors reveals too low, which is frequently the case in small-sized countries. In interpreting the results of the performance analysis, it should be kept in mind that, in addition to the above said assumptions embedded in the FSS measurement, all the usual limits, caveats, assumptions and qualifications of evaluative scientometrics apply, in particular: i) publications are not representative of all knowledge produced; ii) bibliometric repertories applied (WoS) do not cover all publications; and iii) citations are not always certification of real use and representative of all use. Furthermore, results are sensitive to the classification schemes adopted for both publications and professors. Finally, the limited availability of comparable input data required the adoption of few assumptions. Subject to the availability of input data, the present study can be replicated to include other countries or single research institutions of other countries. Comparative research assessment data provides governments, universities, industry and prospective students with valuable information about research performance in a country’s higher education institutions. A rigorous and fine-grained, disciplined-based information about research performance that is not readily available through other means. Such data allows research managers and investors to identify and assess performance in research and opportunities for further development; assists institutions with their strategic planning, decision making and their research promotional activities in the country and internationally; helps reduce the information asymmetry between research suppliers and demand, i.e. students, industry and other stakeholders, making it possible to optimize their choices and inducing a continuous improvement on the supply side to be “chosen”. Moreover, monitoring research performance and using results, assures taxpayers that their investment in research is well spent. The results of the current comparison offer a number of stimuli to further delve into the performance analyses. It emerged in fact a different distribution of individual performance between the two countries. Future research might investigate the dispersion of research performance within and between universities. The comparison could reveal different competitive intensity in the two higher education systems. Furthermore, the single components of the FSS indicator could be compared, namely the intensity of publication, the average impact per publication, and the collaboration behaviour. The two countries present also a quite different history of female emancipation, and attitudes concerning the role of women in the family, in the workplace, and in the society in general. It would be interesting then to explore whether gender representation across academic ranks, and differences in performance occur to the same extent in the two countries. Giovanni Abramo: Conceived and designed the analysis, Collected the data, Contributed data or analysis tools, Performed the analysis, Wrote the paper Dag W. Aksnes: Conceived and designed the analysis, Collected the data, Contributed data or analysis tools, Performed the analysis, Wrote the paper Ciriaco Andrea D’Angelo: Conceived and designed the analysis, Collected the data, Contributed data or analysis tools, Performed the analysis, Wrote the paper We wish to thank Gunnar Sivertsen and Lin Zhang for being the catalysts of this work. Supplementary material related to this article can be found, in the online version, at doi: The following is Supplementary data to this article: