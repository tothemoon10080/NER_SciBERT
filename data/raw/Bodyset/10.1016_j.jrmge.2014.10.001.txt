The combined finite–discrete element method (FDEM) belongs to a family of methods of computational mechanics of discontinua. The method is suitable for problems of discontinua, where particles are deformable and can fracture or fragment. The applications of FDEM have spread over a number of disciplines including rock mechanics, where problems like mining, mineral processing or rock blasting can be solved by employing FDEM. In this work, a novel approach for the parallelization of two-dimensional (2D) FDEM aiming at clusters and desktop computers is developed. Dynamic domain decomposition based parallelization solvers covering all aspects of FDEM have been developed. These have been implemented into the open source Y2D software package and have been tested on a PC cluster. The overall performance and scalability of the parallel code have been studied using numerical examples. The results obtained confirm the suitability of the parallel implementation for solving large scale problems.The combined finite–discrete element method (FDEM), pioneered by The limitation of FDEM is that it is CPU-intensive and, as a consequence, is difficult to analyze large scale problems on sequential CPU hardware. Thus, the use of high-performance parallel computers is indispensable. Parallelization strategies for both finite element method (FEM) ( The parallelization of FDEM itself is somewhat less explored. A computational procedure for finite–discrete element simulation on shared-memory parallel computers was presented by Some general strategies for parallelization of FDEM are described by In recent years, the use of graphics processing units (GPUs) is also being explored for both FEM and DEM. The GPU implementation of FDEM was presented by The parallelization strategy described in this paper aims at performing all tasks (domain decomposition, LB) concurrently on all processors. As such, the authors hope that it will provide some additional contribution to the development in the field. In the rest of the paper, the detailed description of the proposed two-dimensional (2D) parallelization solutions is provided together with numerical results. FDEM couples DEM and FEM by generating a finite element mesh separately for each particle (discrete element) located within the computational domain. Thus, finite element based analysis of continua is combined with discrete element based transient dynamics, CD and contact interaction solutions. The equation of motion is solved for each element separately. The governing equation is The joint element acts as a bond between edges of two triangular elements. The bond is a representation of a fracture mechanism termed as a “combined single and smeared crack model” developed by An explicit time integration scheme based on a central difference method is employed to solve the equation of motion. The following calculations must be performed in each time step: Evaluation of internal forces based on deformation of particles. Evaluation of joint forces based on the deformation of joint elements. Fracture of joints. The CD. Contact interaction (evaluation of contact forces). Application of external forces. Solution of the equation of motion for each discrete element separately. It should be noted that even though the CD could be done in each time step, it would be very expensive. Thus, a buffer controlling the CD frequency is introduced and the CD is done only if the maximum traveled distance exceeds the size of this buffer. The CD is the process of finding all contacting couples, i.e. all pairs of interacting discrete elements (DEs). The computational domain is overlaid by a CD grid of a chosen cell size ( The CD algorithms developed for FDEM include Munjiza-NBS algorithm ( Contact interaction is the mathematical model to compute the penetration of a DE into other DE. The contact force evaluation from the calculated penetration is based on a penalty function method ( The FDEM code Y2D was originally written to illustrate concepts described in FDEM book written by It is out of the scope of this paper to provide detailed description of the above principles. The details on each can be found in the FDEM book ( Parallelization strategies usually attempt to divide the large problem (computational domain) into a number of smaller sub-problems (sub-domains). Domain decomposition is one way to accomplish this. A good parallel implementation must meet two, often competing, requirements; each processor must be kept busy doing useful work and communication between processors must be kept to a minimum. For typical problems in FDEM, the first requirement can be achieved only by employing dynamic domain decomposition and LB since objects (discrete and finite elements) migrate from one sub-domain to another, thus creating a workload imbalance. Partitioning methods used for domain decomposition can be either geometric or topological ( One of the most commonly used topological methods is a partitioner termed METIS ( To demonstrate the parallelization strategy used, the computational domain is discretized into a rectangular grid of sub-domains by using a modified RCB algorithm ( In order to adopt the chosen partitioning algorithm, each sub-domain is confined to a rectangular shape. A buffer-zone is introduced around the borders of each sub-domain (see A bigger size of the buffer-zone means a higher number of elements shared among processors. This increases the communication overhead within each time step (updating nodal forces), but the migration, which is a very expensive operation, is performed less often. Thus, for a highly dynamic example, the size of the buffer-zone can be set bigger while for a quasi-static problem it can be set smaller. Elements are classified into several categories (statuses) depending on their location within the sub-domain. This is based on a modified approach of Joint elements are not classified by their location in the sub-domain but by the combination of triangular elements (A-A, A-B, A-C3, etc.) to which they are attached. For instance, a combination B-C3/C4 gives a status of joint element as B ( Computational domain is partitioned by a modified RCB algorithm ( For instance, partitioning for 16 processors ( Nodal forces are a summation of internal forces, joint forces and contact forces. Internal forces (triangular elements) and joint forces are calculated for both internal and interfacial elements, but any force of an interfacial element is divided by 2, 3 or 4 depending on the number of processors sharing the element. Contact forces between triangular elements are divided by a number depending on the classification of those elements. For instance, an internal element in contact with any interfacial element produces a unique contact force and therefore this force is divided by 1.0. If two interfacial elements (B-C4) are in contact, the contact force is calculated on only two out of four processors and must be therefore divided by 2.0. In general, the contact forces between interfacial elements must be divided by the lower number of processors derived from the classification of both elements. Two exceptions to this rule exist. Firstly, if one interfacial element B is located at a horizontal border and the second interfacial element B at a vertical border, the contact force is unique because corresponding counterparts of these two elements are located on two different processors ( When a calculation of nodal forces for all elements is finished, nodal forces of interfacial elements are exchanged between corresponding processors in each time step. It follows from the rules outlined in the nodal and contact forces section that any sequential CD algorithm can be used directly in parallel implementation of the code without any modification. Instead of performing a global contact search and parallelizing it, the CD is performed locally on each sub-domain independently from other sub-domains. The advantage of this approach is a simplification of parallel programming. Singly connected lists of interfacial and internal elements located in the proximity of each border/corner are assembled during the CD ( As mentioned above, elements are migrating from one sub-domain to another. Since the buffer zone (calculated from the CD buffer) is introduced around borders of the sub-domain ( When forces are exchanged, the equation of motion (Eq. Singly connected lists assembled during the CD ( Elements which leave the sub-domain must be deleted from the database and new elements that moved into the sub-domain from neighboring processors are added to the database. It is therefore necessary to perform a new CD search in the next time step. It is worth noting that the CD is not done from scratch but singly connected lists of contacting couples are only updated taking into account deleted/received elements. All main communications (nodal force exchange, migration of elements, redistribution of elements during the LB) are performed in two separate stages. Horizontal messages (assembled for right and left borders) are exchanged in the first stage. In the second stage, messages assembled for top and bottom borders are exchanged vertically. Information for a neighboring diagonal processor (if needed) is first sent in the horizontal message and then sent again from the receiving processor in the vertical message. Hence there is no communication diagonally. Both horizontal and vertical communications are divided into two time slots. In the first time slot, messages are exchanged between processors in columns/rows 0–1, 2–3, 4–5, etc., and in the second time slot between columns/rows 1–2, 3–4, etc. The horizontal communication in each time slot is done in several steps to avoid interlocking ( It should be noted that the communication described above employs a so-called blocking communication. If the computation on the receiving processor is completed faster than the computation on the sending processor, the receiving processor must wait for the sending processor to send the message. This provides the user with means to synchronize the execution of the program on different processors. As a consequence, overlapping communication and computation are not possible. Each sub-domain is assigned to a single processor in the PC cluster. Elements migrate between processors (size of sub-domain does not change) until an imbalance in the workload exceeds a value specified in the input file. Then the re-partitioning (size of each sub-domain is updated) and LB is performed. This is done in the following steps.  The cell size of the CD grid is given by the maximum element diameter  Calculation of sums of each column in the LB grid. Partitioning in Calculation of sums of each row in the LB grid for each column separately. Partitioning each column in Partitioning to four processors is illustrated by an example in  It is worth noting that borders are moving in increments. The size of increment is equal to the cell size of the LB grid Parallel code was tested on a PC cluster with 3592 nodes. Each node contains two 8-core 2.60 GHz Intel Xeon E5-2670 CPUs and 32 GB DDR3 1600 MHz RAM memory. To illustrate the LB and re-partitioning procedures, a box filled by 32,400 discrete elements, each comprising 6 finite elements, with initial velocity 100 m/s in diagonal direction, was tested on up to 32 processors. The box is fixed in both Recorded CPU time and calculated speedup are summarized in This performance test can be considered a worst case scenario from the LB point of view since all discrete elements are moving in the same direction across the box. Moreover, the number of contact interactions, which is very expensive in terms of CPU time, is limited for the majority of the simulation time. Therefore, the performance is dominated by a communication overhead, caused mainly by element migration and also by redistribution of elements during the LB. This is especially true for 2 processors since the size of messages (number of elements located at the border between processors) is quite big. The performance improves with higher number of processors ( The Barre Granite Brazilian disc test is numerically simulated on up to 32 processors. The Barre Granite is a heterogeneous rock comprising approximately 24% quartz, 68% feldspar and 8% biotite ( Recorded simulation time for different numbers of processors and calculated speedup are summarized in Stress The fracture patterns obtained for different numbers of processors show a good correspondence. The small differences are caused by the presence of rounding errors. Rounding errors are introduced due to the limited amount of memory available for storing real numbers ( The simple Coulomb friction model implemented in Y2D is not suitable for quasi-static problems (Brazilian disc). A version of the Y code named Y-Geo ( A dynamic domain decomposition parallelization strategy for FDEM has been presented in this work. Performance tests of the current parallel implementation confirm suitability of this approach for parallelizing FDEM. The speedup calculated for Brazilian disc test simulation scales well with an increasing number of processors. Decreasing performance with an increasing number of processors can be observed as the ratio of computation to communication decreases. Thus performance is expected to improve with increasing problem sizes. The speedup calculated for a box filled by discrete elements is scaling linearly and is approximately equal to half the number of processors used since the simulation time is dominated by the communication cost. This example shows that the communication cost scales with a higher number of processors. The performance tests show the suitability of the communication pattern proposed as the number of horizontal communication steps increases moderately while the vertical communication steps remain constant and are independent of an increasing number of processors. The current version of the parallel implementation employs a blocking communication. Further improvement of the performance should be achieved by employing non-blocking communication, thus performing communication concurrently with computations. The actual implementation of the parallelization is available in Y2D open source format. The authors wish to confirm that there are no known conflicts of interest associated with this publication and there has been no significant financial support for this work that could have influenced its outcome. The authors would like to express their thanks to Mohammad Saadatfar from Australian National University for the access to NCI's Raijin supercomputer.