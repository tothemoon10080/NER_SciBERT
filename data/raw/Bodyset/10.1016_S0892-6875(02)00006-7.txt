An automation system of an industrial process records hundreds of signals. Simultaneous study of all these signals is not possible: the analyst must focus onto the essential â€“ find the variables of interest. The method presented in this paper supports the search. The user defines what is interesting by labeling the data points, for example, using `A' to mark the data points from a period of a given process control problem, and `N' to mark a period of normal operation. The method of this paper uses a cluster separability measure and an optimization algorithm to find the set of variables that best explain the differences between these two states. The method is illustrated with the case of the concentrator of the Outokumpu Hitura mine, where the changes in the mineralogy of the concentrator feed require major changes in the process control strategy. In Hitura the problem is that the process operator has no direct measurement of the mineralogy of the feed. The case of this paper is about finding the on-line variables that contain information of the feed type.The study of a recurring process disturbance is a common task of a research engineer of a plant. The natural starting point is to study data recorded by the automation system. The problem is that there are hundreds of signals. Only few of them can support the study, the others lead the analysis astray. The current practice of variable selection is to browse the signals manually one at a time, and look for correlations with the process control problem. This takes time. Also, if two or more signals should be studied together, for example, the measured pH together with the flow of acid, the engineer looking only at one variable at a time may fail to find the interesting combination. Standard statistical analysis methods can efficiently browse through variables and reveal multivariate correlations. All the above methods look for linear correlations between the variables. Their strength is simplicity and computational efficiency. However, process data are always more or less non-linear. A simple non-linear method is cluster analysis. It is used to study whether the points of data form separable clouds; see The variable selection technique of this paper is based on finding the set of variables that portray the clusters as compact and separate as possible. Before this can be done with process data, the clusters must be defined. In the method of this paper, the user defines the clusters by labeling the data points: each cluster comprises the points with a given label. In labeling the user defines what is interesting. For example, in The variable selection technique makes a number of applications possible. For example, the analyst can look for the variables that best explain variance in process control success ( The body of this paper is divided into four sections. The first presents a measure for cluster separability. The next shows how this measure is used to find the variables of interest. The third presents the case of the Outokumpu Hitura mine. And the fourth part compares the method of this paper to the standard methods of literature. A reader with limited interest may only want to read the presentation of the Hitura case. The method of this paper looks for variables that portray the clusters as compact and separate. This is illustrated by the synthetic data of This above visual study can be automated using optimization. Optimization requires a cost function, a mathematical measure of the separability. This paper proposes Both the numerator and denominator of The optimization has to compare spaces of dissimilar dimensionality, for example, to decide whether separability is better using the Finally, two possible modifications to The previous section studied one part of the variable selection methodology. This section presents the complete view: how to label the data, prune off-layers, use optimization, and study the result. The first and most important task of the user is to label the data points. By labels, the user defines, what is interesting. The analyst can resort to his/her own knowledge or ask the process the experts for time periods that manifest the phenomenon of interest in the process. Or the user can resort to an external data set, for example, laboratory data, to identify the process state. In the Hitura case, the daily laboratory analysis of the concentrator feed provided the feed type labels for the on-line data points. The second task for the user is to scale the variables of the data. The variable selection algorithm of this paper, along with most of the tools of statistical data analysis, is sensitive to scaling. In the Hitura case, the average deviation of each variable was set to one. This differs from the commonly used scaling method that scales the variances to one. The variance approach was not used because it is sensitive to off-layers, which are data points far away from the majority of other points. Such data points do not usually carry process information, but are caused, for example, by faulty instrumentation. To illustrate the effect of an off-layer, consider a one-dimensional data set with a hundred data points evenly spaced between 0 and 1. Their variance is 0.085, and average deviation 0.25. Now, add a point to the position 10. The new variance is 0.96, and average deviation 0.35. Note that the variance increased with a factor of 11.3, while the average deviation only increased with a factor of 1.38. The variance increased more, because it is calculated with the squares of distances: the off-layer became dominant. Any algorithm assuming that the data have Gaussian distribution will have this problem. If the user wishes to use such tools, emphasis should be put to the removal of off-layers. The tools of Statistical Process Control (SPC) can support this task; see The labeled and scaled data are fed to the variable selection algorithm. The algorithm runs an optimization to find the variables that span a space in which Now, the user can study the space spanned by the selected variables. If the number of variables is low, the data can be plotted and studied directly. High dimensional spaces can be studied with dimension reduction algorithms, for example, the Self-Organizing Map (SOM) by Last, a modification of the optimization scheme is proposed. The user may want to decide how many variables should be selected. For example, the user may want only two variables, as they are easy to study using the scatter-plot. Or the user may want to gradually increase the number of returned variables, and study how variables appear and disappear from the optimal set. The original optimization scheme and this modified version are both used in the Hitura case presented. The previous two sections studied theory, this section studies practice: the case of the concentrator of the Outokumpu Hitura mine. The task is to find the on-line measurements that contain information of the mineralogy of the concentrator feed. Such measures could be the foundation of an expert system supporting the decisions of the process operators ( According to These changes disturb the process. For example, the optimal frother flow may change from 120 to 10 ml/min. However, the operator has no on-line estimator of the feed type, he/she must experiment with the frother flow and other control parameters to find the optimal treatment. During this experimentation, valuables are lost; and the found state may not be optimal. As a solution, As described in the previous section, the first task is to label the data points. In the Hitura case, the labels describe the concentrator feed type. The plant laboratory measures the feed type by the variables of On-line data were collected. Most of the variables are basic automation signals such as flow and level measurements. An important addition comes from the Courier 30 (C30) analyzer that uses X-ray fluorescence to measures element contents on-line from the process streams. The total number of variables was 55. Then the time period was selected to match the 457 off-line data points. The off-line based daily feed type analysis is used to label the respective on-line data points. The variable selection algorithm returned 19 on-line variables that best separate the feed type clusters. The separability value, measured by Then, the algorithm was run to return a given number of variables. The best single variable is the C30 copper channel intensity of the tailings (JICU). This is reasonable as the converted copper mineral of Hitura floats differently from the unconverted one. The separability of the clusters measured by The best three variable set is the consumption of sulphuric acid (HAPPO), describing the silicate phase by indicating the amount of serpentine; the second variable is the C30 copper intensity in the feed (MICU), describing the sulphide phase; the third variable is niobium intensity for the concentrate (RINB), describing the feed with a mechanism unknown to the author of this paper. Next, the 19-dimensional space returned by the unbound optimization is studied. The Euclidean distances between cluster centroids are presented in The plant personnel assessed the found set of 19 variables meaningful. Most of the 19 variables were already assessed interestingly in the manual study performed by This section compares the above methodology with the techniques of the literature. The first studied technique is clustering, used to reveal any natural groupings among data points. The clustering algorithm is inputted a matrix of numbers; the algorithm outputs labels to these data points. The labels partition the input data space so that neighboring points are often assigned the same label, that is, assigned to the same cluster. For example, in the Hitura case, laboratory data were clustered to acquire the feed type labels. Some clustering algorithms use a distance measure and optimization to assign the labels so that distances within a cluster are small and distances between cluster centroids are large. Such distance measures are conceptually close to the measure of The Euclidean measure calculates the distances between the cluster means, in a same way that the distance would be calculated between two city centers. This measure is robust to off-layers: as the centroid is calculated by the mean, an off-layer does not change the cluster centroids much; calculation by variance would be more sensitive to off-layers. The Euclidean measure is fast and robust to calculate, no complex matrix algebra is required. The measure has two disadvantages. The Euclidean distance between two clusters will always increase or remain the same with any added variable. In the optimization scheme of this paper, any new variable would be admitted to the optimal set. Furthermore, the Euclidean distance does not study the possible overlapping of the clusters, only the distance between the cluster centroids. A more powerful measure is the Mahalanobis distance; for example, see The DB-index presented by From the scope of this paper, classification seems an interesting field. A classification algorithm creates a model that can separate the clusters. Usually, the created model is of interest to the user: it can be used to give an identity, a label, to a new data point. This paper considers the accuracy of classification more interesting: high accuracy implies that the clusters are separable; for example, see The last of the studied separability measures is entropy, theoretically the most elegant one. Entropy is minimized when the data are well organized, that is, the clusters are separate and compact. The above five measures could have been used to replace The PLS creates a model between the variables of two spaces. Prior to the model creation, both spaces are reduced with a PCA-like technique. This reduces the number of variables and allows the creation of a simpler model. For an introduction, see the tutorial by The Analysis of Variance (ANOVA) and its multidimensional extension MANOVA study whether a variable or a set of variables can separate two clusters. For introduction, see In the study of hundreds of variables of process data, the user must concentrate on the essential. Only the essential information promotes learning; other information will distract. This paper showed how to find the variables of interest. The user defines labels to the data points to define what is interesting. The tool of this paper finds the set of variables that best separate the clusters defined by these labels. Variable selection can support a number of practical applications. This paper presented how the on-line variables containing interesting off-line information can be found. Another application presented by The future research will branch into three directions. The first is the implementation of this system onto a process control platform to be used in practice. The second is to study whether the use of classification accuracy is feasible. And the third is to study how a continuous variables could be used to define what is interesting, instead of the nominal information of the labels used in the approach of this paper.