With increased emphasis on plant automation and the implementation of advanced process control systems in the mineral processing industries, the need for accurate process models has become greater than ever. These models are often developed or identified from historic process data, as the process systems may be too complex to model from first principles. Under these circumstances, identification can be a challenging task and in the process industries the problem is complicated considerably by the presence of noise from various sources, nonstationarity of the data and intermittence, such as observed in particulate flows. To complicate matters, it may not be possible to build satisfactory models with the aid of traditional methods, such as frequency analysis and linear modelling either. Similar problems arise with the application of nonlinear theory developed over the last few decades, where much of the analysis depends on embedding of the data in a phase space or pseudophase space, since these methods were not originally designed to deal with noisy systems.
                  In contrast, most of these disadvantages can be surmounted by use of singular spectrum analysis. It allows the time series to be decomposed into different components, e.g. the underlying signal itself, as well as various noise components, which can subsequently be removed from the data. As is shown in this paper, removal of the minor components of the data can lead to significant improvement in the identification of the system. Four strategies are considered, viz. system identification with models fitted to the original data, models fitted to the smoothed data, an assembly of models fitted to the components of the time series and smoothing of the data based on multivariate embedding of time series.The modelling of process systems has attracted considerable interest in industry and academia, owing to increased emphasis on the automation of plant operations, which are driven in turn by environmental constraints, safety legislation and competitive pressures. Since it is often not feasible to model complex systems first principles, models are usually constructed from process data. Even so, identification of nonlinear systems can be a daunting task and in the process industries the problem is complicated considerably by the presence of noise from various sources, nonstationarity of the data and intermittence, such as observed in particulate flows. Traditional methods, such as frequency analysis and linear modelling do not handle these systems well. Similar problems arise with the application of nonlinear theory developed over the last few decades, where much of the analysis depends on embedding of the data in a phase space or pseudophase space, since these methods were not originally designed to deal with noisy systems. In contrast, singular spectrum analysis does not suffer from these drawbacks. It allows the time series to be decomposed into different components, e.g. the underlying signal itself, as well as various noise components, which can subsequently be removed from the data to enable better identification of the system. In this paper, this powerful and versatile method is briefly explained and strategies towards system identification with singular spectrum analysis are proposed. With these strategies the data are smoothed prior to fitting models, which can lead to a significant improvement over models built on the original data. The spectral decomposition of matrices has only recently been applied to time series analysis ( As indicated in   The aim of the grouping step is to separate the additive components of the time series. In this situation, it can be seen as separating the time series into two groups, viz. the original signal and the noise component. The criteria for this separation are once again a matter that has not been formalized completely and it depends on knowledge of the data. Once the principal components to be retained have been identified, a new time series with a reduced number of principal components can be reconstructed. This is done in the fourth step, diagonal averaging. The calculations involve taking the average of the matrix elements on each ‘diagonal’ of the matrix of the retained principal components. If the following matrix were obtained as the additive result of the main principal components, the averages of the diagonals can be obtained as indicated. Generally, in applications in process engineering, one is presented with series of measurements on a set of variables, rather than just a single variable. Multivariate or multichannel SSA as it is sometimes referred to, is a natural extension of the approach discussed above for a single time series ( Finally, in order to identify the underlying process dynamics represented by the time series, a predictive model has to be constructed. Different classes of models can be used to fit the data, whether local or global. In this paper, neural networks will be used to identify the process systems. A neural network model typically consists of an input layer, which is presented with variables derived from the embedded time series, a hidden layer with a specified number of nodes and an output layer, which generates predicted values for the time series. Care should be taken in determining the number of hidden nodes, as the more nodes there are, the more accurately the data can be modelled, but the less general the model will be. The modified Schwartz information criterion ( The strategies for the identification of process systems can be summarized as follows:  Given a set of time series Compute the lagged covariance matrices of Reduce the dimensionality of the embedding if possible by retaining Reconstruct the original time series from  See step i above. Assemble the trajectory matrices of the individual time series, Compute the lagged covariance matrix of Remove noise from the data by approximating Separate Reconstruct the original time series from the appropriate parts of The following four modelling strategies were considered. r-step ahead prediction of the original time series based on embedding of the original time series r-step ahead prediction of the original time series based on a model of the reconstructed time series r-step ahead prediction of the original time series based on an ensemble of models of the individual Similar to strategy B, except that it is based on the multivariate embedding ( Strategies A–C are depicted diagrammatically in To illustrate the use of singular spectrum analysis, consider the second order response of the flow of two noninteracting tanks in series, the overall transfer functions of which can be described by The autocorrelation function of the time series reached the point of weak to negligible correlation (<0.2) at an index of 23, hence the trajectory matrix consisted of 23 columns of the time series, each copy delayed by a time step of one. This matrix formed the basis from which 23 principal components were extracted. The eigenvalues associated with each of the 23 principal components (eigenspectrum) are shown in The results obtained by fitting models according to strategies A–C discussed above are shown in Frothing is a common phenomenon in mineral engineering operations and in flotation especially, it is of fundamental importance to the efficiency of grades and recoveries. In the last five years, considerable progress has been made concerning the use of control systems based on direct monitoring of the froth. At present, state-of-the-art digital image processing systems are based on sophisticated algorithms for the measurement of bubble size distributions in the froth, the analysis of flow patterns in flotation cells, as well as measurement of the stability of the froth surface near the concentrate overflow. In addition, the presence of reagents or mineral species can also be related to the appearance of the froth. The data in this first example were obtained from a South African copper flotation plant. The plant consists of a crushing section and milling circuit, followed by a magnetic separation section. The purpose of the magnetic separation is to remove the high percentage of magnetic material in the ore and thereby reduce the load on the flotation circuit. The flotation circuit itself is designed to operate with feed grades of 0.6% Cu, 9.0% Pb, 2.4% Zn and 130 g/t silver. The circuit configuration consists of two conditioners, in which sulphurous acid, two copper collectors and a frother is added. From the two rougher banks the concentrate is circulated to the three cleaner banks, where zinc is depressed by acid in the first cleaner and lead depressed in the second and third cleaners by adding lime. The cleaner tails and the scavenger concentrate are returned to the copper feed of the flotation circuit, and these two streams make up the bulk of the feed. The time series for the rougher, cleaner and scavenger units consisted of 1234 observations each, obtained at 12-min intervals, as indicated in The sum of the statistical distributions of the differences in the grey-scale values of the pixels was used to represent this information, i.e. The optimal sizes of the embedding windows for each of the time series were determined by means of autocorrelation analysis as described earlier, i.e. the cleaner, rougher and scavenger time series were scaled to zero mean and unit variance and embedded in trajectory matrices (  During the analysis of the time series considered here, the eigenvalues were used as the main source of information on decomposition of the time series. A visual inspection of the eigenvalue distribution ( Multilayer perceptron neural network models were built for each of the three reconstructed time series, as well as the three original time series according to strategies A and B previously discussed. In addition, models were also fitted to the components of each time series (strategy C), as well as the time series reconstructed from the multivariate trajectory matrix of the system (strategy D). The single hidden layer neural neural networks with sigmoidal activation functions were automatically constructed based on a modified Schwarz information criterion and the Levenberg–Marquardt search algorithm. The quality of the models A–C was assessed by means of validation data sets not used during the development of the models. The output of each model was validated against data from the In the final example, a froth monitoring system on a lead flotation plant is considered. Images from the froths in a zinc rougher cell were captured and digitised, after which image features were extracted from the images. Five such features were extracted, viz. For a discrete time series of finite length, singular spectrum analysis makes use of the principal component decomposition of an estimate of the correlation matrix that is based on m lagged copies of the time series. The resultant eigenvectors form an optimal basis that is orthonormal at zero lag and permit the signal to be decomposed into its possibly oscillatory and aperiodic components. These SSA basis functions are determined by the data and presents an advantage over the classical Fourier basis method, since a nonlinear oscillation can be captured by a single pair of principal components (basis functions). As indicated by several case studies in this paper, considerably better predictive models could be developed by decomposing and reconstructing the process data with singular spectrum analysis, as this could lead to a significant reduction in the stochastic components of the observed data. In the first case study, strategies B and C performed better than strategy A, and similar to what the benchmark model, viz. the autocorrelation model at lag one, AC(1) or In the second case study, strategies B and C proved to be superior to strategy A on all the time series considered and slightly or significantly better than the benchmark model, AC(1). In this case strategy D, based on a multivariate embedding of the data, performed did not perform as well as strategies B and C. In theory it is supposed to exploit the redundancy in the data, but in practice the reduced presentation of the trajectory matrices led to a net loss in information, which impacted on the models. In the third case study, in the case of feature  The case studies considered in this paper demonstrate the potentially beneficial effect of smoothing of the data prior to system identification. Reconstruction of the data can lead to significant improvement in the model, since the ability of the model to generalize can be seen as being guided by the smoothed data. The real value of SSA is that it provides a principled approach to reconstruct (smooth) the data, even when the time series are short and noisy. These strategies for system identification (including automatic construction of the neural network models) proposed in this paper have been formalized to expedite automation and by using simple default values, they can be implemented without any user input, apart from specification of the data (time series) on which the model(s) should be fitted. Such a system is currently being evaluated on a commercial process control platform.