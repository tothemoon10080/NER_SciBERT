Background
                  Traditionally, distal radius fractures (DRFs) have been described using eponyms, e.g. Colles, Smith, Barton, Chauffeur. During the last half of the 20th century several classification systems for DRF have emerged. We evaluated the inter- and intra-observer agreement of the AO/OTA, Frykman and Older classification systems.
               
                  Methods
                  Four observers, an intern, an orthopaedic registrar, an orthopaedic consultant and a radiology consultant, independently evaluated DRF radiograms and classified the fractures according to the AO/OTA, Frykman and Older classification systems. After an interval of 6 months, radiograms of 30 randomly chosen patients were re-evaluated by the same observers.
               
                  Results
                  Radiograms of 573 DRF patients were evaluated in the study. The inter-observer reliability of the AO/OTA fracture types (A, B and C) was ‘weak’ (kappa = 0.45). The agreement dropped to ‘minimal’ (kappa = 0.24) regarding the AO/OTA groups (A2, A3, B1, B2, B3, C1, C2 and C3). The reliability of the Frykman classification system was ‘weak’ (kappa = 0.41), and we observed the lowest inter-observer reliability for the Older classification system (kappa = 0.10). The kappa values for the intra-observer reproducibility of the AO/OTA fracture types (A, B and C) ranged from 0.58 to 0.87. For the AO/OTA groups (A2, A3, B1, B2, B3, C1, C2 and C3) the reproducibility was lower ranging from ‘minimal’ to ‘weak’. The intra-observer reproducibility of the Frykman system was ‘weak’ to ‘moderate’ and even worse for the Older classification system.
               
                  Conclusion
                  Based on these findings the AO/OTA classification system seems to be most reliable for routine use, however, with lower kappa values concerning the agreement for the groups. The Frykman and Older classification systems cannot be recommended because of less convincing results.Distal radius fractures (DRFs) are among the most common fractures in the Western world with a reported incidence rate of 190–200 per 100,000 person-years Traditionally, DRFs have been described using eponyms, e.g. Colles, Smith, Barton, Chauffeur. During the last half of the 20 Radiographic examinations of patients with DRF were retrospectively obtained from the electronic picture archiving and communication system at Aarhus University Hospital. Details regarding patient selection, inclusion and exclusion criteria and time to follow-up were previously described The standardized radiographic examination of the wrist consisted of anteroposterior and lateral projections. Four observers independently evaluated the radiograms and classified the fractures according to the AO/OTA, Frykman and Older classification systems. The four observers were an intern, an orthopaedic registrar, an orthopaedic consultant and a radiology consultant. A visual illustration together with a written explanation were available for each classification system throughout the assessment of the electronic radiograms ( The evaluations were collected on preformatted forms before transfer to our data analysis program. After six months the electronic radiograms of 30 randomly selected patients were re-evaluated by the same observers in order to estimate the intra-observer reproducibility. Cohen’s kappa was calculated using STATA. In the present study the interpretation of the kappa values, e.g. the level of agreement, is presented as suggested by McHugh Radiographs of 573 patients, who were operatively treated for DRF, were evaluated in the present study.  The inter-observer reliability of the AO/OTA fracture types (A, B and C) was ‘weak’ (kappa = 0.45), however, concerning the assessment of the AO/OTA groups (A2, A3, B1, B2, B3, C1, C2 and C3) the agreement dropped to ‘minimal’ (kappa = 0.24). The reliability of the Frykman classification system was ‘weak’ (kappa = 0.41). The lowest inter-observer reliability was observed for the Older classification system with a kappa value of 0.10.  The intra-observer reproducibility of the AO/OTA fracture types (A, B and C) was ‘moderate’ to ‘strong’ for the more experienced observers with kappa values of 0.74, 0.86, 0.87, while the intern reached only a ‘weak’ level of agreement (kappa = 0.58). For the AO/ OTA groups (A2, A3, B1, B2, B3, C1, C2 and C3) the level of agreement was lower ranging from ‘minimal’ to ‘weak’. The intra-observer reproducibility of the Frykman groups was ‘weak’ to ‘moderate’ with kappa values ranging from 0.46 to 0.63, and even lower for the Older classification system ranging from 0.10 to 0.21. To be clinically useful, a fracture classification system should have an acceptable inter-observer reliability and intra-observer reproducibility in order to guide the clinician to select the best treatment and to predict the prognosis. Furthermore, it should be easy to remember, comprehensive and simple to use Concerning the AO/OTA classification system we found a ‘weak’ inter-observer reliability (kappa = 0.45) for classification into the AO/OTA fracture types (A, B and C), but only a ‘minimal’ agreement (kappa = 0.24) for classification into the AO/OTA fracture groups (A2, A3, B1, B2, B3, C1, C2, C3), which is consistent with previous studies We have previously reported that the complication rate for AO/OTA type C fractures was significantly higher than for fracture types A and B Concerning the Frykman classification system, the inter-observer reliability was ‘weak’ (kappa = 0.41) in the present study. This is better than in previous studies, which have reported kappa values of approximately 0.2. In the present study, the inter-observer reliability of the Older classification system was ‘none’ (kappa = 0.10) and the intra-observer reproducibility was ‘none’ to ‘minimal’ (kappa <0.21), which makes this system unreliable. Our results regarding the Older classification system contradict the results of a previous study The fact that none of the evaluated classification systems are used routinely at our department could account for the relatively low inter-observer reliability observed in the present study. A second potential explanation is the fact that the four observers did not get any special training prior to the assessment and no consensus meeting was held. As expected, however, the intra-observer reproducibility was higher than the inter-observer reliability for all classification systems in the present study. Several other classification systems have been proposed, for example the Fernandez classification system Another, newly proposed, DRF classification system is the “IDEAL classification” In Denmark the current official indications for operative treatment of DRF include one or more of the following specific radiographic findings Dorsal tilt of the radius of more than 10° Ulnar variance of more than 2 Articular step off of more than 2 Incongruence of the distal radioulnar joint Gross instability, such as Smith-, Chauffeur-, Barton’s fracture or substantial dorsal comminution To the best of our knowledge, the reliability and reproducibility of any of these radiographic findings have not been systematically evaluated but it could have the potential to be more reliable than other classification systems. In conclusion, in the present study the intra- and inter-observer reliability was best for the AO/OTA types (A, B and C), but we cannot recommend using the Frykman and Older classification systems. The authors do not have any conflicts related to this study. The authors of this manuscript express their thanks to the Osteosynthesis and Trauma Care Foundation for the sponsorship of the publication of this Supplement in Injury.