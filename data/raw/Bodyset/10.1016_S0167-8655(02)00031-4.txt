Clustering algorithms require a large amount of computations of distances among patterns and centers of clusters. Hence, their complexity is dominated by the number of patterns. On the other hand, there is an explosive growth of business or scientific databases storing huge volumes of data. One of the main challenges of today's knowledge discovery systems is their ability to scale up to very large data sets. In this paper, we present a clustering methodology for scaling up any clustering algorithm. It is an iterative process that it is based on partitioning a sample of data into subsets. We, also, present extensive empirical tests that demonstrate the proposed methodology reduces the time complexity and at the same time may maintain the accuracy that would be achieved by a single clustering algorithm supplied with all the data.Clustering is a widely used technique, whose goal is to partition a set of patterns into disjoint and homogeneous clusters. Clustering algorithms have been widely studied in various fields including Machine Learning, Neural Networks and Statistics. They, also, have been utilized in many areas including data mining, engineering, taxonomy, statistical data analysis and business applications. Clustering algorithms can be classified as either partitional clustering or hierarchical clustering algorithms. Clustering algorithms require a large amount of computations of distances among patterns and centers of clusters. Hence, their complexity is dominated by the number of patterns. On the other hand, there is an explosive growth of business or scientific databases storing huge volumes of data. One of the main challenges of today's data mining systems is their ability to scale up to very large data sets. There is an increasing concern in clustering algorithms handling data that are substantially larger than available main memory on a single processor. There are several approaches to the problem, in the literature. An obvious approach is sampling of the data set (e.g., In this paper, we present a clustering methodology for scaling up any clustering algorithm. It is an iterative process that it is based on partitioning a sample of data into subsets. In a first phase, each subset is given as an input to a clustering algorithm. The partial results form a dataset that it is partitioned into clusters, the meta-clusters, during a second phase. Under certain circumstances, meta-clusters are considered as the final clusters. We, also, present extensive empirical tests that demonstrate the proposed methodology reduces the time complexity and at the same time may maintain the accuracy that would be achieved by a single clustering algorithm supplied with all the data. In the rest of the paper we first present the proposed methodology and then we present the experimental results. The paper ends with some concluding remarks. The concept of the proposed methodology, that we call the                      GOTO 1 Firstly we divide the dataset into In order to assess that similarity we merge all the centers of cluster sets (step 2) into a new table that we call If the algorithm succeeds for a significant percentage ( The process will surely succeed at some point and convergence will definitely occur as the subsets grow and become more similar each time. Even if Since the clustering of the subsets takes place simultaneously for all the subsets, we are decreasing significantly the execution time. The extra steps of the process, i.e., the creation and clustering of the meta-table, take an insignificant amount of time to complete, so the actual time for the whole process is the execution time for clustering a subset. Even if the process has to be repeated three or four times, which is rarely the case, the execution time gain is great. In order to evaluate the Proseggisis methodology, we have implemented a system, in Borland C++Builder ( We have used two different versions of the The first version, the BR-version, selects the first The second version, the FF-version, selects the initial modes calculating the frequencies of all categories for all attributes and assigning the most frequent categories equally to the initial The above variations have a great impact on the performance of the algorithms. The FF-version is a robust algorithm, in contrast to BR-version that has a non-deterministic behavior, although it has a reduced time complexity ( Empirical tests aim at evaluating the proposed methodology with respect to an analog non-distributed clustering process. Therefore, for each version of We applied the Proseggisis methodology to two datasets for four clusters, for each version and for the analog non-distributed clustering process. We tested several number of splits (5, 10 and 20 for the telecom dataset and 2, 5, and 10 for the car dataset), searching for four clusters ( In It, also, seems that the speedup is irrelevant of the size of the dataset, as it is shown in In In the first chart of The stopping criteria of the program depend on the values of The proposed Proseggisis methodology is a scalable framework for any clustering algorithm either partitional or hierarchical. To our knowledge, the proposed in the literature methodologies for scaling clustering algorithms to large databases accommodate specific clustering algorithm. Of course, this paper is focused on partitional clustering. However, to apply the Proseggisis methodology in the hierarchical clustering, the only need is to extract clusters from dendrograms, that are obtained from splits and from the meta-table, in a similar manner. This need is due to the fact that hierarchical clustering does not try to find “best” clusters. Some other scalable clustering schemes impose constraints on the input set of patterns. For instance, the CLARA algorithm presented in ( The BIRCH ( The methodologies which are based on parallelization are the most relevant to Proseggisis methodology algorithms we aware of. However, they require to transform a specific clustering algorithm to an optimized parallel one for a specific architecture. Of course, such parallel clustering algorithms achieve a great performance gain over serial ones. For instance, the parallel partitional clustering algorithm presented in ( The recent PDBSCAN algorithm presented in ( We presented the Proseggisis methodology, a novel methodology for distributing the clustering process. The key idea is like running the clustering algorithm Empirical results, also, demonstrate that the proposed methodology can significantly speed up the analog non-distributed clustering process. Notice that the proposed methodology does not require a substantial pre-processing phase and it does not suffer of any communication overhead. There are several variations of the proposed methodology regarding the implementation of the similarity and the stopping condition. For instance, following the idea of one of the referees of this paper, we have implemented a version of the Proseggisis methodology, where the meta-table is formed in a different manner (second step). In order to construct the meta-table, the centers of cluster sets, found in the first step, are weighted by the percentage of data points they cover in their cluster. We tested this version using the car dataset and the FF-version. We set Results are summarized in In general, although more sophisticated implementations may improve the performance, the basic strategy, presented in this paper, performs well on numerous different datasets. This work is supported by Enterprise Programmes for Research and Technology II, General Secretariat for Research and Technology, Hellenic Ministry of Development.