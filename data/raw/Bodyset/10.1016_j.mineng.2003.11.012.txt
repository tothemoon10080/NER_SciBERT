Mineral processing plants use two main processes; these are comminution and separation. The objective of the comminution process is to break complex particles consisting of numerous minerals into smaller simpler particles where individual particles consist primarily of only one mineral. The process in which the mineral composition distribution in particles changes due to breakage is called ‘liberation’. The purpose of separation is to separate particles consisting of valuable mineral from those containing nonvaluable mineral.
                  The energy required to break particles to fine sizes is expensive, and therefore the mineral processing engineer must design the circuit so that the breakage of liberated particles is reduced in favour of breaking composite particles.
                  In order to effectively optimize a circuit through simulation it is necessary to predict how the mineral composition distributions change due to comminution. Such a model is called a ‘liberation model for comminution’.
                  It was generally considered that such a model should incorporate information about the ore, such as the texture. However, the relationship between the feed and product particles can be estimated using a probability method, with the probability being defined as the probability that a feed particle of a particular composition and size will form a particular product particle of a particular size and composition. The model is based on maximizing the entropy of the probability subject to mass constraints and composition constraint. Not only does this methodology allow a liberation model to be developed for binary particles, but also for particles consisting of many minerals.
                  Results from applying the model to real plant ore are presented. A laboratory ball mill was used to break particles. The results from this experiment were used to estimate the kernel which represents the relationship between parent and progeny particles. A second feed, consisting primarily of heavy particles subsampled from the main ore was then ground through the same mill. The results from the first experiment were used to predict the product of the second experiment. The agreement between the predicted results and the actual results are very good. It is therefore recommended that more extensive validation is needed to fully evaluate the substance of the method.parent particle type classes progeny particle type classes progeny size classes frequency of parent particle types, frequency of progeny particle types the frequency of progeny particle types formed from the parent particle types the liberation component of the kernel, and represents the probability that the is the average composition of the is the average composition of the is the average composition of the frequency of progeny size-classes frequency of progeny size-classes resulting from parent particle types initial fitting prediction breakage assumed nonpreferential parameter to ensure that the sum of the parent/progeny joint frequency distribution for each parent particle type equals the frequency of the parent particle type parameter to ensure that the sum of the parent/progeny joint frequency distribution for each progeny particle type equals the frequency of the progeny particle type parameters to ensure that the average composition (for each mineral) of progeny particles formed from a particular parent particle type equals the average composition of the parent particles probability-entropy set of pairings of parents and progeny particles used for Markov-Chain Monte-Carlo weighting used to transform parent distributions moment degree When ore particles are large, they consist of mineral grains dispersed throughout the particles. By breaking the particles to, say, smaller than the initial average grain size, they consist of fewer grains, and in many cases they consist of just part of a single initial mineral grain. Thus breakage is a process to separate minerals from each other within particles. This process is called ‘liberation’, and ‘liberated’ particles are ones that consist of only of the mineral of interest. In contrast particles are called ‘barren’ if they do not consist of any mineral of interest. A good introductory text for mineral processing is by In order to increase liberation, particles need to be broken as fine as possible; however by decreasing particle size, energy costs substantially increase, and it becomes more difficult to separate particles. In order to understand and optimize a circuit then, it is necessary to model how the particle composition distribution will change due to breakage. The particle composition distribution is often referred to as a ‘liberation distribution’. In order to model changes to product composition distribution by changing the feed composition distribution, the most common approach is to estimate the ‘kernel’. The kernel represents the frequency (based on volume) that a particle of a particular composition and size will form a product particle of a particular size and composition due to breakage. Once this kernel has been determined, it can be applied to a new set of feed particles to predict the new composition distribution of progeny particles. In addition to modelling how the composition distribution varies, there is also the underlying problem of estimating how the size distribution changes due to comminution. Comminution models are well-known and commonly used ( The more difficult problem of estimating changes in composition distribution makes use of the available comminution models. Rather than use the terminology ‘feed’ and ‘product’, the terminology ‘parent’ and ‘progeny’ are often used (  The objective of the analysis of liberation during breakage is to determine the probability that each parent particles formed each progeny particles. Once this probability relationship has been determined, it can then be used for prediction should the feed particle composition distribution change. There are a number of ways this probability relationship can be determined. One method is to use textural modelling where mineral grain size distribution is inferred using mineralogical information. Such an approach is ‘mechanistic’ in the sense that the changes in composition distribution are a manifestation of real properties of the ore ( An alternative approach is to use a probabilistic approach. For this approach, one does not infer the actual texture information or mechanistic properties, but simply examines actual data to infer the properties of the kernel. The principle of maximum entropy is a probability-based technique for estimating distributions and a very well-written text is by This paper will explain how this approach is used for single-sized parent particles to multi-sized progeny particles. The method is explained for binary particles, multiphase particles and for incorporating preferential breakage. The method is then validated using real ore. To date, most research effort in liberation modelling has focused on considering breakage of binary particles focusing only on the mineral of interest. However in this paper the modelling is extended to multiphase particles (such as shown in There are a number of reasons for extending modelling to treat multiphase particles. An obvious reason is that particles are indeed multiphase, and by confining models to consider particles as binary, the actual particles are not correctly being represented. A second reason is that mineral association is important. Flotation behaviour is controlled not necessarily by the mineral of interest, but by all the minerals that exist on the surface of particles. Similarly for density separation of particles, the density of particles (which is related to the mineral composition and density of the minerals) must also be determined. A further reason is that particle breakage is not controlled by the ‘mineral of interest’ but by the presence of all minerals. Indeed for the ore studied (explained in more detail in In order to extend the modelling approach to multiphase particles a new way of modelling particles was conceived. For binary particles, particles are generally grouped into bins with each bin representing a composition class. The twelve conventional composition classes are [0%], (0–10%], (10–20%], An alternative approach was that used by This methodology is made more efficient by grouping together particles with vary similar composition properties, for instance grouping together all liberated particles. Hence modelling is applied to Rather than explain the approach for modelling liberation directly for the case of single-size to multi-sized particles with preferential breakage, the approach is explained one step at a time. The first explanation is for the schematic case of single-size to single-sized particles. The second explanation is for single-sized to multi-sized, assuming breakage if nonpreferential. Finally the approach is explained for preferential breakage. This paper is inherently mathematical and to help in understanding the notation, a list of symbols containing the major variables is contained in the Nomenclature. In this section the problem considered is how one would model liberation for single-sized parent particles forming single-sized product particles. The common method of representing the relationship between parent particles and progeny particles is to use, what is called a kernel. Denote Once the kernel has been determined, one can then predict the new progeny composition distribution should the parent composition distribution vary. The multiple of The entropy approach provides a mathematical method of solving problems which are ill-posed; in particular problems that have nonunique solutions. The entropy approach is based on the concept that, in the absence of other information, the probability distribution is at maximum randomness. The entropy is a measure of randomness, and to avoid confusion with the ‘entropy’ for physics will from now on be referred to as the probability-entropy. For the joint parent/progeny distribution, the probability-entropy with respect to the particle types ( Although the probability-entropy provides a method, one can also incorporate prior information to the problem. This is achieved by defining the entropy as In order to obtain a solution that is physically consistent, one needs to impose real practical constraints. There are only three sets of constraints that could be clearly identified. These are the original composition distribution of parent particles must be satisfied; the original composition distribution of progeny particles must be satisfied; when a parent particle forms progeny particles, the average mineral composition of the resulting progeny particles is the same as the parent particles. All these constraints can be considered as by volume. To express these constraints, the following notation is used:   These constraints are given by If progeny particles are different sizes, and breakage is nonpreferential, then the size distribution of progeny particles is independent of the parent particle composition; e.g. barren particles break to the same size distribution as liberated particles. Let With the problem now extended to multi-sizes, the analogous constraints to Eqs. The probability-entropy (analogous to Eq. As all variables are related to size-class (each having In simple terms, preferential breakage means that breakage occurs dependent on the mineral composition of the particles. Therefore Eqs. However Eqs. However, the approach of ignoring Eqs. The way to overcome this problem is to first model liberation assuming nonpreferential breakage, then use this model as a reference basis for modelling preferential breakage. That is, one would like to model breakage, whether breakage is preferential or nonpreferential, but allowing the useful constraints (Eqs. In order to model the first stage (treating breakage as nonpreferential) all the results must be consistent with the nonpreferential assumption. This means that the liberation data must be initially adjusted so as satisfy the nonpreferential breakage assumption. In order to find this distribution, denoted by In order to transform the data, the principle of maximum entropy may still be used. The objective is to maximize the probability-entropy given by Maximization of the probability-entropy (given by Eq. Once the data are adjusted, the kernel is estimated assuming nonpreferential breakage using Eq. The solution is The method of representing the particles here is based on a parent particle type to progeny particle type kernel. This is in contrast to the standard technique which is a ‘bin approach’. The bin approach represents particles in composition classes. For binary particles, the approach of using bins is computationally efficient. However it becomes inefficient as the number of minerals increases. Whether a bin method is used, or a particle representation is used (as in this paper), the three dimensional matrix A numerical method that greatly increases computational efficiency is called Markov-Chain Monte-Carlo (MCMC) with the particular algorithm being the Metropolis–Hastings algorithm ( The basis idea of MCMC is to find a representative sample for a specified distribution. In the case of the ‘liberation for comminution problem’ the kernel, Instead of using the Thus, there may be, say, 1000 pairings. Each pairing here is denoted by The distribution of Having obtained an initial estimate of the distribution, the Metropolis–Hastings algorithm is implemented in the following manner. The algorithm iterates by successively replacing old pointer values ( As a simple validation of this algorithm, consider the scenario in which there are only two points of a distribution, and they have frequency Thus, for given parameters defining a probability function, the function (here Having obtained a representative set of pairings, one can evaluate any error in the constraining equations. These errors are used to find better probability function parameters, and the process is repeated. The method of improving the probability function parameters is based on a Quasi-Newton method ( The methodology as explained thus far has three different ways of representing the kernel. These are: The next objective is to use the kernel (however it is represented) to predict how the product composition distribution changes due to the variation of the parent composition distribution. In order to consider prediction, a further representation is required. This representation requires separating the kernel into a ‘comminution’ component, and a ‘liberation’ component with the comminution component used for predicting comminution. The two main terms are: The various terms are related by If breakage is nonpreferential, then breakage is independent of particle composition, so the equation simplifies to In the next subsection, it is explained how to predict the product composition distribution is the parent composition distribution changes. In the subsection following that, it is explained how the product composition distribution changes due to changed comminution. The predictive method relies on relating the two feed particle composition distributions. There are then three particle composition distributions to consider. These are    The subscript The latter particle composition distribution, which is used to relate the initial and new particle composition distributions, is given by This covariance is given by The various moments are given by In order to determine the weights (denoted by The covariance is fundamental to modelling multiphase particle distributions ( Once the weights have been determined, these weights are applied to One of the important results of the nonpreferential breakage assumption is that size distribution prediction can be decoupled from liberation prediction. The comminution is modelled by predicting There appears to be no standard approach for modelling preferential breakage. Here the kernel obtained from the initial data is denoted as One approach to estimate One maximizes For this approach, one obtains the progeny composition distribution for each size-class (here denoted as An experiment was performed to test the probability-entropy model. A sample of single-sized particles (710–850 μm) was broken in a pilot plant ball mill. Another sample of these particles were separated according to density using a Wilfley Table into two sets of particles. One set was called the ‘heavy particles’, and the other set was called the ‘light particles’. The feed and product for the standard particles were used to determine the kernel, and this kernel was then applied to the heavy particles to predict the heavy product. The ore had a number of minerals, and these were classified into four groups: galena, sphalerite, quartz and ‘other’. The latter mineral ‘other’ consisted of all minerals not grouped into the first three. Galena is the primary valuable mineral. The quartz is the primary nonvaluable mineral and is much harder than both galena and sphalerite. Some data analysis was used to reconcile the data; in particular the liberation data were adjusted to be consistent with the chemical assays, and stereological correction was applied so that particle distributions were analysed rather than particle section distributions. The stereological correction used was the multiphase stereological correction developed by  The real test of the model is to test its predictive capabilities and this is achieved by applying the kernel to the feed of the heavy particles. The kernel was adjusted using the method explained in the subsection ‘incorporation of the comminution model’ by ensuring that the predicted size distribution corresponded to the actual size distribution of the heavy particles. This was the only step in which the actual heavy particle information was used in the prediction, and provided an artificial approach to incorporating a comminution model.  The general agreement between the model-predicted results and the actual results are quite good. The largest discrepancy occurs for the largest size-class of product particles (+710 μm); and this discrepancy appears to be due to insufficient sampling of the standard ore (250 particle sections in the mineralogical section mount). Given the fact that there are experimental and statistical errors involved in mineralogical analysis consisting of segregation of the particles when mounted in resin, incorrect identification of minerals by image analysis and statistical uncertainty. The results show great promise. It has been proposed that this method now be further tested on a variety of ores and that the method be extended to model multi-sized particles to multi-sized particles. The entropy-based model of particle composition distribution allows modelling the relationship between parent and progeny particles incorporating preferential breakage. The method allows modelling particles as multiphase, thereby modelling breakage where minerals are of different hardnesses. The results show that the probability-entropy model performs reasonably. The ore was made available from Cannington (BHP-Billiton). This work was funded partly by: The Australian Research Council (ARC) and the Australian Mining Industry Research Association (AMIRA): Optimisation and Simulation of Mineral Processing Plants (P9M) Project sponsors. Some of the ideas presented in this paper were developed from discussion between the author and colleagues of which special mention is given to Jonathan Keith who developed a stereological correction using entropy, and Pam Davy who suggested using Markov-Chain Monte-Carlo to improve computational efficiency. The mineralogical analysis was performed using the Julius Kruttschnitt Mineral Research Centre’s Mineral Liberation Analyser (MLA). The laboratory work was carried out by Trevor Bilney.