In the application of machine learning to geographic object based image analysis, several parameters influence overall classifier performance. One of the first parameters is segmentation size—for example, how many pixels should be grouped together to form an image object. Often, trial and error methods are used to obtain segmentation parameters that best delineate the borders of real world objects. Several attempts at automated methods have produced promising results, but manual intervention is still necessary. Meanwhile, numerous measures of segmentation quality have been defined, but their relationship to classifier performance is not then directly shown. For example, as measures of segmentation quality improve, do classification results improve as well? Our work considers the problem of building classification in high resolution aerial imagery of urban areas. Based on user defined training polygons generated with or without a reference segmentation, we have found several measures of segmentation quality and feature performance that can help users narrow the range of appropriate segmentations. Furthermore, our work finds that given this range, performance of machine learning algorithms remains relatively constant for any given segmentation as long as features used for classification are chosen correctly. We find that the range of scale parameters capable of producing an accurate classification is much broader than typically assumed and trial and error methods for finding this parameter may be an acceptable approach.Geographic Object Based Image Analysis (GEOBIA) offers many advantages over traditional pixel based methods. It improves classification results by avoiding the salt and pepper effect commonly seen in pixel based classifiers ( The first step in GEOBIA is to choose an appropriate segmentation. While there are a range of segmentation algorithms available, most use a set of parameters to drive the resulting size and shape of image objects. We focus our efforts on choosing the best possible scale parameter for multi-resolution segmentation algorithm ( Methods of determining optimal segmentation parameters generally fall into two categories; supervised and unsupervised. Supervised methods look to use training data, usually in the form of polygons delineating meaningful real world objects, and some measure of segmentation quality to define the optimal segmentation ( For supervised methods of segmentation optimization as well as general assessment of segmentation quality, many measures have been developed which assess both the shape and number of segments needed to delineate a geographic object (  There has been significant work in determining the segmentation scale's effect on classification results. This poses an interesting quandary, as segmentation is only one of several steps to a successful classification algorithm. Class hierarchy, classification algorithm, and feature selection all play a roll, but they are dependent to some degree on the initial segmentation chosen ( The primary goal of the classification problem studied here is to find post-event earthquake damage. By first solving the building classification problem, the damage classification results are improved ( This paper considers the example of the Christchurch, New Zealand 6.2 magnitude (Richter scale) earthquake that occurred on February 22, 2011. Post-event imagery was obtained from Land Information New Zealand ( Three different datasets were created for the purposes of training and validation. For all datasets, only complete building segments without any visually detectable damage are labeled as building. Any segments containing any indication of earthquake damage are labeled as damage. All datasets were created by different operators with prior experience in GIS and remote sensing. Datasets A and B were initially created to explore the relationship between classification results and the creation of training and validation data with and without prior knowledge of segment borders. When it become clear that there was some kind of relationship between classification results and the over and under segmentation values, Dataset C was included from a prior study for comparison ( In Dataset A, the operator was given images segmented with scale parameter 10, and told to classify all image objects that clearly fell into one of five categories: building, damaged areas, pavement, vegetation, and vehicles. For the purposes of this study, all objects labeled anything other than building were considered as part of the non-building class. Unless otherwise stated, unlabeled segments were not considered. In Dataset B, the operator was instructed to label all pixels in the image that corresponded with only the building and damage classes. Borders were decided on a per pixel basis. All unlabeled pixels as well as damage pixels were assigned to the non-building class. Polygons were then generated delineating the borders of these labeled pixels. In Dataset C, polygons were hand drawn in QGIS around select examples of the 5 classes described. Borders are delineated by the operators interpretation of the actual real world border of the object, that is—a single GIS polygon encompasses the outline of a single building. Not all data were labeled in both images. In this instance, the non-building class represents data labeled as damage, vegetation, pavement, or vehicle.  Segmentation was performed using multi-resolution segmentation algorithm ( After each segmentation was generated, a set of 66 features was calculated for each object composed of spectral, shape, and texture values as listed in Image segments were assigned to either the building or non-building class based on a 70% overlap rule. When the training polygons are overlaid on the segmentation, we can calculate the percentage of the segment covered by a given class. If 70% of a given segment was covered by the training polygon, it was assigned to the class of the training polygon. Experiments have shown that overlap measures between 50% and 100% produce >80% user and producer accuracy with values close to 70% generally producing the best results for the building class ( From these features and labels, a random forest classifier containing 100 individual trees was trained and validated 100 times for each image pair. The scikit-learn implementation of the random forest was used ( For the purposes of comparison, we generated several evaluation metrics based on various outputs of this process. From these evaluation metrics, we can better understand the relationship between segmentation scale parameter and its impact on classifier performance and feature importance. Results are compared by considering the area under a receiver operator characteristic curve (ROC AUC) using the building class as the positive case. An ideal classifier would produce an area value of 1 whereas assigning a class at random would yield an area of 0.5. This measure gives a single number to measure performance for classification results between different segmentation scales. Vector maps are converted to raster maps, and all labeled pixels are considered when tabulating results. Because individual pixels may be labeled differently and image objects contain differing number of pixels at different segmentation levels, a purely pixel based evaluation is a fairer comparison between object based classifiers using different segmentation parameters as long as all pixels are considered in each comparison ( Feature importance of the three classes of features is calculated by scikit-learn. Importance for each feature for each of the 100 runs of the classifier are combined and averaged together. Values for the three classes of features are then combined into a value for each class, represented as a value ranging from 0 to 1 which can be interpreted as a percentage of contribution to the overall classification. The feature importance values should total 1 for all features used in a classification. However, when segmentation scales grow too large, the classifier can produce a total contribution value for all features that totals less than one. Average Feature importance of 0 indicates there are no longer two classes to distinguish between. Several measures of segmentation quality were calculated and considered; we found that Clinton's metrics of oversegmentation and undersegmentation to be the best indicators of classification results. Segmentation metrics such as Clinton's Quality Rate consider parameters such as shape and border adherence. In this study, we are primarily concerned with the resulting size of the segmentation. Although the multi-resolution segmentation algorithm does offer parameters to tune the resulting shape of segments, we focus only on size. We calculated these oversegmentation and undersegmentation values for all training data for a given image and dataset using By nature of their calculation, the measurements of undersegmentation and oversegmentation will converge as the segmentation scale parameter increases and the size of the resulting segments increases. The convergence of these two values typically occurs at a segmentation scale parameter of interest for those considering an appropriate segmentation level used when classifying an image. As long as the segmentation level remains significantly less than the crossover of the undersegmentation and oversegmentation values, the classification results, as indicated by the ROC AUC, stay relatively constant given various segmentation scale parameters (illustrated in Different feature sets play different roles in classification results at different scale parameters ( An important parameter to monitor is the overall feature importance. At higher segmentation values, the overall value of the combined feature importances drops below 1 meaning all features are making less of an impact on the resulting decision. While a model suffering from this issue can produce reasonable classification results, this is usually indicative that the number of training samples has become exceedingly small and the classifier is no longer producing an effective model. This is demonstrated in The different labeling approaches for training and validation Datasets A, B, and C have markedly different results as the segmentation scale parameter increases. This is readily apparent in the drop off of overall feature importance. We can see this happen at a point relative to the amount of data classified as listed in The importance of the quantity and quality of labeled data available is also made readily apparent by both the classification performance and feature importance results of Dataset B. We can see that the transition of feature importance between segmentation levels is much more gradual. This may be because all data is labeled in Dataset B as opposed to specific examples in Datasets A and C. This seems to smooth the transition in feature importance, possibly because there are just more examples and extreme examples are not pushing the model to favor certain features as much. However, because all data is labeled—less clear examples of the classes are also labeled, likely contributing to the lower all performance of the classifier in Dataset B. While much of the work described in Section An interesting suggestion of this study is that classification results can remain relatively constant across increasing segmentation scale parameters. Usually, we might expect classifier results to suffer as segmentation scales go up as the number of training samples is both decreasing and representing increasingly heterogeneous groups of pixels. This may be the result of dynamic feature selection finding different ways to differentiate segments in a meaningful way. This relationship between the segmentation scale, the number of training points, and the ability of different features to best describe different quantities of pixels in an object warrants further investigation. Despite the existing work in avoiding the trial and error approach to segmentation scale parameter selection, this method may in fact be a reasonable approach capable of producing maps of similar accuracy to non trial and error methods. Based on the standard of classification results, a large range of segmentation values have proven to produce comparable results in the problem presented, many of which are easily found by minimal trial and error. While this study considers only a specific image classification problem, we have approached it using a variety of techniques for labeling the input datasets. Given this variety of inputs, it is hoped that these results are applicable to other object based classification problems as well. Across all these different techniques, one major outcome holds true regardless—the availability of sufficient labeled data available has the biggest impact on classification results. This quantity of labeled data is directly driven by the segmentation parameter. In machine learning applications, making sure there are enough appropriate examples is more important the ability of a segmentation to adhere to the most appropriate size. Research described in this presentation was carried out in part at the Jet Propulsion Laboratory, under contract with the National Aeronautics and Space Administration. This material is based upon work supported by the